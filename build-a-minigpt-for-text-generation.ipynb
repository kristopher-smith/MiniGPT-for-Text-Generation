{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# MiniGPT For Generating Synthetic Text Data\n\nby Kris Smith","metadata":{}},{"cell_type":"markdown","source":"# ***WARNING*** \n\n## The data required to train the model for this task is known to be vulgar, offensive, toxic, racist, and otherwise not pleasant.","metadata":{}},{"cell_type":"markdown","source":"## Problem Statement\n\nToxic comments online come in many forms and in many arenas. There are currently several ways to mitigate these comments(for those organizations who wish to do so). Some of these ways include human moderators, and training machine learning models to detect toxicity in online comments.\n\nThe issue with human moderators is that some of these platforms have grown so large so quickly that there are not nearly enough moderators to achieve any sense of control for most of these comments. The shear volume of toxicity and bots online makes it unrealistic to think we could do this job with humans at this point.\n\nMany companies are employing machine learning to assist with identifying toxic comments online automatically. The problem with this approach is the lack of labeled training data to train the models on.\n\nThis is the problem I am going to solve using generative deep learning techniques. ","metadata":{}},{"cell_type":"markdown","source":"## References\n\n* [Improving Language Understanding by Generative Pre-Training](https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035)\n\n* [Language Models are Unsupervised Multitask Learners](https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe)\n\n* [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)\n\n* Many of the ideas and code were adapted from this Keras resource: https://keras.io/examples/generative/text_generation_with_miniature_gpt/","metadata":{}},{"cell_type":"markdown","source":"## Import Libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport string\nimport random\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import TextVectorization","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-25T16:54:32.531057Z","iopub.execute_input":"2023-06-25T16:54:32.531413Z","iopub.status.idle":"2023-06-25T16:54:39.630246Z","shell.execute_reply.started":"2023-06-25T16:54:32.531384Z","shell.execute_reply":"2023-06-25T16:54:39.629097Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Data\n\nThe data I will be using to train the generative model was released on Kaggle as part of an ongoing series of competitions sponsored by the [Google company Jigsaw](https://en.wikipedia.org/wiki/Jigsaw_(company)).\n\nThe data consists of online comments with various severity levels of toxicity. There are versions of these comments labeled by human annotators wherein they label each comment as toxic or not, or other sets where they were labeled as different categories of toxic such as hatespeech, racist/sexist, obscene, etc. Although these are the labeled datasets we would be adding the synthetic data to in order to create more training data, for this task of simply generating similar text data we will only focus on the comments themselves.\n\nThe data provided by this competition includes a total of `14,251` unique toxic comments. Theses are the comments I will use to train the generative model with.","metadata":{}},{"cell_type":"markdown","source":"### Load In Toxic Comment Data\n\nThe data came in two different files.\n\n1) Comments to score: This acts as a test dataset of comments for scoring after the model was trained.\n\n2) Validation data: This was the training data for the competition wherein there are two columns. One column labeled less toxic was a comment which human annotators labeled as less toxic than its more toxic counterpart in the other column. There was no actual training data where a comment was paired with its severity rating. The models were trained using creative techniques with the validation data and other classification data sets to train a model which predicted severity of comments.\n\nSince for our purposes we are only interested in the actual text comments themselves, I will only be using those columns from these datasources.\n\nI start by reading them all into pandas dataframes, isolating the text columns from each one, and stacking them all together so we have a single column of text when it is all said and done.\n","metadata":{}},{"cell_type":"code","source":"data1 = pd.read_csv('../input/jigsaw-toxic-severity-rating/comments_to_score.csv')\ndata1.info()\n\n## Isolate only text column\ndata1 = data1['text']","metadata":{"execution":{"iopub.status.busy":"2023-06-25T18:00:41.605600Z","iopub.execute_input":"2023-06-25T18:00:41.605986Z","iopub.status.idle":"2023-06-25T18:00:41.660914Z","shell.execute_reply.started":"2023-06-25T18:00:41.605956Z","shell.execute_reply":"2023-06-25T18:00:41.659791Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 7537 entries, 0 to 7536\nData columns (total 2 columns):\n #   Column      Non-Null Count  Dtype \n---  ------      --------------  ----- \n 0   comment_id  7537 non-null   int64 \n 1   text        7537 non-null   object\ndtypes: int64(1), object(1)\nmemory usage: 117.9+ KB\n","output_type":"stream"}]},{"cell_type":"code","source":"data1.sample(6)","metadata":{"execution":{"iopub.status.busy":"2023-06-25T18:03:27.243648Z","iopub.execute_input":"2023-06-25T18:03:27.244334Z","iopub.status.idle":"2023-06-25T18:03:27.251868Z","shell.execute_reply.started":"2023-06-25T18:03:27.244301Z","shell.execute_reply":"2023-06-25T18:03:27.250947Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"1128     Your edits to Shockwave (roller coaster) \\nPl...\n5118    \"\\n\\n_________________________________________...\n243      (UTC)\\n\\nYou never been on the damn bus,     ...\n3004    Hey queer Andrew\\n\\nStop messing around!Is you...\n7391    \">>> *** You posted a nonsensical edit in \"\"Is...\n7071    \"\\n\\n A barnstar for you! \\n\\n  The Anti-Vanda...\nName: text, dtype: object"},"metadata":{}}]},{"cell_type":"markdown","source":"We can see the comments to score was the test file which contained only comments and their corresponding id's","metadata":{}},{"cell_type":"code","source":"data2 = pd.read_csv('../input/jigsaw-toxic-severity-rating/validation_data.csv')\ndata2.info()\n\ndata2.sample(6)","metadata":{"execution":{"iopub.status.busy":"2023-06-25T18:03:37.791853Z","iopub.execute_input":"2023-06-25T18:03:37.792551Z","iopub.status.idle":"2023-06-25T18:03:38.060287Z","shell.execute_reply.started":"2023-06-25T18:03:37.792518Z","shell.execute_reply":"2023-06-25T18:03:38.059285Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 30108 entries, 0 to 30107\nData columns (total 3 columns):\n #   Column      Non-Null Count  Dtype \n---  ------      --------------  ----- \n 0   worker      30108 non-null  int64 \n 1   less_toxic  30108 non-null  object\n 2   more_toxic  30108 non-null  object\ndtypes: int64(1), object(2)\nmemory usage: 705.8+ KB\n","output_type":"stream"},{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"       worker                                         less_toxic  \\\n10739     358  \"\\n\\n Is that the best you can say? \\n\\nWOW! M...   \n4049      472  America is so much richer with these niggers a...   \n19438      48  I believe that you should include in your disc...   \n13073     215  Hello==\\n\\nWhat is vandalism? Please help me o...   \n13081     154   Still Don't Like You \\n\\nYour constant editin...   \n1580      572  Hello, and welcome to Wikipedia! We don't take...   \n\n                                              more_toxic  \n10739  \"\\n\\nYou just plain fanatics, stupid stupid he...  \n4049          unblock| jayron32 likes to sleep with boys  \n19438  \"\\n\\n Plot \\nI changed  this is a highly ficti...  \n13073  \"SANDAHL IS OF QUESTIONABLE ETHNICITY AND I HA...  \n13081                            Nobody likes a snitch.   \n1580   I am not. Just checkuser me. But the Kuntopf-S...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>worker</th>\n      <th>less_toxic</th>\n      <th>more_toxic</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>10739</th>\n      <td>358</td>\n      <td>\"\\n\\n Is that the best you can say? \\n\\nWOW! M...</td>\n      <td>\"\\n\\nYou just plain fanatics, stupid stupid he...</td>\n    </tr>\n    <tr>\n      <th>4049</th>\n      <td>472</td>\n      <td>America is so much richer with these niggers a...</td>\n      <td>unblock| jayron32 likes to sleep with boys</td>\n    </tr>\n    <tr>\n      <th>19438</th>\n      <td>48</td>\n      <td>I believe that you should include in your disc...</td>\n      <td>\"\\n\\n Plot \\nI changed  this is a highly ficti...</td>\n    </tr>\n    <tr>\n      <th>13073</th>\n      <td>215</td>\n      <td>Hello==\\n\\nWhat is vandalism? Please help me o...</td>\n      <td>\"SANDAHL IS OF QUESTIONABLE ETHNICITY AND I HA...</td>\n    </tr>\n    <tr>\n      <th>13081</th>\n      <td>154</td>\n      <td>Still Don't Like You \\n\\nYour constant editin...</td>\n      <td>Nobody likes a snitch.</td>\n    </tr>\n    <tr>\n      <th>1580</th>\n      <td>572</td>\n      <td>Hello, and welcome to Wikipedia! We don't take...</td>\n      <td>I am not. Just checkuser me. But the Kuntopf-S...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"This was the data provided to validate the models performance during training. The three columns are workers(annotators) and the other two are text columns which we will use both to train our generative model with.","metadata":{}},{"cell_type":"markdown","source":"#### Combine all columns into a single column","metadata":{}},{"cell_type":"code","source":"## Isolate text column\ndata2 = data2['more_toxic']\n\n## Isolate text column\ndata3 = pd.read_csv('../input/jigsaw-toxic-severity-rating/validation_data.csv')\ndata3 = data3['less_toxic']\n\ntext_column = pd.concat([data1, data2, data3], axis=0, ignore_index=True)\n\ntext_column.value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-06-25T18:05:19.867659Z","iopub.execute_input":"2023-06-25T18:05:19.868365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Total numer of comments in text data = {len(text_column)}\")\nprint(f\"Numer of unique comments in text data = {len(text_column.unique())}\")\n\ntext_column = text_column.drop_duplicates()\nprint(\"Duplicate comments dropped\")","metadata":{"execution":{"iopub.status.busy":"2023-06-25T17:58:22.313428Z","iopub.execute_input":"2023-06-25T17:58:22.313910Z","iopub.status.idle":"2023-06-25T17:58:22.885710Z","shell.execute_reply.started":"2023-06-25T17:58:22.313872Z","shell.execute_reply":"2023-06-25T17:58:22.884611Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Total numer of comments in text data = 67753\nNumer of unique comments in text data = 14251\nDuplicate comments dropped\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Unfortunately due to the structure of the data, many of those comments were duplicates. I removed duplicate comments as to not bias the model. as ","metadata":{}},{"cell_type":"code","source":"text\n\n\n\n\ndef custom_standardization(input_string):\n    \"\"\" Remove html line-break tags and handle punctuation \"\"\"\n    lowercased = tf.strings.lower(input_string)\n    stripped_html = tf.strings.regex_replace(lowercased, \"<br />\", \" \")\n    return tf.strings.regex_replace(stripped_html, f\"([{string.punctuation}])\", r\" \\1\")\n\n\n# Create a vectorization layer and adapt it to the text\nvectorize_layer = TextVectorization(\n    standardize=custom_standardization,\n    max_tokens=vocab_size - 1,\n    output_mode=\"int\",\n    output_sequence_length=maxlen + 1,\n)\nvectorize_layer.adapt(text_ds)\nvocab = vectorize_layer.get_vocabulary()  # To get words back from token indices\n\n## Functoin to create target column\ndef prepare_lm_inputs_labels(text):\n    \"\"\"\n    Shift word sequences by 1 position so that the target for position (i) is\n    word at position (i+1). The model will use all words up till position (i)\n    to predict the next word.\n    \"\"\"\n    text = tf.expand_dims(text, -1)\n    tokenized_sentences = vectorize_layer(text)\n    x = tokenized_sentences[:, :-1]\n    y = tokenized_sentences[:, 1:]\n    return x, y\n\n\n# Create a dataset from the pandas column\ntext_ds = tf.data.Dataset.from_tensor_slices(text_column)\n\n# Shuffle and batch the dataset\ntext_ds = text_ds.shuffle(buffer_size=256)\ntext_ds = text_ds.batch(batch_size)\n\n# Create a vectorization layer and adapt it to the text\nvectorize_layer = TextVectorization(\n    standardize=custom_standardization,\n    max_tokens=vocab_size - 1,\n    output_mode=\"int\",\n    output_sequence_length=maxlen + 1,\n)\n\nvectorize_layer.adapt(text_ds)\nvocab = vectorize_layer.get_vocabulary()  # To get words back from token indices\n\ntext_ds = text_ds.map(prepare_lm_inputs_labels)\ntext_ds = text_ds.prefetch(tf.data.AUTOTUNE)\n\n\n\n# Preview a sample from the text_ds dataset\nsample = text_ds.take(5)  # Take one sample from the dataset\n\nfor x, y in sample:\n    # Convert token indices back to words\n    input_words  = [vocab[i] for i in x[0].numpy()]\n    target_words = [vocab[i] for i in y[0].numpy()]\n\n    # Print the input and target sequences\n    print(\"Input Sequence:\")\n    print(\" \".join(input_words))\n    print(\"\\nTarget Sequence:\")\n    print(\" \".join(target_words))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Cell below was for loading in and preprocessing the IMBD movie quotes dataset. This is the dataset I tested this approach on first.","metadata":{}},{"cell_type":"code","source":"# !curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n# !tar -xf aclImdb_v1.tar.gz\n\n# batch_size = 128\n\n# # The dataset contains each review in a separate text file\n# # The text files are present in four different folders\n# # Create a list all files\n# filenames = []\n# directories = [\n#     \"aclImdb/train/pos\",\n#     \"aclImdb/train/neg\",\n#     \"aclImdb/test/pos\",\n#     \"aclImdb/test/neg\",\n# ]\n# for dir in directories:\n#     for f in os.listdir(dir):\n#         filenames.append(os.path.join(dir, f))\n\n# print(f\"{len(filenames)} files\")\n\n# # Create a dataset from text files\n# random.shuffle(filenames)\n# text_ds = tf.data.TextLineDataset(filenames)\n# text_ds = text_ds.shuffle(buffer_size=256)\n# text_ds = text_ds.batch(batch_size)\n\n# def custom_standardization(input_string):\n#     \"\"\" Remove html line-break tags and handle punctuation \"\"\"\n#     lowercased = tf.strings.lower(input_string)\n#     stripped_html = tf.strings.regex_replace(lowercased, \"<br />\", \" \")\n#     return tf.strings.regex_replace(stripped_html, f\"([{string.punctuation}])\", r\" \\1\")\n\n\n# # Create a vectorization layer and adapt it to the text\n# vectorize_layer = TextVectorization(\n#     standardize=custom_standardization,\n#     max_tokens=vocab_size - 1,\n#     output_mode=\"int\",\n#     output_sequence_length=maxlen + 1,\n# )\n# vectorize_layer.adapt(text_ds)\n# vocab = vectorize_layer.get_vocabulary()  # To get words back from token indices\n\n# ## Functoin to create target column\n# def prepare_lm_inputs_labels(text):\n#     \"\"\"\n#     Shift word sequences by 1 position so that the target for position (i) is\n#     word at position (i+1). The model will use all words up till position (i)\n#     to predict the next word.\n#     \"\"\"\n#     text = tf.expand_dims(text, -1)\n#     tokenized_sentences = vectorize_layer(text)\n#     x = tokenized_sentences[:, :-1]\n#     y = tokenized_sentences[:, 1:]\n#     return x, y\n\n\n# text_ds = text_ds.map(prepare_lm_inputs_labels)\n# text_ds = text_ds.prefetch(tf.data.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2023-06-25T16:54:39.672498Z","iopub.execute_input":"2023-06-25T16:54:39.672899Z","iopub.status.idle":"2023-06-25T16:54:55.769014Z","shell.execute_reply.started":"2023-06-25T16:54:39.672868Z","shell.execute_reply":"2023-06-25T16:54:55.767732Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 80.2M  100 80.2M    0     0  9419k      0  0:00:08  0:00:08 --:--:-- 17.8M\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Implement a Transformer block as a layer","metadata":{}},{"cell_type":"code","source":"def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n    \"\"\"\n    Mask the upper half of the dot product matrix in self attention.\n    This prevents flow of information from future tokens to current token.\n    1's in the lower triangle, counting from the lower right corner.\n    \"\"\"\n    i = tf.range(n_dest)[:, None]\n    j = tf.range(n_src)\n    m = i >= j - n_src + n_dest\n    mask = tf.cast(m, dtype)\n    mask = tf.reshape(mask, [1, n_dest, n_src])\n    mult = tf.concat(\n        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n    )\n    return tf.tile(mask, mult)\n\n\nclass TransformerBlock(layers.Layer):\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n        super().__init__()\n        self.att = layers.MultiHeadAttention(num_heads, embed_dim)\n        self.ffn = keras.Sequential(\n            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n        )\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(rate)\n        self.dropout2 = layers.Dropout(rate)\n\n    def call(self, inputs):\n        input_shape = tf.shape(inputs)\n        batch_size = input_shape[0]\n        seq_len = input_shape[1]\n        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n        attention_output = self.att(inputs, inputs, attention_mask=causal_mask)\n        attention_output = self.dropout1(attention_output)\n        out1 = self.layernorm1(inputs + attention_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output)\n        return self.layernorm2(out1 + ffn_output)","metadata":{"execution":{"iopub.status.busy":"2023-06-25T16:54:39.634011Z","iopub.execute_input":"2023-06-25T16:54:39.635264Z","iopub.status.idle":"2023-06-25T16:54:39.647588Z","shell.execute_reply.started":"2023-06-25T16:54:39.635229Z","shell.execute_reply":"2023-06-25T16:54:39.646676Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Implement an embedding layer\n\nCreate two separate embedding layers: one for tokens and one for token index (positions).","metadata":{}},{"cell_type":"code","source":"class TokenAndPositionEmbedding(layers.Layer):\n    def __init__(self, maxlen, vocab_size, embed_dim):\n        super().__init__()\n        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n\n    def call(self, x):\n        maxlen = tf.shape(x)[-1]\n        positions = tf.range(start=0, limit=maxlen, delta=1)\n        positions = self.pos_emb(positions)\n        x = self.token_emb(x)\n        return x + positions","metadata":{"execution":{"iopub.status.busy":"2023-06-25T16:54:39.649912Z","iopub.execute_input":"2023-06-25T16:54:39.650555Z","iopub.status.idle":"2023-06-25T16:54:39.657795Z","shell.execute_reply.started":"2023-06-25T16:54:39.650524Z","shell.execute_reply":"2023-06-25T16:54:39.656702Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Implement the miniature GPT model","metadata":{}},{"cell_type":"code","source":"vocab_size = 20000  # Only consider the top 20k words\nmaxlen = 80  # Max sequence size\nembed_dim = 256  # Embedding size for each token\nnum_heads = 2  # Number of attention heads\nfeed_forward_dim = 256  # Hidden layer size in feed forward network inside transformer\n\n\ndef create_model():\n    inputs = layers.Input(shape=(maxlen,), dtype=tf.int32)\n    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n    x = embedding_layer(inputs)\n    transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim)\n    x = transformer_block(x)\n    outputs = layers.Dense(vocab_size)(x)\n    model = keras.Model(inputs=inputs, outputs=[outputs, x])\n    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n    model.compile(\n        \"adam\", loss=[loss_fn, None],\n    )  # No loss and optimization based on word embeddings from transformer block\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-06-25T16:54:39.660708Z","iopub.execute_input":"2023-06-25T16:54:39.661322Z","iopub.status.idle":"2023-06-25T16:54:39.670841Z","shell.execute_reply.started":"2023-06-25T16:54:39.661291Z","shell.execute_reply":"2023-06-25T16:54:39.669815Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Prepare the data for word-level language modelling\n\nDownload the IMDB dataset and combine training and validation sets for a text generation task.","metadata":{}},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-06-25T16:55:09.057157Z","iopub.execute_input":"2023-06-25T16:55:09.057508Z","iopub.status.idle":"2023-06-25T16:55:09.903678Z","shell.execute_reply.started":"2023-06-25T16:55:09.057475Z","shell.execute_reply":"2023-06-25T16:55:09.902724Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"14251"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-06-25T16:55:09.905231Z","iopub.execute_input":"2023-06-25T16:55:09.905613Z","iopub.status.idle":"2023-06-25T16:55:12.217453Z","shell.execute_reply.started":"2023-06-25T16:55:09.905579Z","shell.execute_reply":"2023-06-25T16:55:12.216469Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Implement a Keras callback for generating text","metadata":{}},{"cell_type":"code","source":"# class TextGenerator(keras.callbacks.Callback):\n#     \"\"\"A callback to generate text from a trained model.\n#     1. Feed some starting prompt to the model\n#     2. Predict probabilities for the next token\n#     3. Sample the next token and add it to the next input\n\n#     Arguments:\n#         max_tokens: Integer, the number of tokens to be generated after prompt.\n#         start_tokens: List of integers, the token indices for the starting prompt.\n#         index_to_word: List of strings, obtained from the TextVectorization layer.\n#         top_k: Integer, sample from the `top_k` token predictions.\n#         print_every: Integer, print after this many epochs.\n#     \"\"\"\n\n#     def __init__(\n#         self, max_tokens, start_tokens, index_to_word, top_k=10, print_every=1\n#     ):\n#         self.max_tokens = max_tokens\n#         self.start_tokens = start_tokens\n#         self.index_to_word = index_to_word\n#         self.print_every = print_every\n#         self.k = top_k\n\n#     def sample_from(self, logits):\n#         logits, indices = tf.math.top_k(logits, k=self.k, sorted=True)\n#         indices = np.asarray(indices).astype(\"int32\")\n#         preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n#         preds = np.asarray(preds).astype(\"float32\")\n#         return np.random.choice(indices, p=preds)\n\n#     def detokenize(self, number):\n#         return self.index_to_word[number]\n\n#     def on_epoch_end(self, epoch, logs=None):\n#         start_tokens = [_ for _ in self.start_tokens]\n#         if (epoch + 1) % self.print_every != 0:\n#             return\n#         num_tokens_generated = 0\n#         tokens_generated = []\n#         while num_tokens_generated <= self.max_tokens:\n#             pad_len = maxlen - len(start_tokens)\n#             sample_index = len(start_tokens) - 1\n#             if pad_len < 0:\n#                 x = start_tokens[:maxlen]\n#                 sample_index = maxlen - 1\n#             elif pad_len > 0:\n#                 x = start_tokens + [0] * pad_len\n#             else:\n#                 x = start_tokens\n#             x = np.array([x])\n#             y, _ = self.model.predict(x)\n#             sample_token = self.sample_from(y[0][sample_index])\n#             tokens_generated.append(sample_token)\n#             start_tokens.append(sample_token)\n#             num_tokens_generated = len(tokens_generated)\n#         txt = \" \".join(\n#             [self.detokenize(_) for _ in self.start_tokens + tokens_generated]\n#         )\n#         print(f\"generated text:\\n{txt}\\n\")\n        \n        \n\n# # Tokenize starting prompt\n# word_to_index = {}\n# for index, word in enumerate(vocab):\n#     word_to_index[word] = index\n","metadata":{"execution":{"iopub.status.busy":"2023-06-25T16:55:12.311350Z","iopub.execute_input":"2023-06-25T16:55:12.311646Z","iopub.status.idle":"2023-06-25T16:55:12.318127Z","shell.execute_reply.started":"2023-06-25T16:55:12.311622Z","shell.execute_reply":"2023-06-25T16:55:12.317114Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class TextGenerator(keras.callbacks.Callback):\n    \"\"\"A callback to generate text from a trained model.\n    1. Feed some starting prompt to the model\n    2. Predict probabilities for the next token\n    3. Sample the next token and add it to the next input\n\n    Arguments:\n        max_tokens: Integer, the number of tokens to be generated after prompt.\n        start_tokens: List of integers, the token indices for the starting prompt.\n        index_to_word: List of strings, obtained from the TextVectorization layer.\n        top_k: Integer, sample from the `top_k` token predictions.\n        print_every: Integer, print after this many epochs.\n    \"\"\"\n\n    def __init__(\n        self, max_tokens, start_tokens, index_to_word, top_k=10, print_every=1\n    ):\n        self.max_tokens = max_tokens\n        self.start_tokens = start_tokens\n        self.index_to_word = index_to_word\n        self.print_every = print_every\n        self.k = top_k\n\n    def sample_from(self, logits):\n        logits, indices = tf.math.top_k(logits, k=self.k, sorted=True)\n        indices = np.asarray(indices).astype(\"int32\")\n        preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n        preds = np.asarray(preds).astype(\"float32\")\n        return np.random.choice(indices, p=preds)\n\n    def detokenize(self, number):\n        return self.index_to_word[number]\n\n    def on_epoch_end(self, epoch, logs=None):\n        if (epoch + 1) % self.print_every != 0:\n            return\n        print(f\"Epoch {epoch+1}: loss = {logs['loss']:.4f}\")\n        start_tokens = [_ for _ in self.start_tokens]\n        num_tokens_generated = 0\n        tokens_generated = []\n        while num_tokens_generated <= self.max_tokens:\n            pad_len = maxlen - len(start_tokens)\n            sample_index = len(start_tokens) - 1\n            if pad_len < 0:\n                x = start_tokens[:maxlen]\n                sample_index = maxlen - 1\n            elif pad_len > 0:\n                x = start_tokens + [0] * pad_len\n            else:\n                x = start_tokens\n            x = np.array([x])\n            y, _ = self.model.predict(x)\n            sample_token = self.sample_from(y[0][sample_index])\n            tokens_generated.append(sample_token)\n            start_tokens.append(sample_token)\n            num_tokens_generated = len(tokens_generated)\n        txt = \" \".join(\n            [self.detokenize(_) for _ in self.start_tokens + tokens_generated]\n        )\n        print(f\"Generated text:\\n{txt}\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-06-25T16:55:12.319809Z","iopub.execute_input":"2023-06-25T16:55:12.320506Z","iopub.status.idle":"2023-06-25T16:55:12.335714Z","shell.execute_reply.started":"2023-06-25T16:55:12.320475Z","shell.execute_reply":"2023-06-25T16:55:12.334783Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Tokenize starting prompt\nword_to_index = {}\nfor index, word in enumerate(vocab):\n    word_to_index[word] = index","metadata":{"execution":{"iopub.status.busy":"2023-06-25T16:55:40.097478Z","iopub.execute_input":"2023-06-25T16:55:40.097878Z","iopub.status.idle":"2023-06-25T16:55:40.113464Z","shell.execute_reply.started":"2023-06-25T16:55:40.097846Z","shell.execute_reply":"2023-06-25T16:55:40.112474Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"start_prompt = \"this movie is\"\nstart_prompt = \"what is the\"\n\n\nstart_tokens = [word_to_index.get(_, 1) for _ in start_prompt.split()]\nnum_tokens_generated = 40\ntext_gen_callback = TextGenerator(num_tokens_generated, start_tokens, vocab)","metadata":{"execution":{"iopub.status.busy":"2023-06-25T16:55:41.256235Z","iopub.execute_input":"2023-06-25T16:55:41.256582Z","iopub.status.idle":"2023-06-25T16:55:41.261728Z","shell.execute_reply.started":"2023-06-25T16:55:41.256555Z","shell.execute_reply":"2023-06-25T16:55:41.260823Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Train the model","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\n\n# Create a wrapper function for the training loop\ndef train_with_progress_bar(model, dataset, epochs, callbacks):\n    # Disable the progress bar by setting `disable=True`\n    with tqdm(total=epochs, disable=True) as pbar:\n        for epoch in range(epochs):\n            # Perform one epoch of training\n            model.fit(dataset, verbose=0, epochs=epochs, callbacks=[text_gen_callback])\n            \n            # Update the progress bar manually\n            pbar.set_postfix({'Epoch': epoch + 1})\n            pbar.update(1)\n\n# Train the model using the wrapper function\ntrain_with_progress_bar(model, text_ds, N_EPOCHS, callbacks=[text_gen_callback])","metadata":{"execution":{"iopub.status.busy":"2023-06-25T17:29:24.319000Z","iopub.execute_input":"2023-06-25T17:29:24.319381Z","iopub.status.idle":"2023-06-25T17:54:48.932305Z","shell.execute_reply.started":"2023-06-25T17:29:24.319349Z","shell.execute_reply":"2023-06-25T17:54:48.930679Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Epoch 1: loss = 2.2460\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\nGenerated text:\nwhat is the problem i am i am not an attack . the pilot was the next 4 that , i was also you may i am the past , and i am still trying to see that you are the whole point in\n\nEpoch 2: loss = 1.8186\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\nGenerated text:\nwhat is the problem is your problem . . . i am not a brand new article from the new article which i 've got news for a perfect ? i vehemently disagree on the graffiti more neutral . the most of the joker\n\nEpoch 3: loss = 1.4912\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\nGenerated text:\nwhat is the problem , there 's no problem dude , why don 't you keep me and just giving me warnings to your mistakes . but not your problem is that there is no need to be fair enough to tell the problem\n\nEpoch 4: loss = 1.2542\n1/1 [==============================] - 0s 31ms/step\n1/1 [==============================] - 0s 33ms/step\n1/1 [==============================] - 0s 36ms/step\n1/1 [==============================] - 0s 31ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 37ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\nGenerated text:\nwhat is the problem ? that 's some guy ? i have noticed above no power given in the future , you should definitely remove other users like your talk with your \" [UNK] ? ) \"       \n\nEpoch 5: loss = 1.0854\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\nGenerated text:\nwhat is the wrong with you people such [UNK] ? no reason why don 't you delete a category [UNK] being such as an evil .                  \n\nEpoch 6: loss = 0.9561\n1/1 [==============================] - 0s 31ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 32ms/step\n1/1 [==============================] - 0s 31ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 31ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 31ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\nGenerated text:\nwhat is the problem ! i 've done it , it 's easy to make me laugh at the next time , that you cannot get it . i am a big fat [UNK] admin abuse power thing ! ! ! 86 .57 \n\nEpoch 7: loss = 0.8580\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 25ms/step\nGenerated text:\nwhat is the problem ? i 'm not breaching here by this page here . here is why you need to remove this name ? well , why do you need by 2 [UNK] well , you need evidence to your gay ? ?\n\nEpoch 8: loss = 0.7785\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\nGenerated text:\nwhat is the problem ? people like your problem ? i have recently read wikipedia rules and your deletions and a meat turns out , so basically , not convinced regarding your a user page . please stop .     \n\nEpoch 9: loss = 0.7137\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 31ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 34ms/step\n1/1 [==============================] - 0s 34ms/step\n1/1 [==============================] - 0s 32ms/step\n1/1 [==============================] - 0s 33ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\nGenerated text:\nwhat is the problem ? ? why do you reverting my edits ? ! i didn 't start calling you a racist . but i didn 't vandalize your bullshit edits ?            \n\nEpoch 10: loss = 0.6603\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 31ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\nGenerated text:\nwhat is the problem ? go ahead and delete that page ? look at it ! ! ! ! the flags should take a went up to a good notice how to you are .         \n\nEpoch 11: loss = 0.6168\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 22ms/step\nGenerated text:\nwhat is the point of the people . the second thing is ridiculous that i don 't think that [UNK] is what he thinks they should say is any change and [UNK] to the truth , which is . what is the hell does\n\nEpoch 12: loss = 0.5802\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 37ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\nGenerated text:\nwhat is the problem with somebody [UNK] boy . maybe just shut up fuck up                             \n\nEpoch 13: loss = 0.5491\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 43ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 20ms/step\nGenerated text:\nwhat is the right and you don 't mind your own business . i can create re [UNK] , and i will keep on the other pages on wikipedia . if i wish you knew when there were no place will not located the\n\nEpoch 14: loss = 0.5218\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 37ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\nGenerated text:\nwhat is the longer [UNK] is [UNK] and a platform so don 't try ur sick views here .first collect correct information and [UNK] [UNK] [UNK] ) . they use [UNK] and remove misrepresentation of the practice and then it myself will make your\n\nEpoch 15: loss = 0.4994\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 31ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 45ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\nGenerated text:\nwhat is the fuck is you ? what the fuck are you talking about ?                             \n\nEpoch 16: loss = 0.4781\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 22ms/step\nGenerated text:\nwhat is the problem , that 's exactly the phrase , i didn 't put them to vandalize a page , i will be blocked from now until i don 't care about your stupid asshole . ( )     \n\nEpoch 17: loss = 0.4600\n1/1 [==============================] - 0s 37ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 33ms/step\n1/1 [==============================] - 0s 34ms/step\n1/1 [==============================] - 0s 34ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\nGenerated text:\nwhat is the ? ? ? ? ? i can only a couple of weeks thinking absurd game . however , it is that because it was not an incorrect . although apparently it has already been a page against the [UNK] to be\n\nEpoch 18: loss = 0.4434\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\nGenerated text:\nwhat is the right and you should improve the article by harassing me !                              \n\nEpoch 19: loss = 0.4289\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\nGenerated text:\nwhat is the matter who you are so gay ?                                  \n\nEpoch 20: loss = 0.4170\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\nGenerated text:\nwhat is the wrong with you , and you lose all ! i have to take a little shit . 86 .140 .114                     \n\nEpoch 21: loss = 0.4027\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 33ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\nGenerated text:\nwhat is the vandalism so awful , what , he has done ?                               \n\nEpoch 22: loss = 0.3934\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\nGenerated text:\nwhat is the vandalism ? ? ? ? ? ? [UNK] ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ! ? ? clearly why did you read a crew   \n\nEpoch 23: loss = 0.3820\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 31ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\nGenerated text:\nwhat is the problem ! ? some administrators on my talk page ? -                              \n\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[22], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m             pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Train the model using the wrapper function\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[43mtrain_with_progress_bar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtext_gen_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[22], line 9\u001b[0m, in \u001b[0;36mtrain_with_progress_bar\u001b[0;34m(model, dataset, epochs, callbacks)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39mepochs, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;66;03m# Perform one epoch of training\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m         \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtext_gen_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;66;03m# Update the progress bar manually\u001b[39;00m\n\u001b[1;32m     12\u001b[0m         pbar\u001b[38;5;241m.\u001b[39mset_postfix({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m'\u001b[39m: epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m})\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/engine/training.py:1676\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1674\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_epoch_begin(epoch)\n\u001b[1;32m   1675\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n\u001b[0;32m-> 1676\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39msteps():\n\u001b[1;32m   1677\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1678\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1679\u001b[0m             epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m             _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1683\u001b[0m         ):\n\u001b[1;32m   1684\u001b[0m             callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/engine/data_adapter.py:1375\u001b[0m, in \u001b[0;36mDataHandler.steps\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insufficient_data:  \u001b[38;5;66;03m# Set by `catch_stop_iteration`.\u001b[39;00m\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1375\u001b[0m original_spe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_steps_per_execution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m   1376\u001b[0m can_run_full_execution \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1377\u001b[0m     original_spe \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1378\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferred_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1379\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferred_steps \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_step \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m original_spe\n\u001b[1;32m   1380\u001b[0m )\n\u001b[1;32m   1382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_run_full_execution:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/ops/resource_variable_ops.py:647\u001b[0m, in \u001b[0;36mBaseResourceVariable.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnumpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    646\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 647\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    648\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    649\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy() is only available when eager execution is enabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1160\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \n\u001b[1;32m   1139\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1157\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[1;32m   1158\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1159\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[0;32m-> 1160\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1126\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1125\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1127\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"model = create_model()\n\nN_EPOCHS = 25\nverbose = 0 ## Set to a number such as 2 to see each steps progress bar\nmodel.fit(text_ds, verbose=0, epochs=N_EPOCHS, callbacks=[text_gen_callback])","metadata":{"execution":{"iopub.status.busy":"2023-06-25T17:26:15.959623Z","iopub.execute_input":"2023-06-25T17:26:15.960541Z","iopub.status.idle":"2023-06-25T17:28:40.585103Z","shell.execute_reply.started":"2023-06-25T17:26:15.960509Z","shell.execute_reply":"2023-06-25T17:28:40.582658Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Epoch 1: loss = 3.0433\n1/1 [==============================] - 0s 265ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\nGenerated text:\nwhat is the [UNK] ? ? what 's your part of the hell for you are [UNK] [UNK] ? ? ? ? ? ? ? ! ? ? ? ? ? i think it 's no one ? ? ? ? ? ? ?\n\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[21], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m N_EPOCHS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m25\u001b[39m\n\u001b[1;32m      4\u001b[0m verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;66;03m## Set to a number such as 2 to see each steps progress bar\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtext_gen_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/engine/training.py:1673\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1671\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch, iterator \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39menumerate_epochs():\n\u001b[0;32m-> 1673\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1674\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_epoch_begin(epoch)\n\u001b[1;32m   1675\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/engine/training.py:2428\u001b[0m, in \u001b[0;36mModel.reset_metrics\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2424\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribution_strategy \u001b[38;5;241m=\u001b[39m original_pss_strategy\n\u001b[1;32m   2426\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf_utils\u001b[38;5;241m.\u001b[39msync_to_numpy_or_python_type(all_outputs)\n\u001b[0;32m-> 2428\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset_metrics\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   2429\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Resets the state of all the metrics in the model.\u001b[39;00m\n\u001b[1;32m   2430\u001b[0m \n\u001b[1;32m   2431\u001b[0m \u001b[38;5;124;03m    Examples:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2445\u001b[0m \n\u001b[1;32m   2446\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   2447\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"Now we can generate text continuuing from a new prmopt ","metadata":{}},{"cell_type":"code","source":"new_start_prompt = \"start something\"\nnew_start_tokens = [word_to_index.get(word, 1) for word in new_start_prompt.split()]\n\ntext_gen_callback.start_tokens = new_start_tokens\ntext_gen_callback.on_epoch_end(0)","metadata":{"execution":{"iopub.status.busy":"2023-06-25T16:55:12.607587Z","iopub.status.idle":"2023-06-25T16:55:12.608278Z","shell.execute_reply.started":"2023-06-25T16:55:12.608028Z","shell.execute_reply":"2023-06-25T16:55:12.608050Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}