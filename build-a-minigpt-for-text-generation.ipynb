{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import TextVectorization\nimport numpy as np\nimport os\nimport string\nimport random","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-25T03:21:25.677811Z","iopub.execute_input":"2023-06-25T03:21:25.678282Z","iopub.status.idle":"2023-06-25T03:21:25.687664Z","shell.execute_reply.started":"2023-06-25T03:21:25.678242Z","shell.execute_reply":"2023-06-25T03:21:25.686727Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n    \"\"\"\n    Mask the upper half of the dot product matrix in self attention.\n    This prevents flow of information from future tokens to current token.\n    1's in the lower triangle, counting from the lower right corner.\n    \"\"\"\n    i = tf.range(n_dest)[:, None]\n    j = tf.range(n_src)\n    m = i >= j - n_src + n_dest\n    mask = tf.cast(m, dtype)\n    mask = tf.reshape(mask, [1, n_dest, n_src])\n    mult = tf.concat(\n        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n    )\n    return tf.tile(mask, mult)\n\n\nclass TransformerBlock(layers.Layer):\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n        super().__init__()\n        self.att = layers.MultiHeadAttention(num_heads, embed_dim)\n        self.ffn = keras.Sequential(\n            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n        )\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(rate)\n        self.dropout2 = layers.Dropout(rate)\n\n    def call(self, inputs):\n        input_shape = tf.shape(inputs)\n        batch_size = input_shape[0]\n        seq_len = input_shape[1]\n        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n        attention_output = self.att(inputs, inputs, attention_mask=causal_mask)\n        attention_output = self.dropout1(attention_output)\n        out1 = self.layernorm1(inputs + attention_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output)\n        return self.layernorm2(out1 + ffn_output)","metadata":{"execution":{"iopub.status.busy":"2023-06-25T03:21:39.571538Z","iopub.execute_input":"2023-06-25T03:21:39.571900Z","iopub.status.idle":"2023-06-25T03:21:39.584859Z","shell.execute_reply.started":"2023-06-25T03:21:39.571871Z","shell.execute_reply":"2023-06-25T03:21:39.583802Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class TokenAndPositionEmbedding(layers.Layer):\n    def __init__(self, maxlen, vocab_size, embed_dim):\n        super().__init__()\n        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n\n    def call(self, x):\n        maxlen = tf.shape(x)[-1]\n        positions = tf.range(start=0, limit=maxlen, delta=1)\n        positions = self.pos_emb(positions)\n        x = self.token_emb(x)\n        return x + positions","metadata":{"execution":{"iopub.status.busy":"2023-06-25T03:21:51.028721Z","iopub.execute_input":"2023-06-25T03:21:51.029162Z","iopub.status.idle":"2023-06-25T03:21:51.040469Z","shell.execute_reply.started":"2023-06-25T03:21:51.029122Z","shell.execute_reply":"2023-06-25T03:21:51.039556Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"vocab_size = 20000  # Only consider the top 20k words\nmaxlen = 80  # Max sequence size\nembed_dim = 256  # Embedding size for each token\nnum_heads = 2  # Number of attention heads\nfeed_forward_dim = 256  # Hidden layer size in feed forward network inside transformer\n\n\ndef create_model():\n    inputs = layers.Input(shape=(maxlen,), dtype=tf.int32)\n    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n    x = embedding_layer(inputs)\n    transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim)\n    x = transformer_block(x)\n    outputs = layers.Dense(vocab_size)(x)\n    model = keras.Model(inputs=inputs, outputs=[outputs, x])\n    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n    model.compile(\n        \"adam\", loss=[loss_fn, None],\n    )  # No loss and optimization based on word embeddings from transformer block\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-06-25T03:22:03.561050Z","iopub.execute_input":"2023-06-25T03:22:03.561731Z","iopub.status.idle":"2023-06-25T03:22:03.571268Z","shell.execute_reply.started":"2023-06-25T03:22:03.561690Z","shell.execute_reply":"2023-06-25T03:22:03.570232Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n!tar -xf aclImdb_v1.tar.gz","metadata":{"execution":{"iopub.status.busy":"2023-06-25T03:22:19.890630Z","iopub.execute_input":"2023-06-25T03:22:19.890974Z","iopub.status.idle":"2023-06-25T03:22:28.896719Z","shell.execute_reply.started":"2023-06-25T03:22:19.890945Z","shell.execute_reply":"2023-06-25T03:22:28.895329Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 80.2M  100 80.2M    0     0  60.9M      0  0:00:01  0:00:01 --:--:-- 60.9M\n","output_type":"stream"}]},{"cell_type":"code","source":"batch_size = 128\n\n# The dataset contains each review in a separate text file\n# The text files are present in four different folders\n# Create a list all files\nfilenames = []\ndirectories = [\n    \"aclImdb/train/pos\",\n    \"aclImdb/train/neg\",\n    \"aclImdb/test/pos\",\n    \"aclImdb/test/neg\",\n]\nfor dir in directories:\n    for f in os.listdir(dir):\n        filenames.append(os.path.join(dir, f))\n\nprint(f\"{len(filenames)} files\")\n\n# Create a dataset from text files\nrandom.shuffle(filenames)\ntext_ds = tf.data.TextLineDataset(filenames)\ntext_ds = text_ds.shuffle(buffer_size=256)\ntext_ds = text_ds.batch(batch_size)\n\n\ndef custom_standardization(input_string):\n    \"\"\" Remove html line-break tags and handle punctuation \"\"\"\n    lowercased = tf.strings.lower(input_string)\n    stripped_html = tf.strings.regex_replace(lowercased, \"<br />\", \" \")\n    return tf.strings.regex_replace(stripped_html, f\"([{string.punctuation}])\", r\" \\1\")\n\n\n# Create a vectorization layer and adapt it to the text\nvectorize_layer = TextVectorization(\n    standardize=custom_standardization,\n    max_tokens=vocab_size - 1,\n    output_mode=\"int\",\n    output_sequence_length=maxlen + 1,\n)\nvectorize_layer.adapt(text_ds)\nvocab = vectorize_layer.get_vocabulary()  # To get words back from token indices\n\n\ndef prepare_lm_inputs_labels(text):\n    \"\"\"\n    Shift word sequences by 1 position so that the target for position (i) is\n    word at position (i+1). The model will use all words up till position (i)\n    to predict the next word.\n    \"\"\"\n    text = tf.expand_dims(text, -1)\n    tokenized_sentences = vectorize_layer(text)\n    x = tokenized_sentences[:, :-1]\n    y = tokenized_sentences[:, 1:]\n    return x, y\n\n\ntext_ds = text_ds.map(prepare_lm_inputs_labels)\ntext_ds = text_ds.prefetch(tf.data.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2023-06-25T03:22:40.018226Z","iopub.execute_input":"2023-06-25T03:22:40.018641Z","iopub.status.idle":"2023-06-25T03:22:47.859193Z","shell.execute_reply.started":"2023-06-25T03:22:40.018610Z","shell.execute_reply":"2023-06-25T03:22:47.858167Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"50000 files\n","output_type":"stream"}]},{"cell_type":"code","source":"# Preview a sample from the text_ds dataset\nsample = text_ds.take(10)  # Take one sample from the dataset\n\nfor x, y in sample:\n    # Convert token indices back to words\n    input_words = [vocab[i] for i in x[0].numpy()]\n    target_words = [vocab[i] for i in y[0].numpy()]\n\n    # Print the input and target sequences\n    print(\"Input Sequence:\")\n    print(\" \".join(input_words))\n    print(\"\\nTarget Sequence:\")\n    print(\" \".join(target_words))","metadata":{"execution":{"iopub.status.busy":"2023-06-25T03:30:50.547569Z","iopub.execute_input":"2023-06-25T03:30:50.548123Z","iopub.status.idle":"2023-06-25T03:30:50.887845Z","shell.execute_reply.started":"2023-06-25T03:30:50.548081Z","shell.execute_reply":"2023-06-25T03:30:50.886857Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Input Sequence:\nthere are often badly [UNK] couples (in the general sense of the term ) -in huston 's movies : [UNK] queen \" \"heaven knows mister allison \" [UNK] of heaven \" [UNK] \" . . . \"the barbarian and the geisha \" (check the title ) is another good example ,so to speak .it seems that japanese stuff was trendy at the time as such works as [UNK] \" \" the world of [UNK] wong \" and \"the [UNK] of\n\nTarget Sequence:\nare often badly [UNK] couples (in the general sense of the term ) -in huston 's movies : [UNK] queen \" \"heaven knows mister allison \" [UNK] of heaven \" [UNK] \" . . . \"the barbarian and the geisha \" (check the title ) is another good example ,so to speak .it seems that japanese stuff was trendy at the time as such works as [UNK] \" \" the world of [UNK] wong \" and \"the [UNK] of the\nInput Sequence:\ni always enjoy seeing movies that make you think , and don 't just [UNK] [UNK] the answers to their audience . [UNK] \" is one of these films , and although many reviewers have stated that it is difficult to follow , with a bit of concentration and an open mind i got it . first time . true , it doesn 't compare to other mind [UNK] like \"the usual suspects \" or [UNK] \" , but in\n\nTarget Sequence:\nalways enjoy seeing movies that make you think , and don 't just [UNK] [UNK] the answers to their audience . [UNK] \" is one of these films , and although many reviewers have stated that it is difficult to follow , with a bit of concentration and an open mind i got it . first time . true , it doesn 't compare to other mind [UNK] like \"the usual suspects \" or [UNK] \" , but in its\nInput Sequence:\nexcellent story -telling and cinematography . poignant , biting social commentary . superb effects . well -filmed and acted . however , the parallel action between the present and the travel adventures (though very well done ) at times drags on a little too much (about 3 hrs ) , and over [UNK] the flow of the story . i first read the book as a child , and enjoyed the parts about the giants and the tiny people -\n\nTarget Sequence:\nstory -telling and cinematography . poignant , biting social commentary . superb effects . well -filmed and acted . however , the parallel action between the present and the travel adventures (though very well done ) at times drags on a little too much (about 3 hrs ) , and over [UNK] the flow of the story . i first read the book as a child , and enjoyed the parts about the giants and the tiny people - -\nInput Sequence:\ni watched this . . . .let me [UNK] . . [UNK] through this because i 'm a fan of eva 's . i don 't think this is a flick she 'll put on the back of her head shot photos . i like gangsta [UNK] but this wasn 't even close . the budget couldn 't have been more than a few hundred dollars , and that money was probably spent on the [UNK] . the premise was\n\nTarget Sequence:\nwatched this . . . .let me [UNK] . . [UNK] through this because i 'm a fan of eva 's . i don 't think this is a flick she 'll put on the back of her head shot photos . i like gangsta [UNK] but this wasn 't even close . the budget couldn 't have been more than a few hundred dollars , and that money was probably spent on the [UNK] . the premise was interesting\nInput Sequence:\nsome [UNK] people go down in a cave for some reason and there 's some sort of creature that 's killing them . i usually give a more detailed plot , but i wasn 't paying too much attention to this . overall , it was dull and the only time you 'll be really paying attention is during the action scenes , which the director did wonderful on . the acting is alright , but the characters are so\n\nTarget Sequence:\n[UNK] people go down in a cave for some reason and there 's some sort of creature that 's killing them . i usually give a more detailed plot , but i wasn 't paying too much attention to this . overall , it was dull and the only time you 'll be really paying attention is during the action scenes , which the director did wonderful on . the acting is alright , but the characters are so dull\nInput Sequence:\nafter chicago , i was beginning to lose all respect for richard gere and then along came the flock . there 's just so far a nice smile and a couple of stock facial gestures can get you , but he proved to me that he 's finally gotten hold of his craft and can act with the best of them . [UNK] danes was also super as his [UNK] [UNK] \" . some have suggested there was too much\n\nTarget Sequence:\nchicago , i was beginning to lose all respect for richard gere and then along came the flock . there 's just so far a nice smile and a couple of stock facial gestures can get you , but he proved to me that he 's finally gotten hold of his craft and can act with the best of them . [UNK] danes was also super as his [UNK] [UNK] \" . some have suggested there was too much unnecessary\nInput Sequence:\nthe film is based on kipling 's heroic lines that inspire hollywood 's biggest movie 1939 [UNK] of the [UNK] rhythm of kipling 's most famous 85 lines rises a picture that will become known as the one great movie of the year [UNK] on the score of its armies in battle ,its war elephants ,its bandit hordes ,its terror [UNK] thugs and mystic mountains of india .the picture is bigger still in its scope and sweep ,is thrill and\n\nTarget Sequence:\nfilm is based on kipling 's heroic lines that inspire hollywood 's biggest movie 1939 [UNK] of the [UNK] rhythm of kipling 's most famous 85 lines rises a picture that will become known as the one great movie of the year [UNK] on the score of its armies in battle ,its war elephants ,its bandit hordes ,its terror [UNK] thugs and mystic mountains of india .the picture is bigger still in its scope and sweep ,is thrill and action\nInput Sequence:\nthe frustrating thing about a movie like this , with a true potential for greatness , is that it almost enjoys being heavy -handed . we speak of allegory , of metaphor . . .but the truth is , there 's no getting around the fact that there is absolutely no plot or real character . at a certain point , we most know who the people are . . .even if we never understand where they are going .\n\nTarget Sequence:\nfrustrating thing about a movie like this , with a true potential for greatness , is that it almost enjoys being heavy -handed . we speak of allegory , of metaphor . . .but the truth is , there 's no getting around the fact that there is absolutely no plot or real character . at a certain point , we most know who the people are . . .even if we never understand where they are going . the\nInput Sequence:\nas someone who lived through ,and still remembers that decade vividly ,if the actual '70s had been half this funny and [UNK] [UNK] ,they would have been so much more enjoyable [UNK] kids in that era did not act or behave anything close to as bright -eyed and normal as these kids did .the country 's youth was still under the influence of the hippies and the drug culture all that '60s rebellion that it spawned ,especially in the behavior\n\nTarget Sequence:\nsomeone who lived through ,and still remembers that decade vividly ,if the actual '70s had been half this funny and [UNK] [UNK] ,they would have been so much more enjoyable [UNK] kids in that era did not act or behave anything close to as bright -eyed and normal as these kids did .the country 's youth was still under the influence of the hippies and the drug culture all that '60s rebellion that it spawned ,especially in the behavior department\nInput Sequence:\nwow . . . i suspected this one to be bad . . . but now i find myself just at a loss for words . . . honestly , no words of mine can do this movie any justice . . . i 'll try to say something anyway . . . this truly is one unique gem . one of the worst kind . [UNK] la rue - given his background as an actor - doing a whip\n\nTarget Sequence:\n. . . i suspected this one to be bad . . . but now i find myself just at a loss for words . . . honestly , no words of mine can do this movie any justice . . . i 'll try to say something anyway . . . this truly is one unique gem . one of the worst kind . [UNK] la rue - given his background as an actor - doing a whip -fight\n","output_type":"stream"}]},{"cell_type":"code","source":"class TextGenerator(keras.callbacks.Callback):\n    \"\"\"A callback to generate text from a trained model.\n    1. Feed some starting prompt to the model\n    2. Predict probabilities for the next token\n    3. Sample the next token and add it to the next input\n\n    Arguments:\n        max_tokens: Integer, the number of tokens to be generated after prompt.\n        start_tokens: List of integers, the token indices for the starting prompt.\n        index_to_word: List of strings, obtained from the TextVectorization layer.\n        top_k: Integer, sample from the `top_k` token predictions.\n        print_every: Integer, print after this many epochs.\n    \"\"\"\n\n    def __init__(\n        self, max_tokens, start_tokens, index_to_word, top_k=10, print_every=1\n    ):\n        self.max_tokens = max_tokens\n        self.start_tokens = start_tokens\n        self.index_to_word = index_to_word\n        self.print_every = print_every\n        self.k = top_k\n\n    def sample_from(self, logits):\n        logits, indices = tf.math.top_k(logits, k=self.k, sorted=True)\n        indices = np.asarray(indices).astype(\"int32\")\n        preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n        preds = np.asarray(preds).astype(\"float32\")\n        return np.random.choice(indices, p=preds)\n\n    def detokenize(self, number):\n        return self.index_to_word[number]\n\n    def on_epoch_end(self, epoch, logs=None):\n        start_tokens = [_ for _ in self.start_tokens]\n        if (epoch + 1) % self.print_every != 0:\n            return\n        num_tokens_generated = 0\n        tokens_generated = []\n        while num_tokens_generated <= self.max_tokens:\n            pad_len = maxlen - len(start_tokens)\n            sample_index = len(start_tokens) - 1\n            if pad_len < 0:\n                x = start_tokens[:maxlen]\n                sample_index = maxlen - 1\n            elif pad_len > 0:\n                x = start_tokens + [0] * pad_len\n            else:\n                x = start_tokens\n            x = np.array([x])\n            y, _ = self.model.predict(x)\n            sample_token = self.sample_from(y[0][sample_index])\n            tokens_generated.append(sample_token)\n            start_tokens.append(sample_token)\n            num_tokens_generated = len(tokens_generated)\n        txt = \" \".join(\n            [self.detokenize(_) for _ in self.start_tokens + tokens_generated]\n        )\n        print(f\"generated text:\\n{txt}\\n\")\n        \n        \n\n# Tokenize starting prompt\nword_to_index = {}\nfor index, word in enumerate(vocab):\n    word_to_index[word] = index\n","metadata":{"execution":{"iopub.status.busy":"2023-06-25T03:37:22.672857Z","iopub.execute_input":"2023-06-25T03:37:22.673655Z","iopub.status.idle":"2023-06-25T03:37:22.715420Z","shell.execute_reply.started":"2023-06-25T03:37:22.673613Z","shell.execute_reply":"2023-06-25T03:37:22.714189Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"start_prompt = \"this movie is\"\nstart_prompt = \"why is it\"\n\nstart_tokens = [word_to_index.get(_, 1) for _ in start_prompt.split()]\nnum_tokens_generated = 40\ntext_gen_callback = TextGenerator(num_tokens_generated, start_tokens, vocab)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = create_model()\n\nmodel.fit(text_ds, verbose=2, epochs=1, callbacks=[text_gen_callback])","metadata":{"execution":{"iopub.status.busy":"2023-06-25T03:37:24.374776Z","iopub.execute_input":"2023-06-25T03:37:24.376444Z","iopub.status.idle":"2023-06-25T03:38:25.476151Z","shell.execute_reply.started":"2023-06-25T03:37:24.376409Z","shell.execute_reply":"2023-06-25T03:38:25.475087Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"1/1 [==============================] - 0s 214ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\ngenerated text:\nthis movie is a film . the only thing about a lot of movies of the [UNK] and the end . it 's the movie is just to say , the acting was the movie is the story of a little girl [UNK] )\n\n391/391 - 61s - loss: 5.5722 - dense_11_loss: 5.5722 - 61s/epoch - 156ms/step\n","output_type":"stream"},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7ea4341cf880>"},"metadata":{}}]},{"cell_type":"code","source":"text_gen_callback.on_epoch_end(0)","metadata":{},"execution_count":null,"outputs":[]}]}