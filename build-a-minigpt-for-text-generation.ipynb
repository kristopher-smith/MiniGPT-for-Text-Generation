{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# MiniGPT For Generating Synthetic Text Data\n\nby Kris Smith","metadata":{}},{"cell_type":"markdown","source":"# ***WARNING*** \n\n## The data required to train the model for this task is known to be vulgar, offensive, toxic, racist, and otherwise not pleasant.","metadata":{}},{"cell_type":"markdown","source":"## Problem Statement\n\nToxic comments online come in many forms and in many arenas. There are currently several ways to mitigate these comments(for those organizations who wish to do so). Some of these ways include human moderators, and training machine learning models to detect toxicity in online comments.\n\nThe issue with human moderators is that some of these platforms have grown so large so quickly that there are not nearly enough moderators to achieve any sense of control for most of these comments. The shear volume of toxicity and bots online makes it unrealistic to think we could do this job with humans at this point.\n\nMany companies are employing machine learning to assist with identifying toxic comments online automatically. The problem with this approach is the lack of labeled training data to train the models on.\n\nThis is the problem I am going to solve using generative deep learning techniques. ","metadata":{}},{"cell_type":"markdown","source":"## References\n\n* [Improving Language Understanding by Generative Pre-Training](https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035)\n\n* [Language Models are Unsupervised Multitask Learners](https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe)\n\n* [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)\n\n* Many of the ideas and code were adapted from this Keras resource: https://keras.io/examples/generative/text_generation_with_miniature_gpt/","metadata":{}},{"cell_type":"markdown","source":"## Import Libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport string\nimport random\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\n\nfrom nltk import ngrams\nfrom collections import Counter\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import TextVectorization\n\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-25T18:38:33.123369Z","iopub.execute_input":"2023-06-25T18:38:33.123746Z","iopub.status.idle":"2023-06-25T18:38:33.134474Z","shell.execute_reply.started":"2023-06-25T18:38:33.123713Z","shell.execute_reply":"2023-06-25T18:38:33.133337Z"},"trusted":true},"execution_count":71,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n","output_type":"stream"},{"execution_count":71,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"markdown","source":"## Data\n\nThe data I will be using to train the generative model was released on Kaggle as part of an ongoing series of competitions sponsored by the [Google company Jigsaw](https://en.wikipedia.org/wiki/Jigsaw_(company)).\n\nThe data consists of online comments with various severity levels of toxicity. There are versions of these comments labeled by human annotators wherein they label each comment as toxic or not, or other sets where they were labeled as different categories of toxic such as hatespeech, racist/sexist, obscene, etc. Although these are the labeled datasets we would be adding the synthetic data to in order to create more training data, for this task of simply generating similar text data we will only focus on the comments themselves.\n\nThe data provided by this competition includes a total of `14,251` unique toxic comments. Theses are the comments I will use to train the generative model with.","metadata":{}},{"cell_type":"markdown","source":"## EDA\n\nThe data came in two different files.\n\n1) Comments to score: This acts as a test dataset of comments for scoring after the model was trained.\n\n2) Validation data: This was the training data for the competition wherein there are two columns. One column labeled less toxic was a comment which human annotators labeled as less toxic than its more toxic counterpart in the other column. There was no actual training data where a comment was paired with its severity rating. The models were trained using creative techniques with the validation data and other classification data sets to train a model which predicted severity of comments.\n\nSince for our purposes we are only interested in the actual text comments themselves, I will only be using those columns from these datasources.\n\nI start by reading them all into pandas dataframes, isolating the text columns from each one, and stacking them all together so we have a single column of text when it is all said and done.\n","metadata":{}},{"cell_type":"code","source":"data1 = pd.read_csv('../input/jigsaw-toxic-severity-rating/comments_to_score.csv')\ndata1.info()\n\n## Isolate only text column\ndata1 = data1['text']\n\ndata1.sample(6)","metadata":{"execution":{"iopub.status.busy":"2023-06-25T18:07:10.474502Z","iopub.execute_input":"2023-06-25T18:07:10.474894Z","iopub.status.idle":"2023-06-25T18:07:10.531099Z","shell.execute_reply.started":"2023-06-25T18:07:10.474864Z","shell.execute_reply":"2023-06-25T18:07:10.530135Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 7537 entries, 0 to 7536\nData columns (total 2 columns):\n #   Column      Non-Null Count  Dtype \n---  ------      --------------  ----- \n 0   comment_id  7537 non-null   int64 \n 1   text        7537 non-null   object\ndtypes: int64(1), object(1)\nmemory usage: 117.9+ KB\n","output_type":"stream"},{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"108     \"Rabbi Mordechi Eliyahu, the past Chief Rabbi ...\n308     I dont not vandelise this place. Its the other...\n3288    why am I not aloud to put whatever i want on m...\n1695    Pretty humorous comment from someone who promo...\n7248    Your full of shit. What, is the big, bad Zion ...\n3552    SOME PEOLE ARE JUST FUKING COMPKETE ASSHOLE LI...\nName: text, dtype: object"},"metadata":{}}]},{"cell_type":"markdown","source":"We can see the comments to score was the test file which contained only comments and their corresponding id's","metadata":{}},{"cell_type":"code","source":"data2 = pd.read_csv('../input/jigsaw-toxic-severity-rating/validation_data.csv')\ndata2.info()\n\ndata2.sample(6)","metadata":{"execution":{"iopub.status.busy":"2023-06-25T18:07:11.217241Z","iopub.execute_input":"2023-06-25T18:07:11.218169Z","iopub.status.idle":"2023-06-25T18:07:11.503207Z","shell.execute_reply.started":"2023-06-25T18:07:11.218124Z","shell.execute_reply":"2023-06-25T18:07:11.502154Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 30108 entries, 0 to 30107\nData columns (total 3 columns):\n #   Column      Non-Null Count  Dtype \n---  ------      --------------  ----- \n 0   worker      30108 non-null  int64 \n 1   less_toxic  30108 non-null  object\n 2   more_toxic  30108 non-null  object\ndtypes: int64(1), object(2)\nmemory usage: 705.8+ KB\n","output_type":"stream"},{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"       worker                                         less_toxic  \\\n16915     252   Hey! \\n\\nHello. My name is anon. Don't fuck w...   \n205       541  I BLOCKED REVERS! I BLOCKED REVERS! I BLOCKED ...   \n18108     397  That was a mistake the stupid asses at Funimat...   \n17475     540  \"\\n\\nYour accusations are false and malicious....   \n3695      277   Go on, have me blocked. It won't change anyth...   \n2813      317  \"\\n\\n{{#ifeq:{μ|μ|Category:Miscellaneous pages...   \n\n                                              more_toxic  \n16915   Unregistered punk think's he's so cool \\n\\nTh...  \n205     Category Sikh history - User Sikh History \\n\\...  \n18108   Grow some pubic hair while you're at it!\\n\\nT...  \n17475  Then you're a communist, because you are not Z...  \n3695   \"Also, in your commenting about my article, yo...  \n2813   Speaking without knowing what I'm talking abou...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>worker</th>\n      <th>less_toxic</th>\n      <th>more_toxic</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>16915</th>\n      <td>252</td>\n      <td>Hey! \\n\\nHello. My name is anon. Don't fuck w...</td>\n      <td>Unregistered punk think's he's so cool \\n\\nTh...</td>\n    </tr>\n    <tr>\n      <th>205</th>\n      <td>541</td>\n      <td>I BLOCKED REVERS! I BLOCKED REVERS! I BLOCKED ...</td>\n      <td>Category Sikh history - User Sikh History \\n\\...</td>\n    </tr>\n    <tr>\n      <th>18108</th>\n      <td>397</td>\n      <td>That was a mistake the stupid asses at Funimat...</td>\n      <td>Grow some pubic hair while you're at it!\\n\\nT...</td>\n    </tr>\n    <tr>\n      <th>17475</th>\n      <td>540</td>\n      <td>\"\\n\\nYour accusations are false and malicious....</td>\n      <td>Then you're a communist, because you are not Z...</td>\n    </tr>\n    <tr>\n      <th>3695</th>\n      <td>277</td>\n      <td>Go on, have me blocked. It won't change anyth...</td>\n      <td>\"Also, in your commenting about my article, yo...</td>\n    </tr>\n    <tr>\n      <th>2813</th>\n      <td>317</td>\n      <td>\"\\n\\n{{#ifeq:{μ|μ|Category:Miscellaneous pages...</td>\n      <td>Speaking without knowing what I'm talking abou...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"This was the data provided to validate the models performance during training. The three columns are workers(annotators) and the other two are text columns which we will use both to train our generative model with.","metadata":{}},{"cell_type":"markdown","source":"#### Combine all columns into a single column","metadata":{}},{"cell_type":"code","source":"## Isolate text column\ndata2 = data2['more_toxic']\n\n## Isolate text column\ndata3 = pd.read_csv('../input/jigsaw-toxic-severity-rating/validation_data.csv')\ndata3 = data3['less_toxic']\n\ntext_column = pd.concat([data1, data2, data3], axis=0, ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2023-06-25T18:07:16.148674Z","iopub.execute_input":"2023-06-25T18:07:16.149401Z","iopub.status.idle":"2023-06-25T18:07:16.389135Z","shell.execute_reply.started":"2023-06-25T18:07:16.149367Z","shell.execute_reply":"2023-06-25T18:07:16.387580Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":"#### Check for duplicates","metadata":{}},{"cell_type":"code","source":"text_column.value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-06-25T18:07:35.870830Z","iopub.execute_input":"2023-06-25T18:07:35.871237Z","iopub.status.idle":"2023-06-25T18:07:35.918635Z","shell.execute_reply.started":"2023-06-25T18:07:35.871208Z","shell.execute_reply":"2023-06-25T18:07:35.917720Z"},"trusted":true},"execution_count":57,"outputs":[{"execution_count":57,"output_type":"execute_result","data":{"text/plain":" sorry i jumped to conclusions \\n\\non christian terrorism article man, I don't agree with you, and I want you to go and listen to 'prophet of doom' (now in audio format) as it is good. But I was wrong to be so rude. It is not the Southern European way.                                                                                                                                                                               19\nthis irishtom guy is turning every article into an ad for islam                                                                                                                                                                                                                                                                                                                                                                            19\nYou are not sorry one damned bit.  You have yet to refute what I have written.  All you do is pass the insults as if it were salt on the dinner table.  This is on every article in which we disagree.  If you have something useful and constructive to say, then don't be a harpy troll.                                                                                                                                                 19\n YOUR BIASED! \\n\\nPLEASE OTHER THAN HIDE BEHIND WK RULES\\n\\nacutally IDENTITY THE OFFESNES COMMIMITED!\\n\\nYOU JUST SAID you dont care about my OPINIONS!..yet the opinions that where QUOTED WHERE FROM THE REFERENCES YOU HAD ACCEPTED!!\\n\\nLOL...\\n\\nSO in which case i am formally complaining about YOU AND YOUR BIASED STANCE!\\n\\nALL MY REFERNCES HAVE ISBN NUMBERS, YEAR AND PUBLISHERS!\\n\\nYOU ARE PROTECTING YOUR BIASED VIEW!    16\nI erased your cuss word\\nFrom: some random person out there in the world                                                                                                                                                                                                                                                                                                                                                                   16\n                                                                                                                                                                                                                                                                                                                                                                                                                                           ..\nHey\\n\\nI bet you Quinsareth are gay and like telling lies to your mother.                                                                                                                                                                                                                                                                                                                                                                   2\nVandalism on Muhammad page\\n\\nPerhaps that was the wrong way to deal with it, but                                                                                                                                                                                                                                                                                                                                                           2\n Thank You \\n\\nHey Nishkid I really appreciate the unblock.  Once again I apologize for any vandalism I caused on user pages and I have read Wikipedia's user policy.  Thank You!!!                                                                                                                                                                                                                                                         2\n Your low self-esteem \\n\\nI see you have such a low self-esteem that you have to warn others not to attack you.4.130.134.233                                                                                                                                                                                                                                                                                                                2\nVANDALISE MY ASS ==\\n\\n==                                                                                                                                                                                                                                                                                                                                                                                                                   2\nLength: 14251, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"It looks like between the data provided for the competition there are many duplicates. However we can see that some comments are reused many more times than other comments. For example the most used comments were repeated `19` times in the datasets while others only `2` times. \n\nSince the duplications are not balanced if we left the data like this I am afraid we would be biasing the model towards the comments which were present more in the data. \n\nI will remove all duplicate comments.","metadata":{}},{"cell_type":"code","source":"print(f\"Total numer of comments in text data = {len(text_column)}\")\nprint(f\"Numer of unique comments in text data = {len(text_column.unique())}\")\n\ntext_column = text_column.drop_duplicates()\nprint(\"Duplicate comments dropped\")","metadata":{"execution":{"iopub.status.busy":"2023-06-25T18:10:15.937979Z","iopub.execute_input":"2023-06-25T18:10:15.938377Z","iopub.status.idle":"2023-06-25T18:10:16.005847Z","shell.execute_reply.started":"2023-06-25T18:10:15.938347Z","shell.execute_reply":"2023-06-25T18:10:16.004678Z"},"trusted":true},"execution_count":58,"outputs":[{"name":"stdout","text":"Total numer of comments in text data = 67753\nNumer of unique comments in text data = 14251\nDuplicate comments dropped\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Exploring the toxic comments","metadata":{}},{"cell_type":"code","source":"data = pd.DataFrame()\ndata['text'] = text_column\ndata = data.sample(100)\n\n# Function to calculate word count\ndef count_words(text):\n    words = nltk.word_tokenize(text)\n    return len(words)\n\n# Function to calculate verb count\ndef count_verbs(text):\n    words = nltk.word_tokenize(text)\n    tagged_words = nltk.pos_tag(words)\n    verb_count = len([word for word, tag in tagged_words if tag.startswith('V')])\n    return verb_count\n\n# Function to calculate noun count\ndef count_nouns(text):\n    words = nltk.word_tokenize(text)\n    tagged_words = nltk.pos_tag(words)\n    noun_count = len([word for word, tag in tagged_words if tag.startswith('N')])\n    return noun_count\n\n# Add word count column\ndata['word_count'] = data['text'].apply(count_words)\n\n# Add verb count column\ndata['verb_count'] = data['text'].apply(count_verbs)\n\n# Add noun count column\ndata['noun_count'] = data['text'].apply(count_nouns)\n\ndata.describe()","metadata":{"execution":{"iopub.status.busy":"2023-06-25T18:38:59.004415Z","iopub.execute_input":"2023-06-25T18:38:59.004869Z","iopub.status.idle":"2023-06-25T18:39:00.164771Z","shell.execute_reply.started":"2023-06-25T18:38:59.004832Z","shell.execute_reply":"2023-06-25T18:39:00.163670Z"},"trusted":true},"execution_count":73,"outputs":[{"execution_count":73,"output_type":"execute_result","data":{"text/plain":"       word_count  verb_count  noun_count\ncount  100.000000  100.000000  100.000000\nmean    74.500000   12.040000   17.620000\nstd    125.455976   20.452418   27.930209\nmin      2.000000    0.000000    1.000000\n25%     16.750000    2.000000    4.000000\n50%     33.000000    6.000000    9.000000\n75%     70.000000   13.000000   17.500000\nmax    892.000000  147.000000  203.000000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word_count</th>\n      <th>verb_count</th>\n      <th>noun_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>74.500000</td>\n      <td>12.040000</td>\n      <td>17.620000</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>125.455976</td>\n      <td>20.452418</td>\n      <td>27.930209</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>2.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>16.750000</td>\n      <td>2.000000</td>\n      <td>4.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>33.000000</td>\n      <td>6.000000</td>\n      <td>9.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>70.000000</td>\n      <td>13.000000</td>\n      <td>17.500000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>892.000000</td>\n      <td>147.000000</td>\n      <td>203.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"ax = data['word_count'].plot(kind='kde')\ndata['verb_count'].plot(kind='kde', ax=ax)\ndata['noun_count'].plot(kind='kde', ax=ax)\n\nax.legend(['Word Count', 'Verb Count', 'Noun Count'])\nax.set_title('Distribution of Word Count, Verb Count, and Noun Count')\nax.set_xlabel('Count')\nax.set_ylabel('Density')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-25T18:50:41.508633Z","iopub.execute_input":"2023-06-25T18:50:41.509347Z","iopub.status.idle":"2023-06-25T18:50:41.879617Z","shell.execute_reply.started":"2023-06-25T18:50:41.509312Z","shell.execute_reply":"2023-06-25T18:50:41.878688Z"},"trusted":true},"execution_count":84,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"markdown","source":"We can see that these comments on average are quite short in length and contain more nouns than verbs on average.\n\nSince we have not done any cleaning of the data yet these distributions are not exact as the nltk package is not currently looking for misspelled words or different versions of word spellings which are used online sometimes.\n\nFor example if a user knows that the platform they are on has limitations on language than they may spell a profane word to try to fool any auto detecting systems such as `Fuck==>Fxck, F*ck, Fukk, Fuuu*uukk`, etc.\n\nTherefore these counts will not detect all nouns and verbs but should give a decent sample.\n\nKnowing the underlying distributions of some of these features is important because after the synthetic data is generated we would most likely want it to follow the same distributions for these attributes of the text. ","metadata":{}},{"cell_type":"markdown","source":"### Looking at the most common N-grams","metadata":{}},{"cell_type":"code","source":"# Tokenize the text into words\ndata['words'] = data['text'].apply(nltk.word_tokenize)\n\n# Get bigrams and trigrams for each row\ndata['bigrams']   = data['words'].apply(lambda x: list(ngrams(x, 2)))\ndata['trigrams']  = data['words'].apply(lambda x: list(ngrams(x, 3)))\n# data['quadgrams'] = data['words'].apply(lambda x: list(ngrams(x, 4)))\n\n# Count the occurrences of bigrams and trigrams\nbigram_counts   = Counter([gram for grams in data['bigrams'] for gram in grams])\ntrigram_counts  = Counter([gram for grams in data['trigrams'] for gram in grams])\n# quadgram_counts = Counter([gram for grams in data['quadgrams'] for gram in grams])\n\n# Get the most common bigrams, trigrams, and quadgrams\nmost_common_bigrams   = bigram_counts.most_common(50)\nmost_common_trigrams  = trigram_counts.most_common(50)\n# most_common_quadgrams = quadgram_counts.most_common(50)\n\ndf_common_grams = pd.DataFrame()\ndf_common_grams['bigrams']   = most_common_bigrams\ndf_common_grams['trigrams']  = most_common_trigrams\n# df_common_grams['quadgrams'] = most_common_quadgrams\n\n# # Display the results\n# print('Most common bigrams:')\n# for bigram, count in most_common_bigrams:\n#     print(' '.join(bigram), count)\n\n# print('\\nMost common trigrams:')\n# for trigram, count in most_common_trigrams:\n#     print(' '.join(trigram), count)\n    \n# print('\\nMost common quadgrams:')\n# for quadgram, count in most_common_quadgrams:\n#     print(' '.join(quadgram), count)\n\n\ndf_common_grams.iloc[:, :]","metadata":{"execution":{"iopub.status.busy":"2023-06-25T18:49:45.191502Z","iopub.execute_input":"2023-06-25T18:49:45.191917Z","iopub.status.idle":"2023-06-25T18:49:45.383740Z","shell.execute_reply.started":"2023-06-25T18:49:45.191886Z","shell.execute_reply":"2023-06-25T18:49:45.382716Z"},"trusted":true},"execution_count":82,"outputs":[{"execution_count":82,"output_type":"execute_result","data":{"text/plain":"               bigrams                       trigrams\n0       (('', ''), 77)                ((!, !, !), 12)\n1       ((``, ''), 52)             (('', '', ''), 12)\n2      ((of, the), 39)               ((., '', ''), 9)\n3         ((!, !), 24)               ((., WRT, :), 9)\n4         ((., I), 22)              ((WRT, :, ``), 9)\n5       ((., The), 22)               ((:, ``, ''), 9)\n6      ((in, the), 19)               (('', '', .), 8)\n7      ((do, n't), 15)               ((., It, is), 8)\n8      ((is, not), 14)             ((., This, is), 6)\n9        ((., It), 14)              ((., If, you), 6)\n10      ((,, and), 13)               ((,, ``, ''), 6)\n11       ((is, a), 12)        (('', '', National), 6)\n12      ((,, you), 12)         ((the, Jews, were), 6)\n13      ((., You), 12)  ((blocked, from, editing), 5)\n14      ((,, but), 12)               (((, UTC, )), 5)\n15   ((the, Jews), 12)           ((FAG, FAG, FAG), 5)\n16    ((and, the), 11)              (('', '', is), 4)\n17     ((if, you), 11)           ((you, will, be), 4)\n18        ((,, I), 10)            (('', '', with), 4)\n19       ((., ''), 10)               ((?, '', ''), 4)\n20       ((., If), 10)         ((was, a, Kleagle), 4)\n21      ((It, is), 10)  ((ethnically, Chinese, .), 3)\n22        ((I, am), 9)            ((., You, have), 3)\n23       ((to, be), 9)             ((the, ``, ''), 3)\n24     ((you, are), 9)           (((, like, User), 3)\n25       ((., WRT), 9)           ((like, User, :), 3)\n26       ((WRT, :), 9)        ((User, :, Crossmr), 3)\n27        ((:, ``), 9)           ((are, right, ,), 3)\n28        ((WP, :), 9)           ((you, want, to), 3)\n29         ((), .), 8)             ((this, is, a), 3)\n30        ((it, .), 8)         ((Please, do, not), 3)\n31        (('', .), 8)             ((to, be, the), 3)\n32      ((I, have), 8)              ((,, if, you), 3)\n33      ((do, not), 8)        ((``, '', national), 3)\n34      ((do, you), 7)              (('', '', in), 3)\n35      ((., This), 7)           (('', '', which), 3)\n36     ((This, is), 7)               ((,, it, 's), 3)\n37      ((User, :), 7)            ((,, you, will), 3)\n38    ((that, you), 7)          ((do, you, think), 3)\n39        ((,, it), 7)              ((in, ``, ''), 3)\n40      ((If, you), 7)             ((It, is, not), 3)\n41     ((want, to), 7)          ((be, removed, .), 3)\n42      ((on, the), 7)              ((is, not, a), 3)\n43     ((will, be), 7)               ((WP, :, RS), 3)\n44       ((was, a), 7)             ((,, you, 're), 3)\n45  ((the, Church), 7)          ((?, All, Sunnis), 3)\n46     ((,, which), 6)          ((All, Sunnis, .), 3)\n47       ((to, do), 6)           ((do, n't, want), 3)\n48        ((., So), 6)           ((This, is, not), 3)\n49   ((about, the), 6)            ((all, of, the), 3)","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>bigrams</th>\n      <th>trigrams</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>(('', ''), 77)</td>\n      <td>((!, !, !), 12)</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>((``, ''), 52)</td>\n      <td>(('', '', ''), 12)</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>((of, the), 39)</td>\n      <td>((., '', ''), 9)</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>((!, !), 24)</td>\n      <td>((., WRT, :), 9)</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>((., I), 22)</td>\n      <td>((WRT, :, ``), 9)</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>((., The), 22)</td>\n      <td>((:, ``, ''), 9)</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>((in, the), 19)</td>\n      <td>(('', '', .), 8)</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>((do, n't), 15)</td>\n      <td>((., It, is), 8)</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>((is, not), 14)</td>\n      <td>((., This, is), 6)</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>((., It), 14)</td>\n      <td>((., If, you), 6)</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>((,, and), 13)</td>\n      <td>((,, ``, ''), 6)</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>((is, a), 12)</td>\n      <td>(('', '', National), 6)</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>((,, you), 12)</td>\n      <td>((the, Jews, were), 6)</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>((., You), 12)</td>\n      <td>((blocked, from, editing), 5)</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>((,, but), 12)</td>\n      <td>(((, UTC, )), 5)</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>((the, Jews), 12)</td>\n      <td>((FAG, FAG, FAG), 5)</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>((and, the), 11)</td>\n      <td>(('', '', is), 4)</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>((if, you), 11)</td>\n      <td>((you, will, be), 4)</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>((,, I), 10)</td>\n      <td>(('', '', with), 4)</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>((., ''), 10)</td>\n      <td>((?, '', ''), 4)</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>((., If), 10)</td>\n      <td>((was, a, Kleagle), 4)</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>((It, is), 10)</td>\n      <td>((ethnically, Chinese, .), 3)</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>((I, am), 9)</td>\n      <td>((., You, have), 3)</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>((to, be), 9)</td>\n      <td>((the, ``, ''), 3)</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>((you, are), 9)</td>\n      <td>(((, like, User), 3)</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>((., WRT), 9)</td>\n      <td>((like, User, :), 3)</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>((WRT, :), 9)</td>\n      <td>((User, :, Crossmr), 3)</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>((:, ``), 9)</td>\n      <td>((are, right, ,), 3)</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>((WP, :), 9)</td>\n      <td>((you, want, to), 3)</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>((), .), 8)</td>\n      <td>((this, is, a), 3)</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>((it, .), 8)</td>\n      <td>((Please, do, not), 3)</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>(('', .), 8)</td>\n      <td>((to, be, the), 3)</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>((I, have), 8)</td>\n      <td>((,, if, you), 3)</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>((do, not), 8)</td>\n      <td>((``, '', national), 3)</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>((do, you), 7)</td>\n      <td>(('', '', in), 3)</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>((., This), 7)</td>\n      <td>(('', '', which), 3)</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>((This, is), 7)</td>\n      <td>((,, it, 's), 3)</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>((User, :), 7)</td>\n      <td>((,, you, will), 3)</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>((that, you), 7)</td>\n      <td>((do, you, think), 3)</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>((,, it), 7)</td>\n      <td>((in, ``, ''), 3)</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>((If, you), 7)</td>\n      <td>((It, is, not), 3)</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>((want, to), 7)</td>\n      <td>((be, removed, .), 3)</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>((on, the), 7)</td>\n      <td>((is, not, a), 3)</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>((will, be), 7)</td>\n      <td>((WP, :, RS), 3)</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>((was, a), 7)</td>\n      <td>((,, you, 're), 3)</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>((the, Church), 7)</td>\n      <td>((?, All, Sunnis), 3)</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>((,, which), 6)</td>\n      <td>((All, Sunnis, .), 3)</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>((to, do), 6)</td>\n      <td>((do, n't, want), 3)</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>((., So), 6)</td>\n      <td>((This, is, not), 3)</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>((about, the), 6)</td>\n      <td>((all, of, the), 3)</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"We can see the initial 10 or so most common bi-grams and tri-grams are repetitive punctuation marks.\n\nTraditionally these would be cleaned and removed when training models for NLP tasks, however due to the nature of this work many of these traditional techniques will limit the models ability to predict toxicity as well as with clean text.\n\nI happened to have competed in this competition and one thing all of us learned was that leaving capital letters and punctuation improved the models ability to infer toxicity and especially levels of toxicity. \n\nFor example a phrase such as:\n\n`Are you kidding?`\n\nConveys a much different meaning than the same words but put this way:\n\n`ARE YOU KIDDING!!!??`\n\nWhen training sentiment based models or models where feeling and emotion is being conveyed in some way such as toxicity of comments, it is more than just the raw content of the words alone which gives the meaning. The puncuation and capitalizations are very expressive forms of language and as such for these problems do better left in the data.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"\ndef custom_standardization(input_string):\n    \"\"\" Remove html line-break tags and handle punctuation \"\"\"\n    lowercased = tf.strings.lower(input_string)\n    stripped_html = tf.strings.regex_replace(lowercased, \"<br />\", \" \")\n    return tf.strings.regex_replace(stripped_html, f\"([{string.punctuation}])\", r\" \\1\")\n\n\n# Create a vectorization layer and adapt it to the text\nvectorize_layer = TextVectorization(\n    standardize=custom_standardization,\n    max_tokens=vocab_size - 1,\n    output_mode=\"int\",\n    output_sequence_length=maxlen + 1,\n)\nvectorize_layer.adapt(text_ds)\nvocab = vectorize_layer.get_vocabulary()  # To get words back from token indices\n\n## Functoin to create target column\ndef prepare_lm_inputs_labels(text):\n    \"\"\"\n    Shift word sequences by 1 position so that the target for position (i) is\n    word at position (i+1). The model will use all words up till position (i)\n    to predict the next word.\n    \"\"\"\n    text = tf.expand_dims(text, -1)\n    tokenized_sentences = vectorize_layer(text)\n    x = tokenized_sentences[:, :-1]\n    y = tokenized_sentences[:, 1:]\n    return x, y\n\n\n# Create a dataset from the pandas column\ntext_ds = tf.data.Dataset.from_tensor_slices(text_column)\n\n# Shuffle and batch the dataset\ntext_ds = text_ds.shuffle(buffer_size=256)\ntext_ds = text_ds.batch(batch_size)\n\n# Create a vectorization layer and adapt it to the text\nvectorize_layer = TextVectorization(\n    standardize=custom_standardization,\n    max_tokens=vocab_size - 1,\n    output_mode=\"int\",\n    output_sequence_length=maxlen + 1,\n)\n\nvectorize_layer.adapt(text_ds)\nvocab = vectorize_layer.get_vocabulary()  # To get words back from token indices\n\ntext_ds = text_ds.map(prepare_lm_inputs_labels)\ntext_ds = text_ds.prefetch(tf.data.AUTOTUNE)\n\n\n\n# Preview a sample from the text_ds dataset\nsample = text_ds.take(5)  # Take one sample from the dataset\n\nfor x, y in sample:\n    # Convert token indices back to words\n    input_words  = [vocab[i] for i in x[0].numpy()]\n    target_words = [vocab[i] for i in y[0].numpy()]\n\n    # Print the input and target sequences\n    print(\"Input Sequence:\")\n    print(\" \".join(input_words))\n    print(\"\\nTarget Sequence:\")\n    print(\" \".join(target_words))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Cell below was for loading in and preprocessing the IMBD movie quotes dataset. This is the dataset I tested this approach on first.","metadata":{}},{"cell_type":"code","source":"# !curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n# !tar -xf aclImdb_v1.tar.gz\n\n# batch_size = 128\n\n# # The dataset contains each review in a separate text file\n# # The text files are present in four different folders\n# # Create a list all files\n# filenames = []\n# directories = [\n#     \"aclImdb/train/pos\",\n#     \"aclImdb/train/neg\",\n#     \"aclImdb/test/pos\",\n#     \"aclImdb/test/neg\",\n# ]\n# for dir in directories:\n#     for f in os.listdir(dir):\n#         filenames.append(os.path.join(dir, f))\n\n# print(f\"{len(filenames)} files\")\n\n# # Create a dataset from text files\n# random.shuffle(filenames)\n# text_ds = tf.data.TextLineDataset(filenames)\n# text_ds = text_ds.shuffle(buffer_size=256)\n# text_ds = text_ds.batch(batch_size)\n\n# def custom_standardization(input_string):\n#     \"\"\" Remove html line-break tags and handle punctuation \"\"\"\n#     lowercased = tf.strings.lower(input_string)\n#     stripped_html = tf.strings.regex_replace(lowercased, \"<br />\", \" \")\n#     return tf.strings.regex_replace(stripped_html, f\"([{string.punctuation}])\", r\" \\1\")\n\n\n# # Create a vectorization layer and adapt it to the text\n# vectorize_layer = TextVectorization(\n#     standardize=custom_standardization,\n#     max_tokens=vocab_size - 1,\n#     output_mode=\"int\",\n#     output_sequence_length=maxlen + 1,\n# )\n# vectorize_layer.adapt(text_ds)\n# vocab = vectorize_layer.get_vocabulary()  # To get words back from token indices\n\n# ## Functoin to create target column\n# def prepare_lm_inputs_labels(text):\n#     \"\"\"\n#     Shift word sequences by 1 position so that the target for position (i) is\n#     word at position (i+1). The model will use all words up till position (i)\n#     to predict the next word.\n#     \"\"\"\n#     text = tf.expand_dims(text, -1)\n#     tokenized_sentences = vectorize_layer(text)\n#     x = tokenized_sentences[:, :-1]\n#     y = tokenized_sentences[:, 1:]\n#     return x, y\n\n\n# text_ds = text_ds.map(prepare_lm_inputs_labels)\n# text_ds = text_ds.prefetch(tf.data.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2023-06-25T16:54:39.672498Z","iopub.execute_input":"2023-06-25T16:54:39.672899Z","iopub.status.idle":"2023-06-25T16:54:55.769014Z","shell.execute_reply.started":"2023-06-25T16:54:39.672868Z","shell.execute_reply":"2023-06-25T16:54:55.767732Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 80.2M  100 80.2M    0     0  9419k      0  0:00:08  0:00:08 --:--:-- 17.8M\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Implement a Transformer block as a layer","metadata":{}},{"cell_type":"code","source":"def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n    \"\"\"\n    Mask the upper half of the dot product matrix in self attention.\n    This prevents flow of information from future tokens to current token.\n    1's in the lower triangle, counting from the lower right corner.\n    \"\"\"\n    i = tf.range(n_dest)[:, None]\n    j = tf.range(n_src)\n    m = i >= j - n_src + n_dest\n    mask = tf.cast(m, dtype)\n    mask = tf.reshape(mask, [1, n_dest, n_src])\n    mult = tf.concat(\n        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n    )\n    return tf.tile(mask, mult)\n\n\nclass TransformerBlock(layers.Layer):\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n        super().__init__()\n        self.att = layers.MultiHeadAttention(num_heads, embed_dim)\n        self.ffn = keras.Sequential(\n            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n        )\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(rate)\n        self.dropout2 = layers.Dropout(rate)\n\n    def call(self, inputs):\n        input_shape = tf.shape(inputs)\n        batch_size = input_shape[0]\n        seq_len = input_shape[1]\n        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n        attention_output = self.att(inputs, inputs, attention_mask=causal_mask)\n        attention_output = self.dropout1(attention_output)\n        out1 = self.layernorm1(inputs + attention_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output)\n        return self.layernorm2(out1 + ffn_output)","metadata":{"execution":{"iopub.status.busy":"2023-06-25T16:54:39.634011Z","iopub.execute_input":"2023-06-25T16:54:39.635264Z","iopub.status.idle":"2023-06-25T16:54:39.647588Z","shell.execute_reply.started":"2023-06-25T16:54:39.635229Z","shell.execute_reply":"2023-06-25T16:54:39.646676Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Implement an embedding layer\n\nCreate two separate embedding layers: one for tokens and one for token index (positions).","metadata":{}},{"cell_type":"code","source":"class TokenAndPositionEmbedding(layers.Layer):\n    def __init__(self, maxlen, vocab_size, embed_dim):\n        super().__init__()\n        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n\n    def call(self, x):\n        maxlen = tf.shape(x)[-1]\n        positions = tf.range(start=0, limit=maxlen, delta=1)\n        positions = self.pos_emb(positions)\n        x = self.token_emb(x)\n        return x + positions","metadata":{"execution":{"iopub.status.busy":"2023-06-25T16:54:39.649912Z","iopub.execute_input":"2023-06-25T16:54:39.650555Z","iopub.status.idle":"2023-06-25T16:54:39.657795Z","shell.execute_reply.started":"2023-06-25T16:54:39.650524Z","shell.execute_reply":"2023-06-25T16:54:39.656702Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Implement the miniature GPT model","metadata":{}},{"cell_type":"code","source":"vocab_size = 20000  # Only consider the top 20k words\nmaxlen = 80  # Max sequence size\nembed_dim = 256  # Embedding size for each token\nnum_heads = 2  # Number of attention heads\nfeed_forward_dim = 256  # Hidden layer size in feed forward network inside transformer\n\n\ndef create_model():\n    inputs = layers.Input(shape=(maxlen,), dtype=tf.int32)\n    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n    x = embedding_layer(inputs)\n    transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim)\n    x = transformer_block(x)\n    outputs = layers.Dense(vocab_size)(x)\n    model = keras.Model(inputs=inputs, outputs=[outputs, x])\n    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n    model.compile(\n        \"adam\", loss=[loss_fn, None],\n    )  # No loss and optimization based on word embeddings from transformer block\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-06-25T16:54:39.660708Z","iopub.execute_input":"2023-06-25T16:54:39.661322Z","iopub.status.idle":"2023-06-25T16:54:39.670841Z","shell.execute_reply.started":"2023-06-25T16:54:39.661291Z","shell.execute_reply":"2023-06-25T16:54:39.669815Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Prepare the data for word-level language modelling\n\nDownload the IMDB dataset and combine training and validation sets for a text generation task.","metadata":{}},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-06-25T16:55:09.057157Z","iopub.execute_input":"2023-06-25T16:55:09.057508Z","iopub.status.idle":"2023-06-25T16:55:09.903678Z","shell.execute_reply.started":"2023-06-25T16:55:09.057475Z","shell.execute_reply":"2023-06-25T16:55:09.902724Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"14251"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-06-25T16:55:09.905231Z","iopub.execute_input":"2023-06-25T16:55:09.905613Z","iopub.status.idle":"2023-06-25T16:55:12.217453Z","shell.execute_reply.started":"2023-06-25T16:55:09.905579Z","shell.execute_reply":"2023-06-25T16:55:12.216469Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Implement a Keras callback for generating text","metadata":{}},{"cell_type":"code","source":"# class TextGenerator(keras.callbacks.Callback):\n#     \"\"\"A callback to generate text from a trained model.\n#     1. Feed some starting prompt to the model\n#     2. Predict probabilities for the next token\n#     3. Sample the next token and add it to the next input\n\n#     Arguments:\n#         max_tokens: Integer, the number of tokens to be generated after prompt.\n#         start_tokens: List of integers, the token indices for the starting prompt.\n#         index_to_word: List of strings, obtained from the TextVectorization layer.\n#         top_k: Integer, sample from the `top_k` token predictions.\n#         print_every: Integer, print after this many epochs.\n#     \"\"\"\n\n#     def __init__(\n#         self, max_tokens, start_tokens, index_to_word, top_k=10, print_every=1\n#     ):\n#         self.max_tokens = max_tokens\n#         self.start_tokens = start_tokens\n#         self.index_to_word = index_to_word\n#         self.print_every = print_every\n#         self.k = top_k\n\n#     def sample_from(self, logits):\n#         logits, indices = tf.math.top_k(logits, k=self.k, sorted=True)\n#         indices = np.asarray(indices).astype(\"int32\")\n#         preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n#         preds = np.asarray(preds).astype(\"float32\")\n#         return np.random.choice(indices, p=preds)\n\n#     def detokenize(self, number):\n#         return self.index_to_word[number]\n\n#     def on_epoch_end(self, epoch, logs=None):\n#         start_tokens = [_ for _ in self.start_tokens]\n#         if (epoch + 1) % self.print_every != 0:\n#             return\n#         num_tokens_generated = 0\n#         tokens_generated = []\n#         while num_tokens_generated <= self.max_tokens:\n#             pad_len = maxlen - len(start_tokens)\n#             sample_index = len(start_tokens) - 1\n#             if pad_len < 0:\n#                 x = start_tokens[:maxlen]\n#                 sample_index = maxlen - 1\n#             elif pad_len > 0:\n#                 x = start_tokens + [0] * pad_len\n#             else:\n#                 x = start_tokens\n#             x = np.array([x])\n#             y, _ = self.model.predict(x)\n#             sample_token = self.sample_from(y[0][sample_index])\n#             tokens_generated.append(sample_token)\n#             start_tokens.append(sample_token)\n#             num_tokens_generated = len(tokens_generated)\n#         txt = \" \".join(\n#             [self.detokenize(_) for _ in self.start_tokens + tokens_generated]\n#         )\n#         print(f\"generated text:\\n{txt}\\n\")\n        \n        \n\n# # Tokenize starting prompt\n# word_to_index = {}\n# for index, word in enumerate(vocab):\n#     word_to_index[word] = index\n","metadata":{"execution":{"iopub.status.busy":"2023-06-25T16:55:12.311350Z","iopub.execute_input":"2023-06-25T16:55:12.311646Z","iopub.status.idle":"2023-06-25T16:55:12.318127Z","shell.execute_reply.started":"2023-06-25T16:55:12.311622Z","shell.execute_reply":"2023-06-25T16:55:12.317114Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class TextGenerator(keras.callbacks.Callback):\n    \"\"\"A callback to generate text from a trained model.\n    1. Feed some starting prompt to the model\n    2. Predict probabilities for the next token\n    3. Sample the next token and add it to the next input\n\n    Arguments:\n        max_tokens: Integer, the number of tokens to be generated after prompt.\n        start_tokens: List of integers, the token indices for the starting prompt.\n        index_to_word: List of strings, obtained from the TextVectorization layer.\n        top_k: Integer, sample from the `top_k` token predictions.\n        print_every: Integer, print after this many epochs.\n    \"\"\"\n\n    def __init__(\n        self, max_tokens, start_tokens, index_to_word, top_k=10, print_every=1\n    ):\n        self.max_tokens = max_tokens\n        self.start_tokens = start_tokens\n        self.index_to_word = index_to_word\n        self.print_every = print_every\n        self.k = top_k\n\n    def sample_from(self, logits):\n        logits, indices = tf.math.top_k(logits, k=self.k, sorted=True)\n        indices = np.asarray(indices).astype(\"int32\")\n        preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n        preds = np.asarray(preds).astype(\"float32\")\n        return np.random.choice(indices, p=preds)\n\n    def detokenize(self, number):\n        return self.index_to_word[number]\n\n    def on_epoch_end(self, epoch, logs=None):\n        if (epoch + 1) % self.print_every != 0:\n            return\n        print(f\"Epoch {epoch+1}: loss = {logs['loss']:.4f}\")\n        start_tokens = [_ for _ in self.start_tokens]\n        num_tokens_generated = 0\n        tokens_generated = []\n        while num_tokens_generated <= self.max_tokens:\n            pad_len = maxlen - len(start_tokens)\n            sample_index = len(start_tokens) - 1\n            if pad_len < 0:\n                x = start_tokens[:maxlen]\n                sample_index = maxlen - 1\n            elif pad_len > 0:\n                x = start_tokens + [0] * pad_len\n            else:\n                x = start_tokens\n            x = np.array([x])\n            y, _ = self.model.predict(x)\n            sample_token = self.sample_from(y[0][sample_index])\n            tokens_generated.append(sample_token)\n            start_tokens.append(sample_token)\n            num_tokens_generated = len(tokens_generated)\n        txt = \" \".join(\n            [self.detokenize(_) for _ in self.start_tokens + tokens_generated]\n        )\n        print(f\"Generated text:\\n{txt}\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-06-25T16:55:12.319809Z","iopub.execute_input":"2023-06-25T16:55:12.320506Z","iopub.status.idle":"2023-06-25T16:55:12.335714Z","shell.execute_reply.started":"2023-06-25T16:55:12.320475Z","shell.execute_reply":"2023-06-25T16:55:12.334783Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Tokenize starting prompt\nword_to_index = {}\nfor index, word in enumerate(vocab):\n    word_to_index[word] = index","metadata":{"execution":{"iopub.status.busy":"2023-06-25T16:55:40.097478Z","iopub.execute_input":"2023-06-25T16:55:40.097878Z","iopub.status.idle":"2023-06-25T16:55:40.113464Z","shell.execute_reply.started":"2023-06-25T16:55:40.097846Z","shell.execute_reply":"2023-06-25T16:55:40.112474Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"start_prompt = \"this movie is\"\nstart_prompt = \"what is the\"\n\n\nstart_tokens = [word_to_index.get(_, 1) for _ in start_prompt.split()]\nnum_tokens_generated = 75\ntext_gen_callback = TextGenerator(num_tokens_generated, start_tokens, vocab)","metadata":{"execution":{"iopub.status.busy":"2023-06-25T16:55:41.256235Z","iopub.execute_input":"2023-06-25T16:55:41.256582Z","iopub.status.idle":"2023-06-25T16:55:41.261728Z","shell.execute_reply.started":"2023-06-25T16:55:41.256555Z","shell.execute_reply":"2023-06-25T16:55:41.260823Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Train the model","metadata":{}},{"cell_type":"code","source":"# from tqdm import tqdm\n\n# # Create a wrapper function for the training loop\n# def train_with_progress_bar(model, dataset, epochs, callbacks):\n#     # Disable the progress bar by setting `disable=True`\n#     with tqdm(total=epochs, disable=True) as pbar:\n#         for epoch in range(epochs):\n#             # Perform one epoch of training\n#             model.fit(dataset, verbose=0, epochs=epochs, callbacks=[text_gen_callback])\n            \n#             # Update the progress bar manually\n#             pbar.set_postfix({'Epoch': epoch + 1})\n#             pbar.update(1)\n\n# # Train the model using the wrapper function\n# train_with_progress_bar(model, text_ds, N_EPOCHS, callbacks=[text_gen_callback])","metadata":{"execution":{"iopub.status.busy":"2023-06-25T18:22:15.294890Z","iopub.execute_input":"2023-06-25T18:22:15.295598Z","iopub.status.idle":"2023-06-25T18:29:41.560664Z","shell.execute_reply.started":"2023-06-25T18:22:15.295564Z","shell.execute_reply":"2023-06-25T18:29:41.558973Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stdout","text":"Epoch 1: loss = 0.3659\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 22ms/step\nGenerated text:\nwhat is the difference ? i was doing to the same thing . you have a [UNK] ? if i was gonna check the facts before you remove them .              \n\nEpoch 2: loss = 0.3651\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\nGenerated text:\nwhat is the fuck up . [UNK] ? fuck you , just go to a message those who lack of military newspaper is [UNK] - i have a crime . maybe an actual photos of the page . - ] . - i should\n\nEpoch 3: loss = 0.3562\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\nGenerated text:\nwhat is the problem ? i 'm sure that you 're a person and the idiot . . . . i 'm a [UNK] asshole . .you deserve it               \n\nEpoch 4: loss = 0.3487\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 33ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\nGenerated text:\nwhat is the 3rr ? just on it is clear the way i am not only attacking you today .                        \n\nEpoch 5: loss = 0.3417\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 36ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\nGenerated text:\nwhat is the problem ! i know who the hell are you doing ?                              \n\nEpoch 6: loss = 0.3357\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 60ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\nGenerated text:\nwhat is the vandalism . you 've vandalized user : i split into the trouble with the article .                         \n\nEpoch 7: loss = 0.3292\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\nGenerated text:\nwhat is the one idiot keeps reverting                                     \n\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[59], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m             pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Train the model using the wrapper function\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[43mtrain_with_progress_bar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtext_gen_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[59], line 9\u001b[0m, in \u001b[0;36mtrain_with_progress_bar\u001b[0;34m(model, dataset, epochs, callbacks)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39mepochs, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;66;03m# Perform one epoch of training\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m         \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtext_gen_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;66;03m# Update the progress bar manually\u001b[39;00m\n\u001b[1;32m     12\u001b[0m         pbar\u001b[38;5;241m.\u001b[39mset_postfix({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m'\u001b[39m: epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m})\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/engine/training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1678\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1679\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1683\u001b[0m ):\n\u001b[1;32m   1684\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1685\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1687\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    891\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 894\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    896\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    897\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    923\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    924\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    925\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 926\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    928\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    929\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    930\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    141\u001b[0m   (concrete_function,\n\u001b[1;32m    142\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1753\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1755\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1756\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1757\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1758\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1759\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1760\u001b[0m     args,\n\u001b[1;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1762\u001b[0m     executing_eagerly)\n\u001b[1;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    380\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 381\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    389\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    390\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    394\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"model = create_model()\n\nN_EPOCHS = 25\nverbose = 2 ## Set to a number such as 2 to see each steps progress bar\nhistory = model.fit(text_ds, verbose=0, epochs=N_EPOCHS, callbacks=[text_gen_callback])","metadata":{"execution":{"iopub.status.busy":"2023-06-25T18:30:10.796376Z","iopub.execute_input":"2023-06-25T18:30:10.796743Z","iopub.status.idle":"2023-06-25T18:34:14.726457Z","shell.execute_reply.started":"2023-06-25T18:30:10.796708Z","shell.execute_reply":"2023-06-25T18:34:14.723342Z"},"trusted":true},"execution_count":60,"outputs":[{"name":"stdout","text":"Epoch 1: loss = 3.0473\n1/1 [==============================] - 0s 224ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 22ms/step\nGenerated text:\nwhat is the hell ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n\nEpoch 2: loss = 2.2580\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 31ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\nGenerated text:\nwhat is the problem with the problem is this joke on earth . it was not an edit warring , untwirl , i have already been a history of my talk • user page \" \" \" , and \" \" [UNK] \" \"\n\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[60], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m N_EPOCHS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m25\u001b[39m\n\u001b[1;32m      4\u001b[0m verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;66;03m## Set to a number such as 2 to see each steps progress bar\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtext_gen_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/engine/training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1678\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1679\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1683\u001b[0m ):\n\u001b[1;32m   1684\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1685\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1687\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    891\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 894\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    896\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    897\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    923\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    924\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    925\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 926\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    928\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    929\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    930\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    141\u001b[0m   (concrete_function,\n\u001b[1;32m    142\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1753\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1755\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1756\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1757\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1758\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1759\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1760\u001b[0m     args,\n\u001b[1;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1762\u001b[0m     executing_eagerly)\n\u001b[1;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    380\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 381\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    389\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    390\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    394\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"Now we can generate text continuuing from a new prmopt ","metadata":{}},{"cell_type":"code","source":"new_start_prompt = \"start something\"\nnew_start_tokens = [word_to_index.get(word, 1) for word in new_start_prompt.split()]\n\ntext_gen_callback.start_tokens = new_start_tokens\ntext_gen_callback.on_epoch_end(0)","metadata":{"execution":{"iopub.status.busy":"2023-06-25T16:55:12.607587Z","iopub.status.idle":"2023-06-25T16:55:12.608278Z","shell.execute_reply.started":"2023-06-25T16:55:12.608028Z","shell.execute_reply":"2023-06-25T16:55:12.608050Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}