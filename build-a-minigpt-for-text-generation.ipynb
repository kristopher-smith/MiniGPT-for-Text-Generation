{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# MiniGPT For Generating Synthetic Text Data\n\nby Kris Smith","metadata":{}},{"cell_type":"markdown","source":"# ***WARNING*** \n\n## The data required to train the model for this task is known to be vulgar, offensive, toxic, racist, and otherwise not pleasant.","metadata":{}},{"cell_type":"markdown","source":"## Problem Statement\n\nToxic comments online come in many forms and in many arenas. There are currently several ways to mitigate these comments(for those organizations who wish to do so). Some of these ways include human moderators, and training machine learning models to detect toxicity in online comments.\n\nThe issue with human moderators is that some of these platforms have grown so large so quickly that there are not nearly enough moderators to achieve any sense of control for most of these comments. The shear volume of toxicity and bots online makes it unrealistic to think we could do this job with humans at this point.\n\nMany companies are employing machine learning to assist with identifying toxic comments online automatically. The problem with this approach is the lack of labeled training data to train the models on.\n\nThis is the problem I am going to solve using generative deep learning techniques. ","metadata":{}},{"cell_type":"markdown","source":"## References\n\n* [Improving Language Understanding by Generative Pre-Training](https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035)\n\n* [Language Models are Unsupervised Multitask Learners](https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe)\n\n* [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)\n\n* Many of the ideas and code were adapted from this Keras resource: https://keras.io/examples/generative/text_generation_with_miniature_gpt/","metadata":{}},{"cell_type":"markdown","source":"## Import Libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport string\nimport random\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\n\nfrom nltk import ngrams\nfrom collections import Counter\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import TextVectorization\n\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-26T00:04:45.852149Z","iopub.execute_input":"2023-06-26T00:04:45.852541Z","iopub.status.idle":"2023-06-26T00:04:54.318332Z","shell.execute_reply.started":"2023-06-26T00:04:45.852510Z","shell.execute_reply":"2023-06-26T00:04:54.317348Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"},{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"markdown","source":"## Data\n\nThe data I will be using to train the generative model was released on Kaggle as part of an ongoing series of competitions sponsored by the [Google company Jigsaw](https://en.wikipedia.org/wiki/Jigsaw_(company)).\n\nThe data consists of online comments with various severity levels of toxicity. There are versions of these comments labeled by human annotators wherein they label each comment as toxic or not, or other sets where they were labeled as different categories of toxic such as hatespeech, racist/sexist, obscene, etc. Although these are the labeled datasets we would be adding the synthetic data to in order to create more training data, for this task of simply generating similar text data we will only focus on the comments themselves.\n\nThe data provided by this competition includes a total of `14,251` unique toxic comments. Theses are the comments I will use to train the generative model with.","metadata":{}},{"cell_type":"markdown","source":"## EDA\n\nThe data came in two different files.\n\n1) Comments to score: This acts as a test dataset of comments for scoring after the model was trained.\n\n2) Validation data: This was the training data for the competition wherein there are two columns. One column labeled less toxic was a comment which human annotators labeled as less toxic than its more toxic counterpart in the other column. There was no actual training data where a comment was paired with its severity rating. The models were trained using creative techniques with the validation data and other classification data sets to train a model which predicted severity of comments.\n\nSince for our purposes we are only interested in the actual text comments themselves, I will only be using those columns from these datasources.\n\nI start by reading them all into pandas dataframes, isolating the text columns from each one, and stacking them all together so we have a single column of text when it is all said and done.\n","metadata":{}},{"cell_type":"code","source":"data1 = pd.read_csv('../input/jigsaw-toxic-severity-rating/comments_to_score.csv')\ndata1.info()\n\n## Isolate only text column\ndata1 = data1['text']\n\ndata1.sample(6)","metadata":{"execution":{"iopub.status.busy":"2023-06-26T00:04:54.320422Z","iopub.execute_input":"2023-06-26T00:04:54.321244Z","iopub.status.idle":"2023-06-26T00:04:54.433748Z","shell.execute_reply.started":"2023-06-26T00:04:54.321210Z","shell.execute_reply":"2023-06-26T00:04:54.432848Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 7537 entries, 0 to 7536\nData columns (total 2 columns):\n #   Column      Non-Null Count  Dtype \n---  ------      --------------  ----- \n 0   comment_id  7537 non-null   int64 \n 1   text        7537 non-null   object\ndtypes: int64(1), object(1)\nmemory usage: 117.9+ KB\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"5588                  Important \\ndo you spit or swallow?\n1653                                CHickens has no penis\n6699     Learn to read \\n\\nAnd stop being a surly pric...\n4725    \"\\n\\nIf you don't believe me, go look at the s...\n1447     Ban me for what ?\\nYou cant put me on my fina...\n1860    Meeples is a joke account.  Please disregard e...\nName: text, dtype: object"},"metadata":{}}]},{"cell_type":"markdown","source":"We can see the comments to score was the test file which contained only comments and their corresponding id's","metadata":{}},{"cell_type":"code","source":"data2 = pd.read_csv('../input/jigsaw-toxic-severity-rating/validation_data.csv')\ndata2.info()\n\ndata2.sample(6)","metadata":{"execution":{"iopub.status.busy":"2023-06-26T00:04:54.435268Z","iopub.execute_input":"2023-06-26T00:04:54.435935Z","iopub.status.idle":"2023-06-26T00:04:54.978925Z","shell.execute_reply.started":"2023-06-26T00:04:54.435901Z","shell.execute_reply":"2023-06-26T00:04:54.977167Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 30108 entries, 0 to 30107\nData columns (total 3 columns):\n #   Column      Non-Null Count  Dtype \n---  ------      --------------  ----- \n 0   worker      30108 non-null  int64 \n 1   less_toxic  30108 non-null  object\n 2   more_toxic  30108 non-null  object\ndtypes: int64(1), object(2)\nmemory usage: 705.8+ KB\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"       worker                                         less_toxic  \\\n8184      349  \"\\n\\nAm I the only one who thinks this article...   \n20462     615   Marie Luv \\n\\n 20:18, 2 September 2009 (hist)...   \n13424     633          Everybody lock ur dicks cuz theyre gay!!!   \n1809      634                              CHickens has no penis   \n7143      157  I am a stupid bitch who needs shutting the fuc...   \n14091     201  \"\\n\\n Wealth and Justice sections \\n\\nThese tw...   \n\n                                              more_toxic  \n8184   Niteshift you are one seriously ignorant indiv...  \n20462   Take a break. \\n\\nTake a break, Culver. What ...  \n13424  \"\\n\\nUseless\\nThis page is not helpful to Wiki...  \n1809   You are a Problem\\n\\nyour probably some freshm...  \n7143   \"\\nYou are a biased Columbia grad with a COI. ...  \n14091  wtf is this\\nconfusing why cant peope have the...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>worker</th>\n      <th>less_toxic</th>\n      <th>more_toxic</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>8184</th>\n      <td>349</td>\n      <td>\"\\n\\nAm I the only one who thinks this article...</td>\n      <td>Niteshift you are one seriously ignorant indiv...</td>\n    </tr>\n    <tr>\n      <th>20462</th>\n      <td>615</td>\n      <td>Marie Luv \\n\\n 20:18, 2 September 2009 (hist)...</td>\n      <td>Take a break. \\n\\nTake a break, Culver. What ...</td>\n    </tr>\n    <tr>\n      <th>13424</th>\n      <td>633</td>\n      <td>Everybody lock ur dicks cuz theyre gay!!!</td>\n      <td>\"\\n\\nUseless\\nThis page is not helpful to Wiki...</td>\n    </tr>\n    <tr>\n      <th>1809</th>\n      <td>634</td>\n      <td>CHickens has no penis</td>\n      <td>You are a Problem\\n\\nyour probably some freshm...</td>\n    </tr>\n    <tr>\n      <th>7143</th>\n      <td>157</td>\n      <td>I am a stupid bitch who needs shutting the fuc...</td>\n      <td>\"\\nYou are a biased Columbia grad with a COI. ...</td>\n    </tr>\n    <tr>\n      <th>14091</th>\n      <td>201</td>\n      <td>\"\\n\\n Wealth and Justice sections \\n\\nThese tw...</td>\n      <td>wtf is this\\nconfusing why cant peope have the...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"This was the data provided to validate the models performance during training. The three columns are workers(annotators) and the other two are text columns which we will use both to train our generative model with.","metadata":{}},{"cell_type":"markdown","source":"#### Combine all columns into a single column","metadata":{}},{"cell_type":"code","source":"## Isolate text column\ndata2 = data2['more_toxic']\n\n## Isolate text column\ndata3 = pd.read_csv('../input/jigsaw-toxic-severity-rating/validation_data.csv')\ndata3 = data3['less_toxic']\n\ntext_column = pd.concat([data1, data2, data3], axis=0, ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2023-06-26T00:04:54.982454Z","iopub.execute_input":"2023-06-26T00:04:54.982935Z","iopub.status.idle":"2023-06-26T00:04:55.249172Z","shell.execute_reply.started":"2023-06-26T00:04:54.982892Z","shell.execute_reply":"2023-06-26T00:04:55.248018Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"#### Check for duplicates","metadata":{}},{"cell_type":"code","source":"text_column.value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-06-26T00:04:55.250827Z","iopub.execute_input":"2023-06-26T00:04:55.251972Z","iopub.status.idle":"2023-06-26T00:04:55.296608Z","shell.execute_reply.started":"2023-06-26T00:04:55.251921Z","shell.execute_reply":"2023-06-26T00:04:55.295245Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":" sorry i jumped to conclusions \\n\\non christian terrorism article man, I don't agree with you, and I want you to go and listen to 'prophet of doom' (now in audio format) as it is good. But I was wrong to be so rude. It is not the Southern European way.                                                                                                                                                                               19\nthis irishtom guy is turning every article into an ad for islam                                                                                                                                                                                                                                                                                                                                                                            19\nYou are not sorry one damned bit.  You have yet to refute what I have written.  All you do is pass the insults as if it were salt on the dinner table.  This is on every article in which we disagree.  If you have something useful and constructive to say, then don't be a harpy troll.                                                                                                                                                 19\n YOUR BIASED! \\n\\nPLEASE OTHER THAN HIDE BEHIND WK RULES\\n\\nacutally IDENTITY THE OFFESNES COMMIMITED!\\n\\nYOU JUST SAID you dont care about my OPINIONS!..yet the opinions that where QUOTED WHERE FROM THE REFERENCES YOU HAD ACCEPTED!!\\n\\nLOL...\\n\\nSO in which case i am formally complaining about YOU AND YOUR BIASED STANCE!\\n\\nALL MY REFERNCES HAVE ISBN NUMBERS, YEAR AND PUBLISHERS!\\n\\nYOU ARE PROTECTING YOUR BIASED VIEW!    16\nI erased your cuss word\\nFrom: some random person out there in the world                                                                                                                                                                                                                                                                                                                                                                   16\n                                                                                                                                                                                                                                                                                                                                                                                                                                           ..\nHey\\n\\nI bet you Quinsareth are gay and like telling lies to your mother.                                                                                                                                                                                                                                                                                                                                                                   2\nVandalism on Muhammad page\\n\\nPerhaps that was the wrong way to deal with it, but                                                                                                                                                                                                                                                                                                                                                           2\n Thank You \\n\\nHey Nishkid I really appreciate the unblock.  Once again I apologize for any vandalism I caused on user pages and I have read Wikipedia's user policy.  Thank You!!!                                                                                                                                                                                                                                                         2\n Your low self-esteem \\n\\nI see you have such a low self-esteem that you have to warn others not to attack you.4.130.134.233                                                                                                                                                                                                                                                                                                                2\nVANDALISE MY ASS ==\\n\\n==                                                                                                                                                                                                                                                                                                                                                                                                                   2\nLength: 14251, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"It looks like between the data provided for the competition there are many duplicates. However we can see that some comments are reused many more times than other comments. For example the most used comments were repeated `19` times in the datasets while others only `2` times. \n\nSince the duplications are not balanced if we left the data like this I am afraid we would be biasing the model towards the comments which were present more in the data. \n\nI will remove all duplicate comments.","metadata":{}},{"cell_type":"code","source":"print(f\"Total numer of comments in text data = {len(text_column)}\")\nprint(f\"Numer of unique comments in text data = {len(text_column.unique())}\")\n\ntext_column = text_column.drop_duplicates()\nprint(\"Duplicate comments dropped\")","metadata":{"execution":{"iopub.status.busy":"2023-06-26T00:04:55.298280Z","iopub.execute_input":"2023-06-26T00:04:55.298950Z","iopub.status.idle":"2023-06-26T00:04:55.373317Z","shell.execute_reply.started":"2023-06-26T00:04:55.298912Z","shell.execute_reply":"2023-06-26T00:04:55.372048Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Total numer of comments in text data = 67753\nNumer of unique comments in text data = 14251\nDuplicate comments dropped\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Exploring the toxic comments","metadata":{}},{"cell_type":"code","source":"data = pd.DataFrame()\ndata['text'] = text_column\ndata = data.sample(100)\n\n# Function to calculate word count\ndef count_words(text):\n    words = nltk.word_tokenize(text)\n    return len(words)\n\n# Function to calculate verb count\ndef count_verbs(text):\n    words = nltk.word_tokenize(text)\n    tagged_words = nltk.pos_tag(words)\n    verb_count = len([word for word, tag in tagged_words if tag.startswith('V')])\n    return verb_count\n\n# Function to calculate noun count\ndef count_nouns(text):\n    words = nltk.word_tokenize(text)\n    tagged_words = nltk.pos_tag(words)\n    noun_count = len([word for word, tag in tagged_words if tag.startswith('N')])\n    return noun_count\n\n# Add word count column\ndata['word_count'] = data['text'].apply(count_words)\n\n# Add verb count column\ndata['verb_count'] = data['text'].apply(count_verbs)\n\n# Add noun count column\ndata['noun_count'] = data['text'].apply(count_nouns)\n\ndata.describe()","metadata":{"execution":{"iopub.status.busy":"2023-06-26T00:04:55.375103Z","iopub.execute_input":"2023-06-26T00:04:55.375531Z","iopub.status.idle":"2023-06-26T00:04:56.658766Z","shell.execute_reply.started":"2023-06-26T00:04:55.375493Z","shell.execute_reply":"2023-06-26T00:04:56.657582Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"       word_count  verb_count  noun_count\ncount  100.000000   100.00000  100.000000\nmean    83.160000    13.85000   19.300000\nstd    119.187292    19.15296   29.181598\nmin      4.000000     0.00000    1.000000\n25%     23.000000     3.00000    5.000000\n50%     50.000000     8.00000   10.000000\n75%     91.500000    15.25000   15.250000\nmax    965.000000   144.00000  209.000000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word_count</th>\n      <th>verb_count</th>\n      <th>noun_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>100.000000</td>\n      <td>100.00000</td>\n      <td>100.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>83.160000</td>\n      <td>13.85000</td>\n      <td>19.300000</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>119.187292</td>\n      <td>19.15296</td>\n      <td>29.181598</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>4.000000</td>\n      <td>0.00000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>23.000000</td>\n      <td>3.00000</td>\n      <td>5.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>50.000000</td>\n      <td>8.00000</td>\n      <td>10.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>91.500000</td>\n      <td>15.25000</td>\n      <td>15.250000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>965.000000</td>\n      <td>144.00000</td>\n      <td>209.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"ax = data['word_count'].plot(kind='kde')\ndata['verb_count'].plot(kind='kde', ax=ax)\ndata['noun_count'].plot(kind='kde', ax=ax)\n\nax.legend(['Word Count', 'Verb Count', 'Noun Count'])\nax.set_title('Distribution of Word Count, Verb Count, and Noun Count')\nax.set_xlabel('Count')\nax.set_ylabel('Density')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-26T00:04:56.660657Z","iopub.execute_input":"2023-06-26T00:04:56.661041Z","iopub.status.idle":"2023-06-26T00:04:57.147960Z","shell.execute_reply.started":"2023-06-26T00:04:56.661007Z","shell.execute_reply":"2023-06-26T00:04:57.147090Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"markdown","source":"We can see that these comments on average are quite short in length and contain more nouns than verbs on average.\n\nSince we have not done any cleaning of the data yet these distributions are not exact as the nltk package is not currently looking for misspelled words or different versions of word spellings which are used online sometimes.\n\nFor example if a user knows that the platform they are on has limitations on language than they may spell a profane word to try to fool any auto detecting systems such as `Fuck==>Fxck, F*ck, Fukk, Fuuu*uukk`, etc.\n\nTherefore these counts will not detect all nouns and verbs but should give a decent sample.\n\nKnowing the underlying distributions of some of these features is important because after the synthetic data is generated we would most likely want it to follow the same distributions for these attributes of the text. ","metadata":{}},{"cell_type":"markdown","source":"### Looking at the most common N-grams","metadata":{}},{"cell_type":"code","source":"# Tokenize the text into words\ndata['words'] = data['text'].apply(nltk.word_tokenize)\n\n# Get bigrams and trigrams for each row\ndata['bigrams']   = data['words'].apply(lambda x: list(ngrams(x, 2)))\ndata['trigrams']  = data['words'].apply(lambda x: list(ngrams(x, 3)))\n# data['quadgrams'] = data['words'].apply(lambda x: list(ngrams(x, 4)))\n\n# Count the occurrences of bigrams and trigrams\nbigram_counts   = Counter([gram for grams in data['bigrams'] for gram in grams])\ntrigram_counts  = Counter([gram for grams in data['trigrams'] for gram in grams])\n# quadgram_counts = Counter([gram for grams in data['quadgrams'] for gram in grams])\n\n# Get the most common bigrams, trigrams, and quadgrams\nmost_common_bigrams   = bigram_counts.most_common(50)\nmost_common_trigrams  = trigram_counts.most_common(50)\n# most_common_quadgrams = quadgram_counts.most_common(50)\n\ndf_common_grams = pd.DataFrame()\ndf_common_grams['bigrams']   = most_common_bigrams\ndf_common_grams['trigrams']  = most_common_trigrams\n# df_common_grams['quadgrams'] = most_common_quadgrams\n\n# # Display the results\n# print('Most common bigrams:')\n# for bigram, count in most_common_bigrams:\n#     print(' '.join(bigram), count)\n\n# print('\\nMost common trigrams:')\n# for trigram, count in most_common_trigrams:\n#     print(' '.join(trigram), count)\n    \n# print('\\nMost common quadgrams:')\n# for quadgram, count in most_common_quadgrams:\n#     print(' '.join(quadgram), count)\n\n\ndf_common_grams.iloc[:, :]","metadata":{"execution":{"iopub.status.busy":"2023-06-26T00:04:57.149446Z","iopub.execute_input":"2023-06-26T00:04:57.150022Z","iopub.status.idle":"2023-06-26T00:04:57.353352Z","shell.execute_reply.started":"2023-06-26T00:04:57.149989Z","shell.execute_reply":"2023-06-26T00:04:57.352451Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"                bigrams                  trigrams\n0        (('', ''), 69)         (('', '', .), 10)\n1        ((``, ''), 58)         ((I, do, n't), 9)\n2        ((,, and), 34)          ((., '', ''), 9)\n3       ((of, the), 30)         ((I, 'm, not), 8)\n4          ((., I), 29)        (('', '', and), 7)\n5         ((I, 'm), 22)          ((a, ``, ''), 7)\n6        ((., The), 21)   (((, Family_Guy, )), 7)\n7       ((do, n't), 20)           ((., I, 'm), 6)\n8       ((in, the), 19)          (('', '', ,), 6)\n9         ((is, a), 18)          ((,, but, I), 5)\n10       ((to, be), 18)        ((the, ``, ''), 5)\n11         ((,, I), 17)          ((:, ``, ''), 5)\n12     ((you, are), 15)       ((you, ca, n't), 4)\n13       ((,, but), 14)          ((., It, is), 4)\n14        ((it, .), 14)      ((do, n't, care), 4)\n15        ((., ''), 13)     ((do, you, think), 4)\n16        ((I, do), 13)        ((``, '', The), 4)\n17       ((., You), 12)     (('', The, Facts), 4)\n18       ((,, you), 12)     ((The, Facts, ''), 4)\n19         ((!, !), 12)      ((Facts, '', ''), 4)\n20    ((that, you), 11)         ((., Also, ,), 4)\n21    ((what, you), 11)            ((!, !, !), 4)\n22      ((to, the), 11)   ((from, the, midst), 4)\n23  ((would, have), 11)     ((the, midst, of), 4)\n24    ((that, the), 11)    ((midst, of, thee), 4)\n25      ((is, not), 10)        (('', and, ``), 3)\n26        ((., It), 10)        ((and, ``, ''), 3)\n27        (('', .), 10)        ((you, '', ''), 3)\n28         ((,, it), 9)       ((ca, n't, see), 3)\n29       ((you, do), 9)       ((you, do, n't), 3)\n30     ((you, have), 8)      ((n't, care, if), 3)\n31        ((to, do), 8)     ((not, going, to), 3)\n32       ((it, was), 8)           ((., I, do), 3)\n33       ((ca, n't), 8)         ((as, ``, ''), 3)\n34      ((did, n't), 8)          (('', '', (), 3)\n35         ((I, am), 8)         ((but, I, 'm), 3)\n36        ((It, 's), 8)        ((., This, is), 3)\n37       (('m, not), 8)  ((would, have, been), 3)\n38          ((), ,), 8)        ((,, such, as), 3)\n39          ((?, ?), 7)     ((I, would, have), 3)\n40       (('', and), 7)     ((that, you, can), 3)\n41     ((the, same), 7)          ((., ``, ``), 3)\n42         ((it, ,), 7)    ((you, think, you), 3)\n43      ((you, can), 7)    ((think, you, are), 3)\n44         ((``, I), 7)          ((), ,, and), 3)\n45     ((from, the), 7)      ((that, it, was), 3)\n46       ((is, the), 7)    ((``, '', missile), 3)\n47    ((have, been), 7)      ((it, would, be), 3)\n48        ((,, the), 7)           ((., I, am), 3)\n49         ((as, a), 7)          ((., It, 's), 3)","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>bigrams</th>\n      <th>trigrams</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>(('', ''), 69)</td>\n      <td>(('', '', .), 10)</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>((``, ''), 58)</td>\n      <td>((I, do, n't), 9)</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>((,, and), 34)</td>\n      <td>((., '', ''), 9)</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>((of, the), 30)</td>\n      <td>((I, 'm, not), 8)</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>((., I), 29)</td>\n      <td>(('', '', and), 7)</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>((I, 'm), 22)</td>\n      <td>((a, ``, ''), 7)</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>((., The), 21)</td>\n      <td>(((, Family_Guy, )), 7)</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>((do, n't), 20)</td>\n      <td>((., I, 'm), 6)</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>((in, the), 19)</td>\n      <td>(('', '', ,), 6)</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>((is, a), 18)</td>\n      <td>((,, but, I), 5)</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>((to, be), 18)</td>\n      <td>((the, ``, ''), 5)</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>((,, I), 17)</td>\n      <td>((:, ``, ''), 5)</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>((you, are), 15)</td>\n      <td>((you, ca, n't), 4)</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>((,, but), 14)</td>\n      <td>((., It, is), 4)</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>((it, .), 14)</td>\n      <td>((do, n't, care), 4)</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>((., ''), 13)</td>\n      <td>((do, you, think), 4)</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>((I, do), 13)</td>\n      <td>((``, '', The), 4)</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>((., You), 12)</td>\n      <td>(('', The, Facts), 4)</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>((,, you), 12)</td>\n      <td>((The, Facts, ''), 4)</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>((!, !), 12)</td>\n      <td>((Facts, '', ''), 4)</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>((that, you), 11)</td>\n      <td>((., Also, ,), 4)</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>((what, you), 11)</td>\n      <td>((!, !, !), 4)</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>((to, the), 11)</td>\n      <td>((from, the, midst), 4)</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>((would, have), 11)</td>\n      <td>((the, midst, of), 4)</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>((that, the), 11)</td>\n      <td>((midst, of, thee), 4)</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>((is, not), 10)</td>\n      <td>(('', and, ``), 3)</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>((., It), 10)</td>\n      <td>((and, ``, ''), 3)</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>(('', .), 10)</td>\n      <td>((you, '', ''), 3)</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>((,, it), 9)</td>\n      <td>((ca, n't, see), 3)</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>((you, do), 9)</td>\n      <td>((you, do, n't), 3)</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>((you, have), 8)</td>\n      <td>((n't, care, if), 3)</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>((to, do), 8)</td>\n      <td>((not, going, to), 3)</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>((it, was), 8)</td>\n      <td>((., I, do), 3)</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>((ca, n't), 8)</td>\n      <td>((as, ``, ''), 3)</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>((did, n't), 8)</td>\n      <td>(('', '', (), 3)</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>((I, am), 8)</td>\n      <td>((but, I, 'm), 3)</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>((It, 's), 8)</td>\n      <td>((., This, is), 3)</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>(('m, not), 8)</td>\n      <td>((would, have, been), 3)</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>((), ,), 8)</td>\n      <td>((,, such, as), 3)</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>((?, ?), 7)</td>\n      <td>((I, would, have), 3)</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>(('', and), 7)</td>\n      <td>((that, you, can), 3)</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>((the, same), 7)</td>\n      <td>((., ``, ``), 3)</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>((it, ,), 7)</td>\n      <td>((you, think, you), 3)</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>((you, can), 7)</td>\n      <td>((think, you, are), 3)</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>((``, I), 7)</td>\n      <td>((), ,, and), 3)</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>((from, the), 7)</td>\n      <td>((that, it, was), 3)</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>((is, the), 7)</td>\n      <td>((``, '', missile), 3)</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>((have, been), 7)</td>\n      <td>((it, would, be), 3)</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>((,, the), 7)</td>\n      <td>((., I, am), 3)</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>((as, a), 7)</td>\n      <td>((., It, 's), 3)</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"We can see the initial 10 or so most common bi-grams and tri-grams are repetitive punctuation marks.\n\nTraditionally these would be cleaned and removed when training models for NLP tasks, however due to the nature of this work many of these traditional techniques will limit the models ability to predict toxicity as well as with clean text.\n\nI happened to have competed in this competition and one thing all of us learned was that leaving capital letters and punctuation improved the models ability to infer toxicity and especially levels of toxicity. \n\nFor example a phrase such as:\n\n`Are you kidding?`\n\nConveys a much different meaning than the same words but put this way:\n\n`ARE YOU KIDDING!!!??`\n\nTraditional NLP techniques would have us convert all characters to lower case and remove punctuation so the model will interpret both of those texts the exact same way.\n\nWhen training sentiment based models or models where feeling and emotion is being conveyed in some way such as toxicity of comments, it is more than just the raw content of the words alone which gives the meaning. The puncuation and capitalizations are very expressive forms of language and as such for these problems do better left in the data.","metadata":{}},{"cell_type":"markdown","source":"## Pre-Processing\n\n* First we need load in our text column as tensorflow formatted dataset\n\n* Next we shuffle the data to avoid any patterns which may have been present\n\n* We then slice the data into batches for processing\n\n* Vectorize the text which will be used to create a corpus of vocabulary used when training and act as vector representations of our text\n\n* Create the corpus of vocabulary which is used to train and evaluate throughout","metadata":{}},{"cell_type":"code","source":"vocab_size = 20000  ## Only consider the top 20k words\nmaxlen = 80  ## Max sequence length\nbatch_size = 32  ## Data loading batch sizes\n\n# Create a dataset from the pandas column\ntext_ds = tf.data.Dataset.from_tensor_slices(text_column)\n\n# Shuffle and batch the dataset\ntext_ds = text_ds.shuffle(buffer_size=256)\ntext_ds = text_ds.batch(batch_size)\n\n# def custom_standardization(input_string):\n#     \"\"\" Remove html line-break tags and handle punctuation \"\"\"\n#     lowercased = tf.strings.lower(input_string)\n#     stripped_html = tf.strings.regex_replace(lowercased, \"<br />\", \" \")\n#     return tf.strings.regex_replace(stripped_html, f\"([{string.punctuation}])\", r\" \\1\")\n\n\n## Create a vectorization layer and adapt it to the text\nvectorize_layer = TextVectorization(\n    standardize=None,\n    max_tokens=vocab_size - 1,\n    output_mode=\"int\",\n    output_sequence_length=maxlen + 1,\n)\nvectorize_layer.adapt(text_ds)\nvocab = vectorize_layer.get_vocabulary()  ## To get words back from token indices","metadata":{"execution":{"iopub.status.busy":"2023-06-26T00:04:57.357653Z","iopub.execute_input":"2023-06-26T00:04:57.358237Z","iopub.status.idle":"2023-06-26T00:05:01.202265Z","shell.execute_reply.started":"2023-06-26T00:04:57.358201Z","shell.execute_reply":"2023-06-26T00:05:01.201037Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Generate Labels\n\nSince we are building a generative auto-regressive model, we must train it to predict the next word by looking backwards and using the previous tokens to predict the highest probability for the next token.\n\nThis is fairly easy to create labels for because we simply shuffle the `TRUE` data be one token and then when training the model compares the predicted text with the next indexed word.\n\nWe can inspect what these samples and labels look like below:","metadata":{}},{"cell_type":"code","source":"## Function to create target column\ndef prepare_lm_inputs_labels(text):\n    \"\"\"\n    Shift word sequences by 1 position so that the target for position (i) is\n    word at position (i+1). The model will use all words up till position (i)\n    to predict the next word.\n    \"\"\"\n    text = tf.expand_dims(text, -1)\n    tokenized_sentences = vectorize_layer(text)\n    x = tokenized_sentences[:, :-1]\n    y = tokenized_sentences[:, 1:]\n    return x, y\n\n\ntext_ds = text_ds.map(prepare_lm_inputs_labels)\ntext_ds = text_ds.prefetch(tf.data.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2023-06-26T00:05:01.206716Z","iopub.execute_input":"2023-06-26T00:05:01.207063Z","iopub.status.idle":"2023-06-26T00:05:01.345537Z","shell.execute_reply.started":"2023-06-26T00:05:01.207031Z","shell.execute_reply":"2023-06-26T00:05:01.344610Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"## Select samples from the training data set to inspect\nsample = text_ds.take(5) \n\n## Display some samples\nfor x, y in sample:\n    # Convert token indices back to words\n    input_words  = [vocab[i] for i in x[0].numpy()]\n    target_words = [vocab[i] for i in y[0].numpy()]\n\n    print(\"\\n\\n\\n\\nInput Sequence:\")\n    print(\" \".join(input_words))\n    print(\"\\nTarget Sequence:\")\n    print(\" \".join(target_words))","metadata":{"execution":{"iopub.status.busy":"2023-06-26T00:05:01.346829Z","iopub.execute_input":"2023-06-26T00:05:01.347260Z","iopub.status.idle":"2023-06-26T00:05:01.427779Z","shell.execute_reply.started":"2023-06-26T00:05:01.347228Z","shell.execute_reply":"2023-06-26T00:05:01.426783Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"\n\n\n\nInput Sequence:\nYou are a racist pig, loser. Don't vandalize again. [UNK] 28 Jan 2005 (UTC)                                                                  \n\nTarget Sequence:\nare a racist pig, loser. Don't vandalize again. [UNK] 28 Jan 2005 (UTC)                                                                   \n\n\n\n\nInput Sequence:\nYou [UNK] are all just [UNK] wannabes ! None of you have contributed one word to the Dog fighting article and none of you even know about Dog [UNK] Any Of You Know Have Any [UNK] I Can [UNK]                                         \n\nTarget Sequence:\n[UNK] are all just [UNK] wannabes ! None of you have contributed one word to the Dog fighting article and none of you even know about Dog [UNK] Any Of You Know Have Any [UNK] I Can [UNK]                                          \n\n\n\n\nInput Sequence:\n[UNK] claim I am most certainly not a sockpuppet of [UNK] please retract your [UNK] claim at once.                                                              \n\nTarget Sequence:\nclaim I am most certainly not a sockpuppet of [UNK] please retract your [UNK] claim at once.                                                               \n\n\n\n\nInput Sequence:\n[UNK] But the real question is WHY is that [UNK] Is it [UNK] Why don't you also write about how my [UNK] [UNK] Shall I write about how you pick your nose in public and then eat your [UNK] Shall I write that you never wipe your arse well enough and that there are always urine stains down your pants and you frequently forget to [UNK] your fly as you eat [UNK] [UNK] I write in the article about you\n\nTarget Sequence:\nBut the real question is WHY is that [UNK] Is it [UNK] Why don't you also write about how my [UNK] [UNK] Shall I write about how you pick your nose in public and then eat your [UNK] Shall I write that you never wipe your arse well enough and that there are always urine stains down your pants and you frequently forget to [UNK] your fly as you eat [UNK] [UNK] I write in the article about you that\n\n\n\n\nInput Sequence:\n\" Not similar to racial [UNK] I'm having difficulty seeing the [UNK] between those who don't like [UNK] because they are worried about [UNK] becoming \"\"Jewish\"\" and those who don't like [UNK] because they are worried about [UNK] becoming [UNK] Whatever the rights and [UNK] of the views, they certainly do seem very similar. If you are arguing that in one case the distinction is racial and in the other [UNK] okay, that's reasonable, but you should note that [UNK]\n\nTarget Sequence:\nNot similar to racial [UNK] I'm having difficulty seeing the [UNK] between those who don't like [UNK] because they are worried about [UNK] becoming \"\"Jewish\"\" and those who don't like [UNK] because they are worried about [UNK] becoming [UNK] Whatever the rights and [UNK] of the views, they certainly do seem very similar. If you are arguing that in one case the distinction is racial and in the other [UNK] okay, that's reasonable, but you should note that [UNK] and\n","output_type":"stream"}]},{"cell_type":"markdown","source":"* ***We can see that the target or label sequence is merely our ground truth text sequence we have just shifted by `1` token. This is what our model will use to evaluate during training.***\n\n* ***Cell below was for loading in and preprocessing the IMBD movie quotes dataset. This is the dataset I tested this approach on first.***","metadata":{}},{"cell_type":"code","source":"# !curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n# !tar -xf aclImdb_v1.tar.gz\n\n# batch_size = 128\n\n# # The dataset contains each review in a separate text file\n# # The text files are present in four different folders\n# # Create a list all files\n# filenames = []\n# directories = [\n#     \"aclImdb/train/pos\",\n#     \"aclImdb/train/neg\",\n#     \"aclImdb/test/pos\",\n#     \"aclImdb/test/neg\",\n# ]\n# for dir in directories:\n#     for f in os.listdir(dir):\n#         filenames.append(os.path.join(dir, f))\n\n# print(f\"{len(filenames)} files\")\n\n# # Create a dataset from text files\n# random.shuffle(filenames)\n# text_ds = tf.data.TextLineDataset(filenames)\n# text_ds = text_ds.shuffle(buffer_size=256)\n# text_ds = text_ds.batch(batch_size)\n\n# def custom_standardization(input_string):\n#     \"\"\" Remove html line-break tags and handle punctuation \"\"\"\n#     lowercased = tf.strings.lower(input_string)\n#     stripped_html = tf.strings.regex_replace(lowercased, \"<br />\", \" \")\n#     return tf.strings.regex_replace(stripped_html, f\"([{string.punctuation}])\", r\" \\1\")\n\n\n# # Create a vectorization layer and adapt it to the text\n# vectorize_layer = TextVectorization(\n#     standardize=custom_standardization,\n#     max_tokens=vocab_size - 1,\n#     output_mode=\"int\",\n#     output_sequence_length=maxlen + 1,\n# )\n# vectorize_layer.adapt(text_ds)\n# vocab = vectorize_layer.get_vocabulary()  # To get words back from token indices\n\n# ## Functoin to create target column\n# def prepare_lm_inputs_labels(text):\n#     \"\"\"\n#     Shift word sequences by 1 position so that the target for position (i) is\n#     word at position (i+1). The model will use all words up till position (i)\n#     to predict the next word.\n#     \"\"\"\n#     text = tf.expand_dims(text, -1)\n#     tokenized_sentences = vectorize_layer(text)\n#     x = tokenized_sentences[:, :-1]\n#     y = tokenized_sentences[:, 1:]\n#     return x, y\n\n\n# text_ds = text_ds.map(prepare_lm_inputs_labels)\n# text_ds = text_ds.prefetch(tf.data.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2023-06-26T00:05:01.429602Z","iopub.execute_input":"2023-06-26T00:05:01.430302Z","iopub.status.idle":"2023-06-26T00:05:01.437381Z","shell.execute_reply.started":"2023-06-26T00:05:01.430266Z","shell.execute_reply":"2023-06-26T00:05:01.436169Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Implement the Transformer Block and Attention Head","metadata":{}},{"cell_type":"code","source":"def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n    \"\"\"\n    Creates a mask for causal (auto-regressive) self-attention. The returned mask has the shape \n    [batch_size, n_dest, n_src], where each entry at position (i, j, k) will be 1 if j >= k and 0 otherwise. \n    This is used to prevent the attention mechanism from attending to future positions during the forward pass.\n\n    Args:\n        batch_size (int): Number of sequences in each batch.\n        n_dest (int): Number of destination attention heads.\n        n_src (int): Number of source attention heads.\n        dtype (tf.DType): Type of the output tensor.\n\n    Returns:\n        tf.Tensor: A tensor of shape [batch_size, n_dest, n_src] representing the mask.\n    \"\"\"\n\n    # Create two range tensors i and j, where i has shape [n_dest, 1] and j has shape [n_src]\n    i = tf.range(n_dest)[:, None]\n    j = tf.range(n_src)\n\n    # Create a mask where entry (i, j) is True if i >= j - n_src + n_dest and False otherwise\n    m = i >= j - n_src + n_dest\n\n    # Cast the mask to the desired data type\n    mask = tf.cast(m, dtype)\n\n    # Reshape the mask to have shape [1, n_dest, n_src]\n    mask = tf.reshape(mask, [1, n_dest, n_src])\n\n    # Create a tensor with shape [2] that represents the multiples for tiling\n    mult = tf.concat(\n        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n    )\n\n    # Tile the mask tensor to have shape [batch_size, n_dest, n_src]\n    return tf.tile(mask, mult)\n\n\n\nclass TransformerBlock(layers.Layer):\n    \"\"\"\n    A Transformer block that includes multi-head self-attention and a feed-forward neural network.\n    Each of these two components has a residual connection and is followed by layer normalization.\n\n    Attributes:\n        att (layers.MultiHeadAttention): Multi-head self-attention layer.\n        ffn (keras.Sequential): Feed-forward neural network.\n        layernorm1 (layers.LayerNormalization): Layer normalization after the self-attention.\n        layernorm2 (layers.LayerNormalization): Layer normalization after the feed-forward network.\n        dropout1 (layers.Dropout): Dropout layer after the self-attention.\n        dropout2 (layers.Dropout): Dropout layer after the feed-forward network.\n    \"\"\"\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n        \"\"\"\n        Initializes the Transformer block.\n\n        Args:\n            embed_dim (int): Dimensionality of the input embeddings.\n            num_heads (int): Number of attention heads.\n            ff_dim (int): Number of units in the hidden layer of the feed-forward network.\n            rate (float): Dropout rate.\n        \"\"\"\n        super().__init__()\n        self.att = layers.MultiHeadAttention(num_heads, embed_dim)\n        self.ffn = keras.Sequential(\n            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n        )\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(rate)\n        self.dropout2 = layers.Dropout(rate)\n\n    def call(self, inputs):\n        \"\"\"\n        Forward pass of the Transformer block.\n\n        Args:\n            inputs (tf.Tensor): Input tensor of shape [batch_size, seq_len, embed_dim].\n\n        Returns:\n            tf.Tensor: Output tensor of shape [batch_size, seq_len, embed_dim].\n        \"\"\"\n        # Compute the shapes\n        input_shape = tf.shape(inputs)\n        batch_size = input_shape[0]\n        seq_len = input_shape[1]\n\n        # Create the causal mask for the multi-head self-attention\n        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n\n        # Compute the output of the multi-head self-attention\n        attention_output = self.att(inputs, inputs, attention_mask=causal_mask)\n\n        # Apply dropout to the attention output\n        attention_output = self.dropout1(attention_output)\n\n        # Add the attention output to the inputs (residual connection) and normalize the result\n        out1 = self.layernorm1(inputs + attention_output)\n\n        # Compute the output of the feed-forward network\n        ffn_output = self.ffn(out1)\n\n        # Apply dropout to the feed-forward output\n        ffn_output = self.dropout2(ffn_output)\n\n        # Add the feed-forward output to the previous output (residual connection) and normalize the result\n        return self.layernorm2(out1 + ffn_output)","metadata":{"execution":{"iopub.status.busy":"2023-06-26T00:48:33.642179Z","iopub.execute_input":"2023-06-26T00:48:33.642602Z","iopub.status.idle":"2023-06-26T00:48:33.658666Z","shell.execute_reply.started":"2023-06-26T00:48:33.642569Z","shell.execute_reply":"2023-06-26T00:48:33.657641Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"# def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n#     \"\"\"\n#     Mask the upper half of the dot product matrix in self attention.\n#     This prevents flow of information from future tokens to current token.\n#     1's in the lower triangle, counting from the lower right corner.\n#     \"\"\"\n#     i = tf.range(n_dest)[:, None]\n#     j = tf.range(n_src)\n#     m = i >= j - n_src + n_dest\n#     mask = tf.cast(m, dtype)\n#     mask = tf.reshape(mask, [1, n_dest, n_src])\n#     mult = tf.concat(\n#         [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n#     )\n#     return tf.tile(mask, mult)\n\n\n# class TransformerBlock(layers.Layer):\n#     def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n#         super().__init__()\n#         self.att = layers.MultiHeadAttention(num_heads, embed_dim)\n#         self.ffn = keras.Sequential(\n#             [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n#         )\n#         self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n#         self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n#         self.dropout1 = layers.Dropout(rate)\n#         self.dropout2 = layers.Dropout(rate)\n\n#     def call(self, inputs):\n#         input_shape = tf.shape(inputs)\n#         batch_size = input_shape[0]\n#         seq_len = input_shape[1]\n#         causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n#         attention_output = self.att(inputs, inputs, attention_mask=causal_mask)\n#         attention_output = self.dropout1(attention_output)\n#         out1 = self.layernorm1(inputs + attention_output)\n#         ffn_output = self.ffn(out1)\n#         ffn_output = self.dropout2(ffn_output)\n#         return self.layernorm2(out1 + ffn_output)","metadata":{"execution":{"iopub.status.busy":"2023-06-26T00:05:01.439047Z","iopub.execute_input":"2023-06-26T00:05:01.439481Z","iopub.status.idle":"2023-06-26T00:05:01.453723Z","shell.execute_reply.started":"2023-06-26T00:05:01.439448Z","shell.execute_reply":"2023-06-26T00:05:01.452767Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Implement Embedding layer\n\n***Create two separate embedding layers:***\n\n1) One for tokens \n2) One for token indices(positions).","metadata":{}},{"cell_type":"code","source":"class TokenAndPositionEmbedding(layers.Layer):\n    \"\"\"\n    Layer for combining token and positional embeddings. Token embeddings provide the model\n    with understanding of the meaning of each token, while positional embeddings provide\n    information about the position of each token in the sequence.\n\n    Attributes:\n        token_emb (layers.Embedding): Token embedding layer.\n        pos_emb (layers.Embedding): Position embedding layer.\n    \"\"\"\n    def __init__(self, maxlen, vocab_size, embed_dim):\n        \"\"\"\n        Initializes the TokenAndPositionEmbedding layer.\n\n        Args:\n            maxlen (int): Maximum length of the sequences for positional encoding.\n            vocab_size (int): Size of the vocabulary for token encoding.\n            embed_dim (int): Dimensionality of the output embeddings.\n        \"\"\"\n        super().__init__()\n        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n\n    def call(self, x):\n        \"\"\"\n        Forward pass of the TokenAndPositionEmbedding layer.\n\n        Args:\n            x (tf.Tensor): Input tensor of shape [batch_size, seq_len].\n\n        Returns:\n            tf.Tensor: Output tensor of shape [batch_size, seq_len, embed_dim], resulting from\n            adding token embeddings and position embeddings.\n        \"\"\"\n        # Compute the maximum sequence length\n        maxlen = tf.shape(x)[-1]\n\n        # Create a range tensor representing positions\n        positions = tf.range(start=0, limit=maxlen, delta=1)\n\n        # Compute the position embeddings\n        positions = self.pos_emb(positions)\n\n        # Compute the token embeddings\n        x = self.token_emb(x)\n\n        # Add the token embeddings and position embeddings\n        return x + positions","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class TokenAndPositionEmbedding(layers.Layer):\n#     def __init__(self, maxlen, vocab_size, embed_dim):\n#         super().__init__()\n#         self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n#         self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n\n#     def call(self, x):\n#         maxlen = tf.shape(x)[-1]\n#         positions = tf.range(start=0, limit=maxlen, delta=1)\n#         positions = self.pos_emb(positions)\n#         x = self.token_emb(x)\n#         return x + positions","metadata":{"execution":{"iopub.status.busy":"2023-06-26T00:05:01.454963Z","iopub.execute_input":"2023-06-26T00:05:01.455309Z","iopub.status.idle":"2023-06-26T00:05:01.465654Z","shell.execute_reply.started":"2023-06-26T00:05:01.455279Z","shell.execute_reply":"2023-06-26T00:05:01.464522Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## Implement the miniature GPT model","metadata":{}},{"cell_type":"code","source":"# vocab_size = 20000  # Only consider the top 20k words\n# maxlen = 80  # Max sequence size\nembed_dim = 256  # Embedding size for each token\nnum_heads = 2  # Number of attention heads\nfeed_forward_dim = 256  # Hidden layer size in feed forward network inside transformer\n\n\ndef MiniGPT():\n    inputs = layers.Input(shape=(maxlen,), dtype=tf.int32)\n    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n    x = embedding_layer(inputs)\n    transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim)\n    x = transformer_block(x)\n    outputs = layers.Dense(vocab_size)(x)\n    model = keras.Model(inputs=inputs, outputs=[outputs, x])\n    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n    model.compile(\n        \"adam\", loss=[loss_fn, None],\n    )  # No loss and optimization based on word embeddings from transformer block\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-06-26T00:27:57.003097Z","iopub.execute_input":"2023-06-26T00:27:57.003482Z","iopub.status.idle":"2023-06-26T00:27:57.011381Z","shell.execute_reply.started":"2023-06-26T00:27:57.003450Z","shell.execute_reply":"2023-06-26T00:27:57.010230Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"## Implement a Keras callback for generating text","metadata":{}},{"cell_type":"code","source":"class TextGenerator(keras.callbacks.Callback):\n    \"\"\"A callback to generate text from a trained model.\n    1. Feed some starting prompt to the model\n    2. Predict probabilities for the next token\n    3. Sample the next token and add it to the next input\n\n    Arguments:\n        max_tokens: Integer, the number of tokens to be generated after prompt.\n        start_tokens: List of integers, the token indices for the starting prompt.\n        index_to_word: List of strings, obtained from the TextVectorization layer.\n        top_k: Integer, sample from the `top_k` token predictions.\n        print_every: Integer, print after this many epochs.\n    \"\"\"\n\n    def __init__(\n        self, max_tokens, start_tokens, index_to_word, top_k=10, print_every=10\n    ):\n        self.max_tokens = max_tokens\n        self.start_tokens = start_tokens\n        self.index_to_word = index_to_word\n        self.print_every = print_every\n        self.k = top_k\n\n    def sample_from(self, logits):\n        logits, indices = tf.math.top_k(logits, k=self.k, sorted=True)\n        indices = np.asarray(indices).astype(\"int32\")\n        preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n        preds = np.asarray(preds).astype(\"float32\")\n        return np.random.choice(indices, p=preds)\n\n    def detokenize(self, number):\n        return self.index_to_word[number]\n\n    def on_epoch_end(self, epoch, logs=None):\n        start_tokens = [_ for _ in self.start_tokens]\n        if (epoch + 1) % self.print_every != 0:\n            return\n        num_tokens_generated = 0\n        tokens_generated = []\n        while num_tokens_generated <= self.max_tokens:\n            pad_len = maxlen - len(start_tokens)\n            sample_index = len(start_tokens) - 1\n            if pad_len < 0:\n                x = start_tokens[:maxlen]\n                sample_index = maxlen - 1\n            elif pad_len > 0:\n                x = start_tokens + [0] * pad_len\n            else:\n                x = start_tokens\n            x = np.array([x])\n            y, _ = self.model.predict(x)\n            sample_token = self.sample_from(y[0][sample_index])\n            tokens_generated.append(sample_token)\n            start_tokens.append(sample_token)\n            num_tokens_generated = len(tokens_generated)\n        txt = \" \".join(\n            [self.detokenize(_) for _ in self.start_tokens + tokens_generated]\n        )\n        print(f\"generated text:\\n{txt}\\n\")\n        \n        \n\n# Tokenize starting prompt\nword_to_index = {}\nfor index, word in enumerate(vocab):\n    word_to_index[word] = index\n","metadata":{"execution":{"iopub.status.busy":"2023-06-26T00:27:58.747094Z","iopub.execute_input":"2023-06-26T00:27:58.747475Z","iopub.status.idle":"2023-06-26T00:27:58.769434Z","shell.execute_reply.started":"2023-06-26T00:27:58.747440Z","shell.execute_reply":"2023-06-26T00:27:58.768456Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"class TextGenerator(keras.callbacks.Callback):\n    \"\"\"A callback to generate text from a trained model.\n    1. Feed some starting prompt to the model\n    2. Predict probabilities for the next token\n    3. Sample the next token and add it to the next input\n\n    Arguments:\n        max_tokens: Integer, the number of tokens to be generated after prompt.\n        start_tokens: List of integers, the token indices for the starting prompt.\n        index_to_word: List of strings, obtained from the TextVectorization layer.\n        top_k: Integer, sample from the `top_k` token predictions.\n        print_every: Integer, print after this many epochs.\n    \"\"\"\n\n    def __init__(\n        self, max_tokens, start_tokens, index_to_word, top_k=10, print_every=1\n    ):\n        self.max_tokens = max_tokens\n        self.start_tokens = start_tokens\n        self.index_to_word = index_to_word\n        self.print_every = print_every\n        self.k = top_k\n        \n## Fix function below for future ease of use      \n#     def predict(start_prompt=''):\n#         \"\"\"Function to generate text sequence from new starting prompt.\"\"\"\n#         new_start_prompt = start_prompt\n#         new_start_tokens = [word_to_index.get(word, 1) for word in new_start_prompt.split()]\n        \n#         text_gen_callback.start_tokens = new_start_tokens\n#         text_gen_callback.on_epoch_end(0)\n\n        \n    def sample_from(self, logits):\n        logits, indices = tf.math.top_k(logits, k=self.k, sorted=True)\n        indices = np.asarray(indices).astype(\"int32\")\n        preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n        preds = np.asarray(preds).astype(\"float32\")\n        return np.random.choice(indices, p=preds)\n\n    \n    def detokenize(self, number):\n        return self.index_to_word[number]\n\n    \n    def on_epoch_end(self, epoch, logs=None):\n        start_tokens = [_ for _ in self.start_tokens]\n        if (epoch + 1) % self.print_every != 0:\n            return\n        num_tokens_generated = 0\n        tokens_generated = []\n        while num_tokens_generated <= self.max_tokens:\n            pad_len = maxlen - len(start_tokens)\n            sample_index = len(start_tokens) - 1\n            if pad_len < 0:\n                x = start_tokens[:maxlen]\n                sample_index = maxlen - 1\n            elif pad_len > 0:\n                x = start_tokens + [0] * pad_len\n            else:\n                x = start_tokens\n            x = np.array([x])\n            y, _ = self.model.predict(x)\n            sample_token = self.sample_from(y[0][sample_index])\n            tokens_generated.append(sample_token)\n            start_tokens.append(sample_token)\n            num_tokens_generated = len(tokens_generated)\n        txt = \" \".join(\n            [self.detokenize(_) for _ in self.start_tokens + tokens_generated]\n        )\n        print(f\"generated text:\\n{txt}\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-06-26T00:44:58.834742Z","iopub.execute_input":"2023-06-26T00:44:58.835121Z","iopub.status.idle":"2023-06-26T00:44:58.851027Z","shell.execute_reply.started":"2023-06-26T00:44:58.835091Z","shell.execute_reply":"2023-06-26T00:44:58.850116Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"## Tokenize starting prompt\nword_to_index = {}\nfor index, word in enumerate(vocab):\n    word_to_index[word] = index","metadata":{"execution":{"iopub.status.busy":"2023-06-26T00:44:59.238648Z","iopub.execute_input":"2023-06-26T00:44:59.239545Z","iopub.status.idle":"2023-06-26T00:44:59.252846Z","shell.execute_reply.started":"2023-06-26T00:44:59.239510Z","shell.execute_reply":"2023-06-26T00:44:59.251883Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"start_prompt = \"I would have\"\n\nstart_tokens = [word_to_index.get(_, 1) for _ in start_prompt.split()]\nnum_tokens_generated = 42\ntext_gen_callback = TextGenerator(num_tokens_generated, start_tokens, vocab)","metadata":{"execution":{"iopub.status.busy":"2023-06-26T00:44:59.771565Z","iopub.execute_input":"2023-06-26T00:44:59.772159Z","iopub.status.idle":"2023-06-26T00:44:59.777476Z","shell.execute_reply.started":"2023-06-26T00:44:59.772123Z","shell.execute_reply":"2023-06-26T00:44:59.776474Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":"## Training\n\n***I apologize for the scrolling your about to do. I wanted to generate text at each epoch so that along with loss there would be some qualitative evaluation on the models performance throughout training but I could not find a way to remove the progress bars for each step inside the epochs... If anyone reading this knows a way please comment.***","metadata":{}},{"cell_type":"code","source":"model = MiniGPT()\n\nN_EPOCHS = 1\nhistory  = model.fit(text_ds, verbose=0, epochs=N_EPOCHS, callbacks=[text_gen_callback])","metadata":{"execution":{"iopub.status.busy":"2023-06-26T00:45:03.339121Z","iopub.execute_input":"2023-06-26T00:45:03.339514Z","iopub.status.idle":"2023-06-26T00:45:18.823254Z","shell.execute_reply.started":"2023-06-26T00:45:03.339477Z","shell.execute_reply":"2023-06-26T00:45:18.822327Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stdout","text":"Epoch 1 completed.\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\ngenerated text:\nI would have been a coward for deleting enemies, to do everything do not get back off... [UNK]                            \n\n","output_type":"stream"},{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x78af76ddf400>"},"metadata":{}}]},{"cell_type":"markdown","source":"Now we can generate text continuuing from a new prmopt ","metadata":{}},{"cell_type":"code","source":"def predict(starting_prompt=''):\n    new_start_prompt = \"start something\"\n    new_start_tokens = [word_to_index.get(word, 1) for word in new_start_prompt.split()]\n\n    text_gen_callback.start_tokens = new_start_tokens\n    text_gen_callback.on_epoch_end(0)","metadata":{"execution":{"iopub.status.busy":"2023-06-26T00:30:29.137298Z","iopub.execute_input":"2023-06-26T00:30:29.138378Z","iopub.status.idle":"2023-06-26T00:30:29.144083Z","shell.execute_reply.started":"2023-06-26T00:30:29.138335Z","shell.execute_reply":"2023-06-26T00:30:29.142868Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"predict(\"you sure\")","metadata":{"execution":{"iopub.status.busy":"2023-06-26T00:30:30.209509Z","iopub.execute_input":"2023-06-26T00:30:30.209902Z","iopub.status.idle":"2023-06-26T00:30:34.998151Z","shell.execute_reply.started":"2023-06-26T00:30:30.209871Z","shell.execute_reply":"2023-06-26T00:30:34.997282Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 32ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\ngenerated text:\nstart something I have a [UNK] but it would like a [UNK]                                                                  \n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}