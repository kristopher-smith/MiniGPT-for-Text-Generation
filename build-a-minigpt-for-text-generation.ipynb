{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Generating Synthetic Data to Identify Toxic Comments Online","metadata":{}},{"cell_type":"markdown","source":"# ***WARNING*** \n\n## The data required to train the model for this task is known to be vulgar, offensive, toxic, racist, and otherwise not pleasant.","metadata":{}},{"cell_type":"markdown","source":"## Problem Statement\n\nToxic comments online come in many forms and in many arenas. There are currently several ways to mitigate these comments(for those organizations who wish to do so). Some of these ways include human moderators, and training machine learning models to detect toxicity in online comments.\n\nThe issue with human moderators is that some of these platforms have grown so large so quickly that there are not nearly enough moderators to achieve any sense of control for most of these comments. The shear volume of toxicity and bots online makes it unrealistic to think we could do this job with humans at this point.\n\nMany companies are employing mahcine learning to assist with identifying toxic comments online automatically. The problem with this approach is the lack of labeled training data to train the models on.\n\nThis is the problem I am going to solve using generative deep learning techniques. ","metadata":{}},{"cell_type":"markdown","source":"## Setup","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf, numpy as np, pandas as pd\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import TextVectorization\nimport os\nimport string\nimport random","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-25T05:36:27.796006Z","iopub.execute_input":"2023-06-25T05:36:27.796483Z","iopub.status.idle":"2023-06-25T05:36:35.648226Z","shell.execute_reply.started":"2023-06-25T05:36:27.796427Z","shell.execute_reply":"2023-06-25T05:36:35.647210Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Implement a Transformer block as a layer","metadata":{}},{"cell_type":"code","source":"def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n    \"\"\"\n    Mask the upper half of the dot product matrix in self attention.\n    This prevents flow of information from future tokens to current token.\n    1's in the lower triangle, counting from the lower right corner.\n    \"\"\"\n    i = tf.range(n_dest)[:, None]\n    j = tf.range(n_src)\n    m = i >= j - n_src + n_dest\n    mask = tf.cast(m, dtype)\n    mask = tf.reshape(mask, [1, n_dest, n_src])\n    mult = tf.concat(\n        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n    )\n    return tf.tile(mask, mult)\n\n\nclass TransformerBlock(layers.Layer):\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n        super().__init__()\n        self.att = layers.MultiHeadAttention(num_heads, embed_dim)\n        self.ffn = keras.Sequential(\n            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n        )\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(rate)\n        self.dropout2 = layers.Dropout(rate)\n\n    def call(self, inputs):\n        input_shape = tf.shape(inputs)\n        batch_size = input_shape[0]\n        seq_len = input_shape[1]\n        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n        attention_output = self.att(inputs, inputs, attention_mask=causal_mask)\n        attention_output = self.dropout1(attention_output)\n        out1 = self.layernorm1(inputs + attention_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output)\n        return self.layernorm2(out1 + ffn_output)","metadata":{"execution":{"iopub.status.busy":"2023-06-25T05:37:27.446004Z","iopub.execute_input":"2023-06-25T05:37:27.447145Z","iopub.status.idle":"2023-06-25T05:37:27.471134Z","shell.execute_reply.started":"2023-06-25T05:37:27.447097Z","shell.execute_reply":"2023-06-25T05:37:27.470117Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Implement an embedding layer\n\nCreate two separate embedding layers: one for tokens and one for token index (positions).","metadata":{}},{"cell_type":"code","source":"class TokenAndPositionEmbedding(layers.Layer):\n    def __init__(self, maxlen, vocab_size, embed_dim):\n        super().__init__()\n        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n\n    def call(self, x):\n        maxlen = tf.shape(x)[-1]\n        positions = tf.range(start=0, limit=maxlen, delta=1)\n        positions = self.pos_emb(positions)\n        x = self.token_emb(x)\n        return x + positions","metadata":{"execution":{"iopub.status.busy":"2023-06-25T05:37:30.068656Z","iopub.execute_input":"2023-06-25T05:37:30.069026Z","iopub.status.idle":"2023-06-25T05:37:30.076730Z","shell.execute_reply.started":"2023-06-25T05:37:30.068982Z","shell.execute_reply":"2023-06-25T05:37:30.075389Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Implement the miniature GPT model","metadata":{}},{"cell_type":"code","source":"vocab_size = 20000  # Only consider the top 20k words\nmaxlen = 80  # Max sequence size\nembed_dim = 256  # Embedding size for each token\nnum_heads = 2  # Number of attention heads\nfeed_forward_dim = 256  # Hidden layer size in feed forward network inside transformer\n\n\ndef create_model():\n    inputs = layers.Input(shape=(maxlen,), dtype=tf.int32)\n    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n    x = embedding_layer(inputs)\n    transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim)\n    x = transformer_block(x)\n    outputs = layers.Dense(vocab_size)(x)\n    model = keras.Model(inputs=inputs, outputs=[outputs, x])\n    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n    model.compile(\n        \"adam\", loss=[loss_fn, None],\n    )  # No loss and optimization based on word embeddings from transformer block\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-06-25T05:37:58.672406Z","iopub.execute_input":"2023-06-25T05:37:58.672938Z","iopub.status.idle":"2023-06-25T05:37:58.680932Z","shell.execute_reply.started":"2023-06-25T05:37:58.672906Z","shell.execute_reply":"2023-06-25T05:37:58.679858Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Prepare the data for word-level language modelling\n\nDownload the IMDB dataset and combine training and validation sets for a text generation task.","metadata":{}},{"cell_type":"code","source":"!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n!tar -xf aclImdb_v1.tar.gz","metadata":{"execution":{"iopub.status.busy":"2023-06-25T05:38:06.878767Z","iopub.execute_input":"2023-06-25T05:38:06.879158Z","iopub.status.idle":"2023-06-25T05:38:22.550239Z","shell.execute_reply.started":"2023-06-25T05:38:06.879126Z","shell.execute_reply":"2023-06-25T05:38:22.548836Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 80.2M  100 80.2M    0     0  10.4M      0  0:00:07  0:00:07 --:--:-- 17.7M\n","output_type":"stream"}]},{"cell_type":"code","source":"batch_size = 128\n\n# The dataset contains each review in a separate text file\n# The text files are present in four different folders\n# Create a list all files\nfilenames = []\ndirectories = [\n    \"aclImdb/train/pos\",\n    \"aclImdb/train/neg\",\n    \"aclImdb/test/pos\",\n    \"aclImdb/test/neg\",\n]\nfor dir in directories:\n    for f in os.listdir(dir):\n        filenames.append(os.path.join(dir, f))\n\nprint(f\"{len(filenames)} files\")\n\n# Create a dataset from text files\nrandom.shuffle(filenames)\ntext_ds = tf.data.TextLineDataset(filenames)\ntext_ds = text_ds.shuffle(buffer_size=256)\ntext_ds = text_ds.batch(batch_size)\n\n\ndef custom_standardization(input_string):\n    \"\"\" Remove html line-break tags and handle punctuation \"\"\"\n    lowercased = tf.strings.lower(input_string)\n    stripped_html = tf.strings.regex_replace(lowercased, \"<br />\", \" \")\n    return tf.strings.regex_replace(stripped_html, f\"([{string.punctuation}])\", r\" \\1\")\n\n\n# Create a vectorization layer and adapt it to the text\nvectorize_layer = TextVectorization(\n    standardize=custom_standardization,\n    max_tokens=vocab_size - 1,\n    output_mode=\"int\",\n    output_sequence_length=maxlen + 1,\n)\nvectorize_layer.adapt(text_ds)\nvocab = vectorize_layer.get_vocabulary()  # To get words back from token indices\n\n\ndef prepare_lm_inputs_labels(text):\n    \"\"\"\n    Shift word sequences by 1 position so that the target for position (i) is\n    word at position (i+1). The model will use all words up till position (i)\n    to predict the next word.\n    \"\"\"\n    text = tf.expand_dims(text, -1)\n    tokenized_sentences = vectorize_layer(text)\n    x = tokenized_sentences[:, :-1]\n    y = tokenized_sentences[:, 1:]\n    return x, y\n\n\ntext_ds = text_ds.map(prepare_lm_inputs_labels)\ntext_ds = text_ds.prefetch(tf.data.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2023-06-25T05:39:33.944050Z","iopub.execute_input":"2023-06-25T05:39:33.944446Z","iopub.status.idle":"2023-06-25T05:39:44.844726Z","shell.execute_reply.started":"2023-06-25T05:39:33.944415Z","shell.execute_reply":"2023-06-25T05:39:44.843642Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"50000 files\n","output_type":"stream"}]},{"cell_type":"code","source":"# data = pd.read_csv('../input/jigsaw-toxic-severity-rating/comments_to_score.csv')\n# text_column = data['text']\n\ndata = pd.read_csv('../input/jigsaw-toxic-severity-rating/validation_data.csv')\ntext_column = data['more_toxic']\n\nprint(len(text_column))\n# Create a dataset from the pandas column\ntext_ds = tf.data.Dataset.from_tensor_slices(text_column)\n\n# Shuffle and batch the dataset\ntext_ds = text_ds.shuffle(buffer_size=256)\ntext_ds = text_ds.batch(batch_size)\n\n# Create a vectorization layer and adapt it to the text\nvectorize_layer = TextVectorization(\n    standardize=custom_standardization,\n    max_tokens=vocab_size - 1,\n    output_mode=\"int\",\n    output_sequence_length=maxlen + 1,\n)\n\nvectorize_layer.adapt(text_ds)\nvocab = vectorize_layer.get_vocabulary()  # To get words back from token indices\n\ntext_ds = text_ds.map(prepare_lm_inputs_labels)\ntext_ds = text_ds.prefetch(tf.data.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2023-06-25T06:26:47.981653Z","iopub.execute_input":"2023-06-25T06:26:47.982053Z","iopub.status.idle":"2023-06-25T06:26:49.410168Z","shell.execute_reply.started":"2023-06-25T06:26:47.981999Z","shell.execute_reply":"2023-06-25T06:26:49.409191Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"30108\n","output_type":"stream"}]},{"cell_type":"code","source":"# Preview a sample from the text_ds dataset\nsample = text_ds.take(5)  # Take one sample from the dataset\n\nfor x, y in sample:\n    # Convert token indices back to words\n    input_words  = [vocab[i] for i in x[0].numpy()]\n    target_words = [vocab[i] for i in y[0].numpy()]\n\n    # Print the input and target sequences\n    print(\"Input Sequence:\")\n    print(\" \".join(input_words))\n    print(\"\\nTarget Sequence:\")\n    print(\" \".join(target_words))","metadata":{"execution":{"iopub.status.busy":"2023-06-25T06:26:53.772393Z","iopub.execute_input":"2023-06-25T06:26:53.772753Z","iopub.status.idle":"2023-06-25T06:26:53.832740Z","shell.execute_reply.started":"2023-06-25T06:26:53.772724Z","shell.execute_reply":"2023-06-25T06:26:53.831677Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"Input Sequence:\ngetting really annoyed ok the list here is barely [UNK] . it sucks . i don 't think we should change it because the [UNK] and other useful information supposedly don 't fit here . but why cant i put a simple link to a website that does have that information ? ? ? ? that website has all kinds of valuable information that is directly related to the article . yet some wikinazi thinks that this is his personal\n\nTarget Sequence:\nreally annoyed ok the list here is barely [UNK] . it sucks . i don 't think we should change it because the [UNK] and other useful information supposedly don 't fit here . but why cant i put a simple link to a website that does have that information ? ? ? ? that website has all kinds of valuable information that is directly related to the article . yet some wikinazi thinks that this is his personal article\nInput Sequence:\nyou are a fagget . i prey you get cancer and die a painful death .                                                                \n\nTarget Sequence:\nare a fagget . i prey you get cancer and die a painful death .                                                                 \nInput Sequence:\nwhy the fuck did you give me a last warning ? twat ! ! ! ! ! ! ! ! ! ! ! ! ! !                                                      \n\nTarget Sequence:\nthe fuck did you give me a last warning ? twat ! ! ! ! ! ! ! ! ! ! ! ! ! !                                                       \nInput Sequence:\nvandalise my ass = = = =                                                                         \n\nTarget Sequence:\nmy ass = = = =                                                                          \nInput Sequence:\npersonal attacks please do not post personal attacks on my talk page , i have deleted the one you did post . i think that 's very rude and out of order . how very [UNK] .67 .56 .138 reported i have reported you to jimbo wales and told him he must ban [UNK] .67 .56 .138                       \n\nTarget Sequence:\nattacks please do not post personal attacks on my talk page , i have deleted the one you did post . i think that 's very rude and out of order . how very [UNK] .67 .56 .138 reported i have reported you to jimbo wales and told him he must ban [UNK] .67 .56 .138                        \n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Implement a Keras callback for generating text","metadata":{}},{"cell_type":"code","source":"class TextGenerator(keras.callbacks.Callback):\n    \"\"\"A callback to generate text from a trained model.\n    1. Feed some starting prompt to the model\n    2. Predict probabilities for the next token\n    3. Sample the next token and add it to the next input\n\n    Arguments:\n        max_tokens: Integer, the number of tokens to be generated after prompt.\n        start_tokens: List of integers, the token indices for the starting prompt.\n        index_to_word: List of strings, obtained from the TextVectorization layer.\n        top_k: Integer, sample from the `top_k` token predictions.\n        print_every: Integer, print after this many epochs.\n    \"\"\"\n\n    def __init__(\n        self, max_tokens, start_tokens, index_to_word, top_k=10, print_every=1\n    ):\n        self.max_tokens = max_tokens\n        self.start_tokens = start_tokens\n        self.index_to_word = index_to_word\n        self.print_every = print_every\n        self.k = top_k\n\n    def sample_from(self, logits):\n        logits, indices = tf.math.top_k(logits, k=self.k, sorted=True)\n        indices = np.asarray(indices).astype(\"int32\")\n        preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n        preds = np.asarray(preds).astype(\"float32\")\n        return np.random.choice(indices, p=preds)\n\n    def detokenize(self, number):\n        return self.index_to_word[number]\n\n    def on_epoch_end(self, epoch, logs=None):\n        start_tokens = [_ for _ in self.start_tokens]\n        if (epoch + 1) % self.print_every != 0:\n            return\n        num_tokens_generated = 0\n        tokens_generated = []\n        while num_tokens_generated <= self.max_tokens:\n            pad_len = maxlen - len(start_tokens)\n            sample_index = len(start_tokens) - 1\n            if pad_len < 0:\n                x = start_tokens[:maxlen]\n                sample_index = maxlen - 1\n            elif pad_len > 0:\n                x = start_tokens + [0] * pad_len\n            else:\n                x = start_tokens\n            x = np.array([x])\n            y, _ = self.model.predict(x)\n            sample_token = self.sample_from(y[0][sample_index])\n            tokens_generated.append(sample_token)\n            start_tokens.append(sample_token)\n            num_tokens_generated = len(tokens_generated)\n        txt = \" \".join(\n            [self.detokenize(_) for _ in self.start_tokens + tokens_generated]\n        )\n        print(f\"generated text:\\n{txt}\\n\")\n        \n        \n\n# Tokenize starting prompt\nword_to_index = {}\nfor index, word in enumerate(vocab):\n    word_to_index[word] = index\n","metadata":{"execution":{"iopub.status.busy":"2023-06-25T06:27:05.170773Z","iopub.execute_input":"2023-06-25T06:27:05.171154Z","iopub.status.idle":"2023-06-25T06:27:05.190218Z","shell.execute_reply.started":"2023-06-25T06:27:05.171125Z","shell.execute_reply":"2023-06-25T06:27:05.189163Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"start_prompt = \"this movie is\"\nstart_prompt = \"what is the\"\n\n\nstart_tokens = [word_to_index.get(_, 1) for _ in start_prompt.split()]\nnum_tokens_generated = 40\ntext_gen_callback = TextGenerator(num_tokens_generated, start_tokens, vocab)","metadata":{"execution":{"iopub.status.busy":"2023-06-25T06:46:38.658404Z","iopub.execute_input":"2023-06-25T06:46:38.658953Z","iopub.status.idle":"2023-06-25T06:46:38.669091Z","shell.execute_reply.started":"2023-06-25T06:46:38.658911Z","shell.execute_reply":"2023-06-25T06:46:38.667948Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"markdown","source":"## Train the model","metadata":{}},{"cell_type":"code","source":"model = create_model()\n\nN_EPOCHS = 25\n\nmodel.fit(text_ds, verbose=2, epochs=N_EPOCHS, callbacks=[text_gen_callback])","metadata":{"execution":{"iopub.status.busy":"2023-06-25T06:46:39.968440Z","iopub.execute_input":"2023-06-25T06:46:39.968843Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/25\n1/1 [==============================] - 0s 225ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\ngenerated text:\nwhat is the hell is my talk                                     \n\n236/236 - 40s - loss: 3.1964 - dense_23_loss: 3.1964 - 40s/epoch - 170ms/step\nEpoch 2/25\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Now we can generate text continuuing from a new prmopt ","metadata":{}},{"cell_type":"code","source":"new_start_prompt = \"start something\"\nnew_start_tokens = [word_to_index.get(word, 1) for word in new_start_prompt.split()]\n\ntext_gen_callback.start_tokens = new_start_tokens\ntext_gen_callback.on_epoch_end(0)","metadata":{"execution":{"iopub.status.busy":"2023-06-25T06:45:57.793162Z","iopub.execute_input":"2023-06-25T06:45:57.793587Z","iopub.status.idle":"2023-06-25T06:46:03.501677Z","shell.execute_reply.started":"2023-06-25T06:45:57.793559Z","shell.execute_reply":"2023-06-25T06:46:03.500671Z"},"trusted":true},"execution_count":64,"outputs":[{"name":"stdout","text":"1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 45ms/step\n1/1 [==============================] - 0s 36ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 67ms/step\n1/1 [==============================] - 0s 33ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 31ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 31ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\ngenerated text:\n[UNK] is a new starting [UNK] of the [UNK] [UNK] is this site .                                                                         \n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}