{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# MiniGPT For Generating Synthetic Text Data\n\nby Kris Smith","metadata":{}},{"cell_type":"markdown","source":"# ***WARNING*** \n\n## The data required to train the model for this task is known to be vulgar, offensive, toxic, racist, and otherwise not pleasant.","metadata":{}},{"cell_type":"markdown","source":"## Problem Statement\n\nToxic comments online come in many forms and in many arenas. There are currently several ways to mitigate these comments(for those organizations who wish to do so). Some of these ways include human moderators, and training machine learning models to detect toxicity in online comments.\n\nThe issue with human moderators is that some of these platforms have grown so large so quickly that there are not nearly enough moderators to achieve any sense of control for most of these comments. The shear volume of toxicity and bots online makes it unrealistic to think we could do this job with humans at this point.\n\nMany companies are employing machine learning to assist with identifying toxic comments online automatically. The problem with this approach is the lack of labeled training data to train the models on.\n\nThis is the problem I am going to solve using generative deep learning techniques. ","metadata":{}},{"cell_type":"markdown","source":"## References\n\n* [Improving Language Understanding by Generative Pre-Training](https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035)\n\n* [Language Models are Unsupervised Multitask Learners](https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe)\n\n* [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)\n\n* Many of the ideas and code were adapted from this Keras resource: https://keras.io/examples/generative/text_generation_with_miniature_gpt/","metadata":{}},{"cell_type":"markdown","source":"## Import Libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport string\nimport random\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\n\nfrom nltk import ngrams\nfrom collections import Counter\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import TextVectorization\n\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-26T00:59:57.462889Z","iopub.execute_input":"2023-06-26T00:59:57.463289Z","iopub.status.idle":"2023-06-26T00:59:57.472415Z","shell.execute_reply.started":"2023-06-26T00:59:57.463259Z","shell.execute_reply":"2023-06-26T00:59:57.471536Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n","output_type":"stream"},{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"markdown","source":"## Data\n\nThe data I will be using to train the generative model was released on Kaggle as part of an ongoing series of competitions sponsored by the [Google company Jigsaw](https://en.wikipedia.org/wiki/Jigsaw_(company)).\n\nThe data consists of online comments with various severity levels of toxicity. There are versions of these comments labeled by human annotators wherein they label each comment as toxic or not, or other sets where they were labeled as different categories of toxic such as hatespeech, racist/sexist, obscene, etc. Although these are the labeled datasets we would be adding the synthetic data to in order to create more training data, for this task of simply generating similar text data we will only focus on the comments themselves.\n\nThe data provided by this competition includes a total of `14,251` unique toxic comments. Theses are the comments I will use to train the generative model with.","metadata":{}},{"cell_type":"markdown","source":"## EDA\n\nThe data came in two different files.\n\n1) Comments to score: This acts as a test dataset of comments for scoring after the model was trained.\n\n2) Validation data: This was the training data for the competition wherein there are two columns. One column labeled less toxic was a comment which human annotators labeled as less toxic than its more toxic counterpart in the other column. There was no actual training data where a comment was paired with its severity rating. The models were trained using creative techniques with the validation data and other classification data sets to train a model which predicted severity of comments.\n\nSince for our purposes we are only interested in the actual text comments themselves, I will only be using those columns from these datasources.\n\nI start by reading them all into pandas dataframes, isolating the text columns from each one, and stacking them all together so we have a single column of text when it is all said and done.\n","metadata":{}},{"cell_type":"code","source":"data1 = pd.read_csv('../input/jigsaw-toxic-severity-rating/comments_to_score.csv')\ndata1.info()\n\n## Isolate only text column\ndata1 = data1['text']\n\ndata1.sample(6)","metadata":{"execution":{"iopub.status.busy":"2023-06-26T00:59:57.482003Z","iopub.execute_input":"2023-06-26T00:59:57.482736Z","iopub.status.idle":"2023-06-26T00:59:57.538615Z","shell.execute_reply.started":"2023-06-26T00:59:57.482699Z","shell.execute_reply":"2023-06-26T00:59:57.537447Z"},"trusted":true},"execution_count":60,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 7537 entries, 0 to 7536\nData columns (total 2 columns):\n #   Column      Non-Null Count  Dtype \n---  ------      --------------  ----- \n 0   comment_id  7537 non-null   int64 \n 1   text        7537 non-null   object\ndtypes: int64(1), object(1)\nmemory usage: 117.9+ KB\n","output_type":"stream"},{"execution_count":60,"output_type":"execute_result","data":{"text/plain":"1157    \"what is that?? \"\"The Van Resistance was a def...\n2412    The majority are posting the exact opposite.  ...\n4678    Argentinosaurus\\nHello, I'm zh:User:Hoseumou f...\n2372    Wikipedia is really a joke. The Sanchez articl...\n6074               Get fucked. \\n\\nJust so you know. )   \n5508           bind, torture, kill. that's my philosophy.\nName: text, dtype: object"},"metadata":{}}]},{"cell_type":"markdown","source":"We can see the comments to score was the test file which contained only comments and their corresponding id's","metadata":{}},{"cell_type":"code","source":"data2 = pd.read_csv('../input/jigsaw-toxic-severity-rating/validation_data.csv')\ndata2.info()\n\ndata2.sample(6)","metadata":{"execution":{"iopub.status.busy":"2023-06-26T00:59:57.540688Z","iopub.execute_input":"2023-06-26T00:59:57.541335Z","iopub.status.idle":"2023-06-26T00:59:57.802519Z","shell.execute_reply.started":"2023-06-26T00:59:57.541302Z","shell.execute_reply":"2023-06-26T00:59:57.801610Z"},"trusted":true},"execution_count":61,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 30108 entries, 0 to 30107\nData columns (total 3 columns):\n #   Column      Non-Null Count  Dtype \n---  ------      --------------  ----- \n 0   worker      30108 non-null  int64 \n 1   less_toxic  30108 non-null  object\n 2   more_toxic  30108 non-null  object\ndtypes: int64(1), object(2)\nmemory usage: 705.8+ KB\n","output_type":"stream"},{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"       worker                                         less_toxic  \\\n28534     519   Mediocre American Man Trilogy  \\n\\ndoes anyon...   \n27885     752  I was blocked because I subscribe to the porno...   \n29994     707  Please stop adding this non-existent category,...   \n23288     479  \"\\n\\n Don't miss it! \\n\\nI am trying to get th...   \n18207     216                           Bellend \\n\\nUr a bellend   \n4616      358  \"\\n\\nAFL Attendance\\nYou know I mean \"\"stays\"\"...   \n\n                                              more_toxic  \n28534   dickhead \\n\\ndickhead im not vandalising, jus...  \n27885  People went to Iran for opportunities? You mak...  \n29994  \"\\nfuck you bastard. consider this your last w...  \n23288  U my fat furry friend are a cunt stop fuckin t...  \n18207  Oh my fucking god.  I'm in tears. I'm hurt.  I...  \n4616    Albanians are terrorists \\n\\nI know that all ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>worker</th>\n      <th>less_toxic</th>\n      <th>more_toxic</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>28534</th>\n      <td>519</td>\n      <td>Mediocre American Man Trilogy  \\n\\ndoes anyon...</td>\n      <td>dickhead \\n\\ndickhead im not vandalising, jus...</td>\n    </tr>\n    <tr>\n      <th>27885</th>\n      <td>752</td>\n      <td>I was blocked because I subscribe to the porno...</td>\n      <td>People went to Iran for opportunities? You mak...</td>\n    </tr>\n    <tr>\n      <th>29994</th>\n      <td>707</td>\n      <td>Please stop adding this non-existent category,...</td>\n      <td>\"\\nfuck you bastard. consider this your last w...</td>\n    </tr>\n    <tr>\n      <th>23288</th>\n      <td>479</td>\n      <td>\"\\n\\n Don't miss it! \\n\\nI am trying to get th...</td>\n      <td>U my fat furry friend are a cunt stop fuckin t...</td>\n    </tr>\n    <tr>\n      <th>18207</th>\n      <td>216</td>\n      <td>Bellend \\n\\nUr a bellend</td>\n      <td>Oh my fucking god.  I'm in tears. I'm hurt.  I...</td>\n    </tr>\n    <tr>\n      <th>4616</th>\n      <td>358</td>\n      <td>\"\\n\\nAFL Attendance\\nYou know I mean \"\"stays\"\"...</td>\n      <td>Albanians are terrorists \\n\\nI know that all ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"This was the data provided to validate the models performance during training. The three columns are workers(annotators) and the other two are text columns which we will use both to train our generative model with.","metadata":{}},{"cell_type":"markdown","source":"#### Combine all columns into a single column","metadata":{}},{"cell_type":"code","source":"## Isolate text column\ndata2 = data2['more_toxic']\n\n## Isolate text column\ndata3 = pd.read_csv('../input/jigsaw-toxic-severity-rating/validation_data.csv')\ndata3 = data3['less_toxic']\n\ntext_column = pd.concat([data1, data2, data3], axis=0, ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2023-06-26T00:59:57.804129Z","iopub.execute_input":"2023-06-26T00:59:57.804882Z","iopub.status.idle":"2023-06-26T00:59:58.035341Z","shell.execute_reply.started":"2023-06-26T00:59:57.804848Z","shell.execute_reply":"2023-06-26T00:59:58.034365Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"markdown","source":"#### Check for duplicates","metadata":{}},{"cell_type":"code","source":"text_column.value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-06-26T00:59:58.037903Z","iopub.execute_input":"2023-06-26T00:59:58.038357Z","iopub.status.idle":"2023-06-26T00:59:58.076489Z","shell.execute_reply.started":"2023-06-26T00:59:58.038320Z","shell.execute_reply":"2023-06-26T00:59:58.075432Z"},"trusted":true},"execution_count":63,"outputs":[{"execution_count":63,"output_type":"execute_result","data":{"text/plain":" sorry i jumped to conclusions \\n\\non christian terrorism article man, I don't agree with you, and I want you to go and listen to 'prophet of doom' (now in audio format) as it is good. But I was wrong to be so rude. It is not the Southern European way.                                                                                                                                                                               19\nthis irishtom guy is turning every article into an ad for islam                                                                                                                                                                                                                                                                                                                                                                            19\nYou are not sorry one damned bit.  You have yet to refute what I have written.  All you do is pass the insults as if it were salt on the dinner table.  This is on every article in which we disagree.  If you have something useful and constructive to say, then don't be a harpy troll.                                                                                                                                                 19\n YOUR BIASED! \\n\\nPLEASE OTHER THAN HIDE BEHIND WK RULES\\n\\nacutally IDENTITY THE OFFESNES COMMIMITED!\\n\\nYOU JUST SAID you dont care about my OPINIONS!..yet the opinions that where QUOTED WHERE FROM THE REFERENCES YOU HAD ACCEPTED!!\\n\\nLOL...\\n\\nSO in which case i am formally complaining about YOU AND YOUR BIASED STANCE!\\n\\nALL MY REFERNCES HAVE ISBN NUMBERS, YEAR AND PUBLISHERS!\\n\\nYOU ARE PROTECTING YOUR BIASED VIEW!    16\nI erased your cuss word\\nFrom: some random person out there in the world                                                                                                                                                                                                                                                                                                                                                                   16\n                                                                                                                                                                                                                                                                                                                                                                                                                                           ..\nHey\\n\\nI bet you Quinsareth are gay and like telling lies to your mother.                                                                                                                                                                                                                                                                                                                                                                   2\nVandalism on Muhammad page\\n\\nPerhaps that was the wrong way to deal with it, but                                                                                                                                                                                                                                                                                                                                                           2\n Thank You \\n\\nHey Nishkid I really appreciate the unblock.  Once again I apologize for any vandalism I caused on user pages and I have read Wikipedia's user policy.  Thank You!!!                                                                                                                                                                                                                                                         2\n Your low self-esteem \\n\\nI see you have such a low self-esteem that you have to warn others not to attack you.4.130.134.233                                                                                                                                                                                                                                                                                                                2\nVANDALISE MY ASS ==\\n\\n==                                                                                                                                                                                                                                                                                                                                                                                                                   2\nLength: 14251, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"It looks like between the data provided for the competition there are many duplicates. However we can see that some comments are reused many more times than other comments. For example the most used comments were repeated `19` times in the datasets while others only `2` times. \n\nSince the duplications are not balanced if we left the data like this I am afraid we would be biasing the model towards the comments which were present more in the data. \n\nI will remove all duplicate comments.","metadata":{}},{"cell_type":"code","source":"print(f\"Total numer of comments in text data = {len(text_column)}\")\nprint(f\"Numer of unique comments in text data = {len(text_column.unique())}\")\n\ntext_column = text_column.drop_duplicates()\nprint(\"Duplicate comments dropped\")","metadata":{"execution":{"iopub.status.busy":"2023-06-26T00:59:58.078100Z","iopub.execute_input":"2023-06-26T00:59:58.078464Z","iopub.status.idle":"2023-06-26T00:59:58.142687Z","shell.execute_reply.started":"2023-06-26T00:59:58.078431Z","shell.execute_reply":"2023-06-26T00:59:58.141724Z"},"trusted":true},"execution_count":64,"outputs":[{"name":"stdout","text":"Total numer of comments in text data = 67753\nNumer of unique comments in text data = 14251\nDuplicate comments dropped\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Exploring the toxic comments","metadata":{}},{"cell_type":"code","source":"data = pd.DataFrame()\ndata['text'] = text_column\ndata = data.sample(100)\n\n# Function to calculate word count\ndef count_words(text):\n    words = nltk.word_tokenize(text)\n    return len(words)\n\n# Function to calculate verb count\ndef count_verbs(text):\n    words = nltk.word_tokenize(text)\n    tagged_words = nltk.pos_tag(words)\n    verb_count = len([word for word, tag in tagged_words if tag.startswith('V')])\n    return verb_count\n\n# Function to calculate noun count\ndef count_nouns(text):\n    words = nltk.word_tokenize(text)\n    tagged_words = nltk.pos_tag(words)\n    noun_count = len([word for word, tag in tagged_words if tag.startswith('N')])\n    return noun_count\n\n# Add word count column\ndata['word_count'] = data['text'].apply(count_words)\n\n# Add verb count column\ndata['verb_count'] = data['text'].apply(count_verbs)\n\n# Add noun count column\ndata['noun_count'] = data['text'].apply(count_nouns)\n\ndata.describe()","metadata":{"execution":{"iopub.status.busy":"2023-06-26T00:59:58.143935Z","iopub.execute_input":"2023-06-26T00:59:58.144670Z","iopub.status.idle":"2023-06-26T00:59:59.439285Z","shell.execute_reply.started":"2023-06-26T00:59:58.144635Z","shell.execute_reply":"2023-06-26T00:59:59.438350Z"},"trusted":true},"execution_count":65,"outputs":[{"execution_count":65,"output_type":"execute_result","data":{"text/plain":"       word_count  verb_count  noun_count\ncount  100.000000  100.000000   100.00000\nmean    95.190000   15.540000    24.56000\nstd    146.229907   24.033403    46.92382\nmin      5.000000    0.000000     1.00000\n25%     19.000000    3.000000     5.00000\n50%     42.500000    7.000000    10.00000\n75%    111.750000   19.000000    26.00000\nmax    988.000000  175.000000   390.00000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word_count</th>\n      <th>verb_count</th>\n      <th>noun_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>100.00000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>95.190000</td>\n      <td>15.540000</td>\n      <td>24.56000</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>146.229907</td>\n      <td>24.033403</td>\n      <td>46.92382</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>5.000000</td>\n      <td>0.000000</td>\n      <td>1.00000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>19.000000</td>\n      <td>3.000000</td>\n      <td>5.00000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>42.500000</td>\n      <td>7.000000</td>\n      <td>10.00000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>111.750000</td>\n      <td>19.000000</td>\n      <td>26.00000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>988.000000</td>\n      <td>175.000000</td>\n      <td>390.00000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"ax = data['word_count'].plot(kind='kde')\ndata['verb_count'].plot(kind='kde', ax=ax)\ndata['noun_count'].plot(kind='kde', ax=ax)\n\nax.legend(['Word Count', 'Verb Count', 'Noun Count'])\nax.set_title('Distribution of Word Count, Verb Count, and Noun Count')\nax.set_xlabel('Count')\nax.set_ylabel('Density')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-26T00:59:59.440858Z","iopub.execute_input":"2023-06-26T00:59:59.441194Z","iopub.status.idle":"2023-06-26T00:59:59.867189Z","shell.execute_reply.started":"2023-06-26T00:59:59.441163Z","shell.execute_reply":"2023-06-26T00:59:59.866318Z"},"trusted":true},"execution_count":66,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"markdown","source":"We can see that these comments on average are quite short in length and contain more nouns than verbs on average.\n\nSince we have not done any cleaning of the data yet these distributions are not exact as the nltk package is not currently looking for misspelled words or different versions of word spellings which are used online sometimes.\n\nFor example if a user knows that the platform they are on has limitations on language than they may spell a profane word to try to fool any auto detecting systems such as `Fuck==>Fxck, F*ck, Fukk, Fuuu*uukk`, etc.\n\nTherefore these counts will not detect all nouns and verbs but should give a decent sample.\n\nKnowing the underlying distributions of some of these features is important because after the synthetic data is generated we would most likely want it to follow the same distributions for these attributes of the text. ","metadata":{}},{"cell_type":"markdown","source":"### Looking at the most common N-grams","metadata":{}},{"cell_type":"code","source":"# Tokenize the text into words\ndata['words'] = data['text'].apply(nltk.word_tokenize)\n\n# Get bigrams and trigrams for each row\ndata['bigrams']   = data['words'].apply(lambda x: list(ngrams(x, 2)))\ndata['trigrams']  = data['words'].apply(lambda x: list(ngrams(x, 3)))\n# data['quadgrams'] = data['words'].apply(lambda x: list(ngrams(x, 4)))\n\n# Count the occurrences of bigrams and trigrams\nbigram_counts   = Counter([gram for grams in data['bigrams'] for gram in grams])\ntrigram_counts  = Counter([gram for grams in data['trigrams'] for gram in grams])\n# quadgram_counts = Counter([gram for grams in data['quadgrams'] for gram in grams])\n\n# Get the most common bigrams, trigrams, and quadgrams\nmost_common_bigrams   = bigram_counts.most_common(50)\nmost_common_trigrams  = trigram_counts.most_common(50)\n# most_common_quadgrams = quadgram_counts.most_common(50)\n\ndf_common_grams = pd.DataFrame()\ndf_common_grams['bigrams']   = most_common_bigrams\ndf_common_grams['trigrams']  = most_common_trigrams\n# df_common_grams['quadgrams'] = most_common_quadgrams\n\n# # Display the results\n# print('Most common bigrams:')\n# for bigram, count in most_common_bigrams:\n#     print(' '.join(bigram), count)\n\n# print('\\nMost common trigrams:')\n# for trigram, count in most_common_trigrams:\n#     print(' '.join(trigram), count)\n    \n# print('\\nMost common quadgrams:')\n# for quadgram, count in most_common_quadgrams:\n#     print(' '.join(quadgram), count)\n\n\ndf_common_grams.iloc[:, :]","metadata":{"execution":{"iopub.status.busy":"2023-06-26T00:59:59.869655Z","iopub.execute_input":"2023-06-26T00:59:59.869984Z","iopub.status.idle":"2023-06-26T01:00:00.083934Z","shell.execute_reply.started":"2023-06-26T00:59:59.869958Z","shell.execute_reply":"2023-06-26T01:00:00.082899Z"},"trusted":true},"execution_count":67,"outputs":[{"execution_count":67,"output_type":"execute_result","data":{"text/plain":"                       bigrams                         trigrams\n0              ((A, DUMB), 92)      ((A, DUMB, CUNTFRANKS), 88)\n1     ((DUMB, CUNTFRANKS), 88)      ((DUMB, CUNTFRANKS, A), 88)\n2        ((CUNTFRANKS, A), 88)      ((CUNTFRANKS, A, DUMB), 88)\n3               (('', ''), 56)                  ((!, !, !), 23)\n4               ((``, ''), 52)    ((a, duplicate, article), 12)\n5                 ((!, !), 35)                  ((:, :, :), 11)\n6                 ((., I), 35)                 (('', '', .), 8)\n7              ((of, the), 35)     ((duplicate, article, .), 8)\n8                ((is, a), 24)                 (('', '', ,), 7)\n9               ((,, and), 23)                 ((,, it, 's), 7)\n10              ((., The), 22)                ((,, but, it), 7)\n11              ((,, but), 21)                   ((?, ?, ?), 6)\n12              ((., You), 19)                ((I, do, n't), 6)\n13              ((it, 's), 19)                ((is, a, sad), 6)\n14             ((do, n't), 18)            ((a, sad, little), 6)\n15             ((I, have), 17)          ((sad, little, man), 6)\n16             ((in, the), 17)         ((little, man, with), 6)\n17           ((that, you), 17)             ((man, with, no), 6)\n18                ((,, I), 16)           ((with, no, penis), 6)\n19          ((article, .), 16)            ((no, penis, and), 6)\n20             ((to, the), 14)            ((penis, and, he), 6)\n21               ((I, am), 14)           ((and, he, enjoys), 6)\n22            ((you, are), 14)       ((he, enjoys, sucking), 6)\n23                ((?, ?), 12)    ((enjoys, sucking, penis), 6)\n24        ((a, duplicate), 12)         ((sucking, penis, .), 6)\n25  ((duplicate, article), 12)           ((the, fact, that), 5)\n26                ((:, :), 12)           ((article, ., The), 5)\n27              ((to, be), 11)          ((of, the, article), 5)\n28        ((the, article), 11)                (((, talk, )), 5)\n29            ((,, which), 11)                  ((., I, 'm), 5)\n30              ((,, you), 11)                ((., If, you), 5)\n31             ((on, the), 11)             ((do, n't, know), 5)\n32               ((I, 'm), 10)               ((this, is, a), 5)\n33          ((talk, page), 10)                 ((it, 's, a), 5)\n34              ((,, the), 10)            ((penis, ., Love), 5)\n35             ((that, I), 10)             ((., Love, your), 5)\n36            ((for, the), 10)       ((Love, your, profile), 5)\n37           ((have, been), 9)         ((your, profile, Mr), 5)\n38              ((if, you), 9)       ((profile, Mr, Martin), 5)\n39              ((is, not), 9)        ((Mr, Martin, Conway), 5)\n40              ((http, :), 9)         ((Martin, Conway, .), 5)\n41               ((it, is), 9)      ((Conway, ., Desperado), 5)\n42                ((,, it), 9)        ((., Desperado, RSVP), 5)\n43                 ((!, I), 8)  ((Desperado, RSVP, profile), 5)\n44             ((want, to), 8)                ((I, 'm, not), 4)\n45                (('', .), 8)              ((., You, have), 4)\n46                ((me, .), 8)                 ((to, be, a), 4)\n47                ((,, as), 8)               ((you, are, a), 4)\n48                ((in, a), 8)            ((block, me, for), 4)\n49               ((., She), 8)               (('', '', and), 4)","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>bigrams</th>\n      <th>trigrams</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>((A, DUMB), 92)</td>\n      <td>((A, DUMB, CUNTFRANKS), 88)</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>((DUMB, CUNTFRANKS), 88)</td>\n      <td>((DUMB, CUNTFRANKS, A), 88)</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>((CUNTFRANKS, A), 88)</td>\n      <td>((CUNTFRANKS, A, DUMB), 88)</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>(('', ''), 56)</td>\n      <td>((!, !, !), 23)</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>((``, ''), 52)</td>\n      <td>((a, duplicate, article), 12)</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>((!, !), 35)</td>\n      <td>((:, :, :), 11)</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>((., I), 35)</td>\n      <td>(('', '', .), 8)</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>((of, the), 35)</td>\n      <td>((duplicate, article, .), 8)</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>((is, a), 24)</td>\n      <td>(('', '', ,), 7)</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>((,, and), 23)</td>\n      <td>((,, it, 's), 7)</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>((., The), 22)</td>\n      <td>((,, but, it), 7)</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>((,, but), 21)</td>\n      <td>((?, ?, ?), 6)</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>((., You), 19)</td>\n      <td>((I, do, n't), 6)</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>((it, 's), 19)</td>\n      <td>((is, a, sad), 6)</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>((do, n't), 18)</td>\n      <td>((a, sad, little), 6)</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>((I, have), 17)</td>\n      <td>((sad, little, man), 6)</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>((in, the), 17)</td>\n      <td>((little, man, with), 6)</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>((that, you), 17)</td>\n      <td>((man, with, no), 6)</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>((,, I), 16)</td>\n      <td>((with, no, penis), 6)</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>((article, .), 16)</td>\n      <td>((no, penis, and), 6)</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>((to, the), 14)</td>\n      <td>((penis, and, he), 6)</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>((I, am), 14)</td>\n      <td>((and, he, enjoys), 6)</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>((you, are), 14)</td>\n      <td>((he, enjoys, sucking), 6)</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>((?, ?), 12)</td>\n      <td>((enjoys, sucking, penis), 6)</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>((a, duplicate), 12)</td>\n      <td>((sucking, penis, .), 6)</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>((duplicate, article), 12)</td>\n      <td>((the, fact, that), 5)</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>((:, :), 12)</td>\n      <td>((article, ., The), 5)</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>((to, be), 11)</td>\n      <td>((of, the, article), 5)</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>((the, article), 11)</td>\n      <td>(((, talk, )), 5)</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>((,, which), 11)</td>\n      <td>((., I, 'm), 5)</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>((,, you), 11)</td>\n      <td>((., If, you), 5)</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>((on, the), 11)</td>\n      <td>((do, n't, know), 5)</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>((I, 'm), 10)</td>\n      <td>((this, is, a), 5)</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>((talk, page), 10)</td>\n      <td>((it, 's, a), 5)</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>((,, the), 10)</td>\n      <td>((penis, ., Love), 5)</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>((that, I), 10)</td>\n      <td>((., Love, your), 5)</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>((for, the), 10)</td>\n      <td>((Love, your, profile), 5)</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>((have, been), 9)</td>\n      <td>((your, profile, Mr), 5)</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>((if, you), 9)</td>\n      <td>((profile, Mr, Martin), 5)</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>((is, not), 9)</td>\n      <td>((Mr, Martin, Conway), 5)</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>((http, :), 9)</td>\n      <td>((Martin, Conway, .), 5)</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>((it, is), 9)</td>\n      <td>((Conway, ., Desperado), 5)</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>((,, it), 9)</td>\n      <td>((., Desperado, RSVP), 5)</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>((!, I), 8)</td>\n      <td>((Desperado, RSVP, profile), 5)</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>((want, to), 8)</td>\n      <td>((I, 'm, not), 4)</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>(('', .), 8)</td>\n      <td>((., You, have), 4)</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>((me, .), 8)</td>\n      <td>((to, be, a), 4)</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>((,, as), 8)</td>\n      <td>((you, are, a), 4)</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>((in, a), 8)</td>\n      <td>((block, me, for), 4)</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>((., She), 8)</td>\n      <td>(('', '', and), 4)</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"We can see the initial 10 or so most common bi-grams and tri-grams are repetitive punctuation marks.\n\nTraditionally these would be cleaned and removed when training models for NLP tasks, however due to the nature of this work many of these traditional techniques will limit the models ability to predict toxicity as well as with clean text.\n\nI happened to have competed in this competition and one thing all of us learned was that leaving capital letters and punctuation improved the models ability to infer toxicity and especially levels of toxicity. \n\nFor example a phrase such as:\n\n`Are you kidding?`\n\nConveys a much different meaning than the same words but put this way:\n\n`ARE YOU KIDDING!!!??`\n\nTraditional NLP techniques would have us convert all characters to lower case and remove punctuation so the model will interpret both of those texts the exact same way.\n\nWhen training sentiment based models or models where feeling and emotion is being conveyed in some way such as toxicity of comments, it is more than just the raw content of the words alone which gives the meaning. The puncuation and capitalizations are very expressive forms of language and as such for these problems do better left in the data.","metadata":{}},{"cell_type":"markdown","source":"## Pre-Processing\n\n* First we need load in our text column as tensorflow formatted dataset\n\n* Next we shuffle the data to avoid any patterns which may have been present\n\n* We then slice the data into batches for processing\n\n* Vectorize the text which will be used to create a corpus of vocabulary used when training and act as vector representations of our text\n\n* Create the corpus of vocabulary which is used to train and evaluate throughout","metadata":{}},{"cell_type":"code","source":"vocab_size = 20000  ## Only consider the top 20k words\nmaxlen = 80  ## Max sequence length\nbatch_size = 32  ## Data loading batch sizes\n\n# Create a dataset from the pandas column\ntext_ds = tf.data.Dataset.from_tensor_slices(text_column)\n\n# Shuffle and batch the dataset\ntext_ds = text_ds.shuffle(buffer_size=256)\ntext_ds = text_ds.batch(batch_size)\n\n# def custom_standardization(input_string):\n#     \"\"\" Remove html line-break tags and handle punctuation \"\"\"\n#     lowercased = tf.strings.lower(input_string)\n#     stripped_html = tf.strings.regex_replace(lowercased, \"<br />\", \" \")\n#     return tf.strings.regex_replace(stripped_html, f\"([{string.punctuation}])\", r\" \\1\")\n\n\n## Create a vectorization layer and adapt it to the text\nvectorize_layer = TextVectorization(\n    standardize=None,\n    max_tokens=vocab_size - 1,\n    output_mode=\"int\",\n    output_sequence_length=maxlen + 1,\n)\nvectorize_layer.adapt(text_ds)\nvocab = vectorize_layer.get_vocabulary()  ## To get words back from token indices","metadata":{"execution":{"iopub.status.busy":"2023-06-26T01:00:00.085475Z","iopub.execute_input":"2023-06-26T01:00:00.085896Z","iopub.status.idle":"2023-06-26T01:00:01.238600Z","shell.execute_reply.started":"2023-06-26T01:00:00.085863Z","shell.execute_reply":"2023-06-26T01:00:01.236791Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"markdown","source":"## Generate Labels\n\nSince we are building a generative auto-regressive model, we must train it to predict the next word by looking backwards and using the previous tokens to predict the highest probability for the next token.\n\nThis is fairly easy to create labels for because we simply shuffle the `TRUE` data be one token and then when training the model compares the predicted text with the next indexed word.\n\nWe can inspect what these samples and labels look like below:","metadata":{}},{"cell_type":"code","source":"## Function to create target column\ndef prepare_lm_inputs_labels(text):\n    \"\"\"\n    Shift word sequences by 1 position so that the target for position (i) is\n    word at position (i+1). The model will use all words up till position (i)\n    to predict the next word.\n    \"\"\"\n    text = tf.expand_dims(text, -1)\n    tokenized_sentences = vectorize_layer(text)\n    x = tokenized_sentences[:, :-1]\n    y = tokenized_sentences[:, 1:]\n    return x, y\n\n\ntext_ds = text_ds.map(prepare_lm_inputs_labels)\ntext_ds = text_ds.prefetch(tf.data.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2023-06-26T01:00:01.244498Z","iopub.execute_input":"2023-06-26T01:00:01.245366Z","iopub.status.idle":"2023-06-26T01:00:01.309501Z","shell.execute_reply.started":"2023-06-26T01:00:01.245323Z","shell.execute_reply":"2023-06-26T01:00:01.308616Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"## Select samples from the training data set to inspect\nsample = text_ds.take(5) \n\n## Display some samples\nfor x, y in sample:\n    # Convert token indices back to words\n    input_words  = [vocab[i] for i in x[0].numpy()]\n    target_words = [vocab[i] for i in y[0].numpy()]\n\n    print(\"\\n\\n\\n\\nInput Sequence:\")\n    print(\" \".join(input_words))\n    print(\"\\nTarget Sequence:\")\n    print(\" \".join(target_words))","metadata":{"execution":{"iopub.status.busy":"2023-06-26T01:00:01.310817Z","iopub.execute_input":"2023-06-26T01:00:01.311143Z","iopub.status.idle":"2023-06-26T01:00:01.360317Z","shell.execute_reply.started":"2023-06-26T01:00:01.311111Z","shell.execute_reply":"2023-06-26T01:00:01.359438Z"},"trusted":true},"execution_count":70,"outputs":[{"name":"stdout","text":"\n\n\n\nInput Sequence:\n\" I agree fully. There's nothing inherently pejorative about the term. \"\"Redskin\"\" and [UNK] [UNK] even enjoy a certain amount use as terms of [UNK] between the two [UNK] I imagine that there are some Native Americans who dislike the term (just as some, maybe even more [UNK] hate the word [UNK] just as there are some who find the terms white, black, and yellow offensive. [UNK] [UNK] is probably a special case, since it is sometimes associated with [UNK]\n\nTarget Sequence:\nI agree fully. There's nothing inherently pejorative about the term. \"\"Redskin\"\" and [UNK] [UNK] even enjoy a certain amount use as terms of [UNK] between the two [UNK] I imagine that there are some Native Americans who dislike the term (just as some, maybe even more [UNK] hate the word [UNK] just as there are some who find the terms white, black, and yellow offensive. [UNK] [UNK] is probably a special case, since it is sometimes associated with [UNK] there\n\n\n\n\nInput Sequence:\nP.S. Do You Have Know Any [UNK] I Can [UNK] ??                                                                     \n\nTarget Sequence:\nDo You Have Know Any [UNK] I Can [UNK] ??                                                                      \n\n\n\n\nInput Sequence:\n\" I'm going to pretend for just a moment, Fred, that you'd actually be interested in my advice. Please don't feel any need to insult me over this I don't have any real belief that your interested in anything I have to say. But anyway, in this imaginary world where you actually are interested in my advice, I would tell you that, in the capacity of an [UNK] you've written a summary of the focus of the dispute that (a)\n\nTarget Sequence:\nI'm going to pretend for just a moment, Fred, that you'd actually be interested in my advice. Please don't feel any need to insult me over this I don't have any real belief that your interested in anything I have to say. But anyway, in this imaginary world where you actually are interested in my advice, I would tell you that, in the capacity of an [UNK] you've written a summary of the focus of the dispute that (a) includes\n\n\n\n\nInput Sequence:\n\" and I will Dear [UNK] I give up. I have posted the following at page [UNK] Please unlock and DELETE all content authored by me and I will flag the pages I created for deletion. I will notify Dr. [UNK] [UNK] and I will be [UNK] some other on-line project for us to [UNK] I am less than pleased with the handling of this site and you have not kept your word to me. As such, I am [UNK]\n\nTarget Sequence:\nand I will Dear [UNK] I give up. I have posted the following at page [UNK] Please unlock and DELETE all content authored by me and I will flag the pages I created for deletion. I will notify Dr. [UNK] [UNK] and I will be [UNK] some other on-line project for us to [UNK] I am less than pleased with the handling of this site and you have not kept your word to me. As such, I am [UNK] to\n\n\n\n\nInput Sequence:\nFUCK YOU [UNK] GO TO [UNK]                                                                          \n\nTarget Sequence:\nYOU [UNK] GO TO [UNK]                                                                           \n","output_type":"stream"}]},{"cell_type":"markdown","source":"* ***We can see that the target or label sequence is merely our ground truth text sequence we have just shifted by `1` token. This is what our model will use to evaluate during training.***\n\n* ***Cell below was for loading in and preprocessing the IMBD movie quotes dataset. This is the dataset I tested this approach on first.***","metadata":{}},{"cell_type":"code","source":"# !curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n# !tar -xf aclImdb_v1.tar.gz\n\n# batch_size = 128\n\n# # The dataset contains each review in a separate text file\n# # The text files are present in four different folders\n# # Create a list all files\n# filenames = []\n# directories = [\n#     \"aclImdb/train/pos\",\n#     \"aclImdb/train/neg\",\n#     \"aclImdb/test/pos\",\n#     \"aclImdb/test/neg\",\n# ]\n# for dir in directories:\n#     for f in os.listdir(dir):\n#         filenames.append(os.path.join(dir, f))\n\n# print(f\"{len(filenames)} files\")\n\n# # Create a dataset from text files\n# random.shuffle(filenames)\n# text_ds = tf.data.TextLineDataset(filenames)\n# text_ds = text_ds.shuffle(buffer_size=256)\n# text_ds = text_ds.batch(batch_size)\n\n# def custom_standardization(input_string):\n#     \"\"\" Remove html line-break tags and handle punctuation \"\"\"\n#     lowercased = tf.strings.lower(input_string)\n#     stripped_html = tf.strings.regex_replace(lowercased, \"<br />\", \" \")\n#     return tf.strings.regex_replace(stripped_html, f\"([{string.punctuation}])\", r\" \\1\")\n\n\n# # Create a vectorization layer and adapt it to the text\n# vectorize_layer = TextVectorization(\n#     standardize=custom_standardization,\n#     max_tokens=vocab_size - 1,\n#     output_mode=\"int\",\n#     output_sequence_length=maxlen + 1,\n# )\n# vectorize_layer.adapt(text_ds)\n# vocab = vectorize_layer.get_vocabulary()  # To get words back from token indices\n\n# ## Functoin to create target column\n# def prepare_lm_inputs_labels(text):\n#     \"\"\"\n#     Shift word sequences by 1 position so that the target for position (i) is\n#     word at position (i+1). The model will use all words up till position (i)\n#     to predict the next word.\n#     \"\"\"\n#     text = tf.expand_dims(text, -1)\n#     tokenized_sentences = vectorize_layer(text)\n#     x = tokenized_sentences[:, :-1]\n#     y = tokenized_sentences[:, 1:]\n#     return x, y\n\n\n# text_ds = text_ds.map(prepare_lm_inputs_labels)\n# text_ds = text_ds.prefetch(tf.data.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2023-06-26T01:00:01.363644Z","iopub.execute_input":"2023-06-26T01:00:01.363918Z","iopub.status.idle":"2023-06-26T01:00:01.370325Z","shell.execute_reply.started":"2023-06-26T01:00:01.363892Z","shell.execute_reply":"2023-06-26T01:00:01.369359Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"markdown","source":"## Implement the Transformer Block and Attention Head","metadata":{}},{"cell_type":"code","source":"def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n    \"\"\"\n    Creates a mask for causal (auto-regressive) self-attention. The returned mask has the shape \n    [batch_size, n_dest, n_src], where each entry at position (i, j, k) will be 1 if j >= k and 0 otherwise. \n    This is used to prevent the attention mechanism from attending to future positions during the forward pass.\n\n    Args:\n        batch_size (int): Number of sequences in each batch.\n        n_dest (int): Number of destination attention heads.\n        n_src (int): Number of source attention heads.\n        dtype (tf.DType): Type of the output tensor.\n\n    Returns:\n        tf.Tensor: A tensor of shape [batch_size, n_dest, n_src] representing the mask.\n    \"\"\"\n\n    # Create two range tensors i and j, where i has shape [n_dest, 1] and j has shape [n_src]\n    i = tf.range(n_dest)[:, None]\n    j = tf.range(n_src)\n\n    # Create a mask where entry (i, j) is True if i >= j - n_src + n_dest and False otherwise\n    m = i >= j - n_src + n_dest\n\n    # Cast the mask to the desired data type\n    mask = tf.cast(m, dtype)\n\n    # Reshape the mask to have shape [1, n_dest, n_src]\n    mask = tf.reshape(mask, [1, n_dest, n_src])\n\n    # Create a tensor with shape [2] that represents the multiples for tiling\n    mult = tf.concat(\n        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n    )\n\n    # Tile the mask tensor to have shape [batch_size, n_dest, n_src]\n    return tf.tile(mask, mult)\n\n\n\nclass TransformerBlock(layers.Layer):\n    \"\"\"\n    A Transformer block that includes multi-head self-attention and a feed-forward neural network.\n    Each of these two components has a residual connection and is followed by layer normalization.\n\n    Attributes:\n        att (layers.MultiHeadAttention): Multi-head self-attention layer.\n        ffn (keras.Sequential): Feed-forward neural network.\n        layernorm1 (layers.LayerNormalization): Layer normalization after the self-attention.\n        layernorm2 (layers.LayerNormalization): Layer normalization after the feed-forward network.\n        dropout1 (layers.Dropout): Dropout layer after the self-attention.\n        dropout2 (layers.Dropout): Dropout layer after the feed-forward network.\n    \"\"\"\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n        \"\"\"\n        Initializes the Transformer block.\n\n        Args:\n            embed_dim (int): Dimensionality of the input embeddings.\n            num_heads (int): Number of attention heads.\n            ff_dim (int): Number of units in the hidden layer of the feed-forward network.\n            rate (float): Dropout rate.\n        \"\"\"\n        super().__init__()\n        self.att = layers.MultiHeadAttention(num_heads, embed_dim)\n        self.ffn = keras.Sequential(\n            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n        )\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(rate)\n        self.dropout2 = layers.Dropout(rate)\n\n    def call(self, inputs):\n        \"\"\"\n        Forward pass of the Transformer block.\n\n        Args:\n            inputs (tf.Tensor): Input tensor of shape [batch_size, seq_len, embed_dim].\n\n        Returns:\n            tf.Tensor: Output tensor of shape [batch_size, seq_len, embed_dim].\n        \"\"\"\n        # Compute the shapes\n        input_shape = tf.shape(inputs)\n        batch_size = input_shape[0]\n        seq_len = input_shape[1]\n\n        # Create the causal mask for the multi-head self-attention\n        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n\n        # Compute the output of the multi-head self-attention\n        attention_output = self.att(inputs, inputs, attention_mask=causal_mask)\n\n        # Apply dropout to the attention output\n        attention_output = self.dropout1(attention_output)\n\n        # Add the attention output to the inputs (residual connection) and normalize the result\n        out1 = self.layernorm1(inputs + attention_output)\n\n        # Compute the output of the feed-forward network\n        ffn_output = self.ffn(out1)\n\n        # Apply dropout to the feed-forward output\n        ffn_output = self.dropout2(ffn_output)\n\n        # Add the feed-forward output to the previous output (residual connection) and normalize the result\n        return self.layernorm2(out1 + ffn_output)","metadata":{"execution":{"iopub.status.busy":"2023-06-26T01:00:01.372056Z","iopub.execute_input":"2023-06-26T01:00:01.373980Z","iopub.status.idle":"2023-06-26T01:00:01.388872Z","shell.execute_reply.started":"2023-06-26T01:00:01.373934Z","shell.execute_reply":"2023-06-26T01:00:01.387913Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"# def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n#     \"\"\"\n#     Mask the upper half of the dot product matrix in self attention.\n#     This prevents flow of information from future tokens to current token.\n#     1's in the lower triangle, counting from the lower right corner.\n#     \"\"\"\n#     i = tf.range(n_dest)[:, None]\n#     j = tf.range(n_src)\n#     m = i >= j - n_src + n_dest\n#     mask = tf.cast(m, dtype)\n#     mask = tf.reshape(mask, [1, n_dest, n_src])\n#     mult = tf.concat(\n#         [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n#     )\n#     return tf.tile(mask, mult)\n\n\n# class TransformerBlock(layers.Layer):\n#     def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n#         super().__init__()\n#         self.att = layers.MultiHeadAttention(num_heads, embed_dim)\n#         self.ffn = keras.Sequential(\n#             [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n#         )\n#         self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n#         self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n#         self.dropout1 = layers.Dropout(rate)\n#         self.dropout2 = layers.Dropout(rate)\n\n#     def call(self, inputs):\n#         input_shape = tf.shape(inputs)\n#         batch_size = input_shape[0]\n#         seq_len = input_shape[1]\n#         causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n#         attention_output = self.att(inputs, inputs, attention_mask=causal_mask)\n#         attention_output = self.dropout1(attention_output)\n#         out1 = self.layernorm1(inputs + attention_output)\n#         ffn_output = self.ffn(out1)\n#         ffn_output = self.dropout2(ffn_output)\n#         return self.layernorm2(out1 + ffn_output)","metadata":{"execution":{"iopub.status.busy":"2023-06-26T01:00:01.390329Z","iopub.execute_input":"2023-06-26T01:00:01.390681Z","iopub.status.idle":"2023-06-26T01:00:01.403086Z","shell.execute_reply.started":"2023-06-26T01:00:01.390649Z","shell.execute_reply":"2023-06-26T01:00:01.401933Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"markdown","source":"## Implement Embedding layer\n\n***Create two separate embedding layers:***\n\n1) One for tokens \n\n2) One for token indices(positions).","metadata":{}},{"cell_type":"code","source":"class TokenAndPositionEmbedding(layers.Layer):\n    \"\"\"\n    Layer for combining token and positional embeddings. Token embeddings provide the model\n    with understanding of the meaning of each token, while positional embeddings provide\n    information about the position of each token in the sequence.\n\n    Attributes:\n        token_emb (layers.Embedding): Token embedding layer.\n        pos_emb (layers.Embedding): Position embedding layer.\n    \"\"\"\n    def __init__(self, maxlen, vocab_size, embed_dim):\n        \"\"\"\n        Initializes the TokenAndPositionEmbedding layer.\n\n        Args:\n            maxlen (int): Maximum length of the sequences for positional encoding.\n            vocab_size (int): Size of the vocabulary for token encoding.\n            embed_dim (int): Dimensionality of the output embeddings.\n        \"\"\"\n        super().__init__()\n        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n\n    def call(self, x):\n        \"\"\"\n        Forward pass of the TokenAndPositionEmbedding layer.\n\n        Args:\n            x (tf.Tensor): Input tensor of shape [batch_size, seq_len].\n\n        Returns:\n            tf.Tensor: Output tensor of shape [batch_size, seq_len, embed_dim], resulting from\n            adding token embeddings and position embeddings.\n        \"\"\"\n        # Compute the maximum sequence length\n        maxlen = tf.shape(x)[-1]\n\n        # Create a range tensor representing positions\n        positions = tf.range(start=0, limit=maxlen, delta=1)\n\n        # Compute the position embeddings\n        positions = self.pos_emb(positions)\n\n        # Compute the token embeddings\n        x = self.token_emb(x)\n\n        # Add the token embeddings and position embeddings\n        return x + positions","metadata":{"execution":{"iopub.status.busy":"2023-06-26T01:00:01.404772Z","iopub.execute_input":"2023-06-26T01:00:01.405104Z","iopub.status.idle":"2023-06-26T01:00:01.416940Z","shell.execute_reply.started":"2023-06-26T01:00:01.405073Z","shell.execute_reply":"2023-06-26T01:00:01.416039Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"# class TokenAndPositionEmbedding(layers.Layer):\n#     def __init__(self, maxlen, vocab_size, embed_dim):\n#         super().__init__()\n#         self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n#         self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n\n#     def call(self, x):\n#         maxlen = tf.shape(x)[-1]\n#         positions = tf.range(start=0, limit=maxlen, delta=1)\n#         positions = self.pos_emb(positions)\n#         x = self.token_emb(x)\n#         return x + positions","metadata":{"execution":{"iopub.status.busy":"2023-06-26T01:00:01.418487Z","iopub.execute_input":"2023-06-26T01:00:01.418816Z","iopub.status.idle":"2023-06-26T01:00:01.429868Z","shell.execute_reply.started":"2023-06-26T01:00:01.418785Z","shell.execute_reply":"2023-06-26T01:00:01.428976Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"markdown","source":"## Implement the Mini GPT\n\n### Hyperparameters","metadata":{}},{"cell_type":"code","source":"vocab_size = 20000  # Only consider the top 20k words\nmaxlen = 80  # Max sequence size\nembed_dim = 256  # Embedding size for each token\nnum_heads = 2  # Number of attention heads\nfeed_forward_dim = 256  # Hidden layer size in feed forward network inside transformer","metadata":{"execution":{"iopub.status.busy":"2023-06-26T01:00:01.431323Z","iopub.execute_input":"2023-06-26T01:00:01.431717Z","iopub.status.idle":"2023-06-26T01:00:01.442560Z","shell.execute_reply.started":"2023-06-26T01:00:01.431686Z","shell.execute_reply":"2023-06-26T01:00:01.441614Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"markdown","source":"### Function to Compile Model","metadata":{}},{"cell_type":"code","source":"def MiniGPT():\n    \"\"\"\n    Constructs a mini version of the GPT model. The architecture is comprised of a\n    token and position embedding layer followed by a single Transformer block. The final\n    layer is a dense layer with softmax activation for prediction. \n\n    Returns:\n        keras.Model: Mini GPT model.\n    \"\"\"\n\n    # Input layer expects inputs of shape (maxlen,) with type int32\n    inputs = layers.Input(shape=(maxlen,), dtype=tf.int32)\n\n    # Create the token and position embedding layer and compute the embeddings\n    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n    x = embedding_layer(inputs)\n\n    # Create the Transformer block and compute its output\n    transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim)\n    x = transformer_block(x)\n\n    # Final dense layer with size equal to the vocabulary size\n    outputs = layers.Dense(vocab_size)(x)\n\n    # Construct the Keras model\n    model = keras.Model(inputs=inputs, outputs=[outputs, x])\n\n    # Loss function for the training \n    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n\n    # Model compilation: use Adam optimizer and the defined loss function\n    # Note that we specify `None` for the second loss to not optimize based on the Transformer block's output\n    model.compile(\"adam\", loss=[loss_fn, None])\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-06-26T01:00:01.443857Z","iopub.execute_input":"2023-06-26T01:00:01.444195Z","iopub.status.idle":"2023-06-26T01:00:01.454201Z","shell.execute_reply.started":"2023-06-26T01:00:01.444165Z","shell.execute_reply":"2023-06-26T01:00:01.453158Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"# # vocab_size = 20000  # Only consider the top 20k words\n# # maxlen = 80  # Max sequence size\n# embed_dim = 256  # Embedding size for each token\n# num_heads = 2  # Number of attention heads\n# feed_forward_dim = 256  # Hidden layer size in feed forward network inside transformer\n\n\n# def MiniGPT():\n#     inputs = layers.Input(shape=(maxlen,), dtype=tf.int32)\n#     embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n#     x = embedding_layer(inputs)\n#     transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim)\n#     x = transformer_block(x)\n#     outputs = layers.Dense(vocab_size)(x)\n#     model = keras.Model(inputs=inputs, outputs=[outputs, x])\n#     loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n#     model.compile(\n#         \"adam\", loss=[loss_fn, None],\n#     )  # No loss and optimization based on word embeddings from transformer block\n#     return model","metadata":{"execution":{"iopub.status.busy":"2023-06-26T01:00:01.455606Z","iopub.execute_input":"2023-06-26T01:00:01.456091Z","iopub.status.idle":"2023-06-26T01:00:01.468292Z","shell.execute_reply.started":"2023-06-26T01:00:01.456059Z","shell.execute_reply":"2023-06-26T01:00:01.467293Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"markdown","source":"## Implement Text Generator Callback Object","metadata":{}},{"cell_type":"code","source":"# class TextGenerator(keras.callbacks.Callback):\n#     \"\"\"A callback to generate text from a trained model.\n#     1. Feed some starting prompt to the model\n#     2. Predict probabilities for the next token\n#     3. Sample the next token and add it to the next input\n\n#     Arguments:\n#         max_tokens: Integer, the number of tokens to be generated after prompt.\n#         start_tokens: List of integers, the token indices for the starting prompt.\n#         index_to_word: List of strings, obtained from the TextVectorization layer.\n#         top_k: Integer, sample from the `top_k` token predictions.\n#         print_every: Integer, print after this many epochs.\n#     \"\"\"\n\n#     def __init__(\n#         self, max_tokens, start_tokens, index_to_word, top_k=10, print_every=10\n#     ):\n#         self.max_tokens = max_tokens\n#         self.start_tokens = start_tokens\n#         self.index_to_word = index_to_word\n#         self.print_every = print_every\n#         self.k = top_k\n\n#     def sample_from(self, logits):\n#         logits, indices = tf.math.top_k(logits, k=self.k, sorted=True)\n#         indices = np.asarray(indices).astype(\"int32\")\n#         preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n#         preds = np.asarray(preds).astype(\"float32\")\n#         return np.random.choice(indices, p=preds)\n\n#     def detokenize(self, number):\n#         return self.index_to_word[number]\n\n#     def on_epoch_end(self, epoch, logs=None):\n#         start_tokens = [_ for _ in self.start_tokens]\n#         if (epoch + 1) % self.print_every != 0:\n#             return\n#         num_tokens_generated = 0\n#         tokens_generated = []\n#         while num_tokens_generated <= self.max_tokens:\n#             pad_len = maxlen - len(start_tokens)\n#             sample_index = len(start_tokens) - 1\n#             if pad_len < 0:\n#                 x = start_tokens[:maxlen]\n#                 sample_index = maxlen - 1\n#             elif pad_len > 0:\n#                 x = start_tokens + [0] * pad_len\n#             else:\n#                 x = start_tokens\n#             x = np.array([x])\n#             y, _ = self.model.predict(x)\n#             sample_token = self.sample_from(y[0][sample_index])\n#             tokens_generated.append(sample_token)\n#             start_tokens.append(sample_token)\n#             num_tokens_generated = len(tokens_generated)\n#         txt = \" \".join(\n#             [self.detokenize(_) for _ in self.start_tokens + tokens_generated]\n#         )\n#         print(f\"generated text:\\n{txt}\\n\")\n        \n        \n\n# # Tokenize starting prompt\n# word_to_index = {}\n# for index, word in enumerate(vocab):\n#     word_to_index[word] = index\n","metadata":{"execution":{"iopub.status.busy":"2023-06-26T01:00:01.470848Z","iopub.execute_input":"2023-06-26T01:00:01.472662Z","iopub.status.idle":"2023-06-26T01:00:01.481494Z","shell.execute_reply.started":"2023-06-26T01:00:01.472630Z","shell.execute_reply":"2023-06-26T01:00:01.480449Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"class TextGenerator(keras.callbacks.Callback):\n    \"\"\"\n    A callback to generate text from a trained model at the end of each epoch. It uses the model's \n    predictions to sample a token, add it to the input, and generate subsequent tokens.\n\n    Attributes:\n        max_tokens (int): The number of tokens to be generated after the prompt.\n        start_tokens (list): The token indices for the starting prompt.\n        index_to_word (list): Mapping from token indices to words, obtained from the TextVectorization layer.\n        k (int): Number of token predictions to consider for sampling the next token.\n        print_every (int): Frequency of print for the generated text (in number of epochs).\n    \"\"\"\n    def __init__(self, max_tokens, start_tokens, index_to_word, top_k=10, print_every=1):\n        \"\"\"\n        Initializes the TextGenerator callback.\n\n        Args:\n            max_tokens (int): Maximum number of tokens to be generated.\n            start_tokens (list): List of integers representing the starting tokens.\n            index_to_word (list): List of strings representing the mapping from indices to words.\n            top_k (int, optional): Number of top token predictions to sample from. Defaults to 10.\n            print_every (int, optional): Frequency of print (in number of epochs). Defaults to 1.\n        \"\"\"\n        super().__init__()\n        self.max_tokens = max_tokens\n        self.start_tokens = start_tokens\n        self.index_to_word = index_to_word\n        self.k = top_k\n        self.print_every = print_every\n        self.generated_texts = [] # for qualitative validation set\n\n    def sample_from(self, logits):\n        \"\"\"\n        Sample a token index from the token predictions based on their probabilities.\n\n        Args:\n            logits (tf.Tensor): The token predictions (logits) of the model.\n\n        Returns:\n            int: The sampled token index.\n        \"\"\"\n        # Select top-k logits and their indices\n        logits, indices = tf.math.top_k(logits, k=self.k, sorted=True)\n        indices = np.asarray(indices).astype(\"int32\")\n\n        # Apply softmax to transform logits into probabilities\n        preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n        preds = np.asarray(preds).astype(\"float32\")\n\n        # Randomly select an index according to the probability distribution\n        return np.random.choice(indices, p=preds)\n\n    def detokenize(self, number):\n        \"\"\"\n        Convert a token index into the corresponding word.\n\n        Args:\n            number (int): The token index.\n\n        Returns:\n            str: The corresponding word.\n        \"\"\"\n        return self.index_to_word[number]\n\n    def on_epoch_end(self, epoch, logs=None):\n        \"\"\"\n        At the end of each epoch, generate text and print it.\n\n        Args:\n            epoch (int): The current epoch number.\n            logs (dict, optional): Dictionary of metrics from the epoch. Defaults to None.\n        \"\"\"\n        # Create a copy of start tokens for generation\n        start_tokens = [_ for _ in self.start_tokens]\n\n        # Only generate text at specified frequency\n        if (epoch + 1) % self.print_every != 0:\n            return\n\n        num_tokens_generated = 0\n        tokens_generated = []\n\n        # Generate tokens until max tokens reached\n        while num_tokens_generated <= self.max_tokens:\n            pad_len = maxlen - len(start_tokens)\n            sample_index = len(start_tokens) - 1\n\n            # Adjust padding based on length of start tokens\n            if pad_len < 0:\n                x = start_tokens[:maxlen]\n                sample_index = maxlen - 1\n            elif pad_len > 0:\n                x = start_tokens + [0] * pad_len\n            else:\n                x = start_tokens\n\n            x = np.array([x])\n\n            # Use the model to predict the probabilities for the next token\n            y, _ = self.model.predict(x)\n\n            # Sample a token from the model's output distribution\n            sample_token = self.sample_from(y[0][sample_index])\n\n            # Append the token to the list of generated tokens\n            tokens_generated.append(sample_token)\n\n            # Add the token to the start tokens for the next generation\n            start_tokens.append(sample_token)\n\n            # Increase the number of tokens generated by 1\n            num_tokens_generated = len(tokens_generated)\n\n        # Convert the tokens into actual words and join them into a string\n        txt = \" \".join(\n            [self.detokenize(_) for _ in self.start_tokens + tokens_generated]\n        )\n        \n        self.generated_texts.append((epoch, txt)) # Store for evalutation after training\n\n\n        # Print the generated text\n        print(f\"generated text:\\n{txt}\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-06-26T01:21:58.428691Z","iopub.execute_input":"2023-06-26T01:21:58.429085Z","iopub.status.idle":"2023-06-26T01:21:58.446109Z","shell.execute_reply.started":"2023-06-26T01:21:58.429055Z","shell.execute_reply":"2023-06-26T01:21:58.445089Z"},"trusted":true},"execution_count":109,"outputs":[]},{"cell_type":"code","source":"# class TextGenerator(keras.callbacks.Callback):\n#     \"\"\"A callback to generate text from a trained model.\n#     1. Feed some starting prompt to the model\n#     2. Predict probabilities for the next token\n#     3. Sample the next token and add it to the next input\n\n#     Arguments:\n#         max_tokens: Integer, the number of tokens to be generated after prompt.\n#         start_tokens: List of integers, the token indices for the starting prompt.\n#         index_to_word: List of strings, obtained from the TextVectorization layer.\n#         top_k: Integer, sample from the `top_k` token predictions.\n#         print_every: Integer, print after this many epochs.\n#     \"\"\"\n\n#     def __init__(\n#         self, max_tokens, start_tokens, index_to_word, top_k=10, print_every=1\n#     ):\n#         self.max_tokens = max_tokens\n#         self.start_tokens = start_tokens\n#         self.index_to_word = index_to_word\n#         self.print_every = print_every\n#         self.k = top_k\n        \n        \n#     def sample_from(self, logits):\n#         logits, indices = tf.math.top_k(logits, k=self.k, sorted=True)\n#         indices = np.asarray(indices).astype(\"int32\")\n#         preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n#         preds = np.asarray(preds).astype(\"float32\")\n#         return np.random.choice(indices, p=preds)\n\n    \n#     def detokenize(self, number):\n#         return self.index_to_word[number]\n\n    \n#     def on_epoch_end(self, epoch, logs=None):\n#         start_tokens = [_ for _ in self.start_tokens]\n#         if (epoch + 1) % self.print_every != 0:\n#             return\n#         num_tokens_generated = 0\n#         tokens_generated = []\n#         while num_tokens_generated <= self.max_tokens:\n#             pad_len = maxlen - len(start_tokens)\n#             sample_index = len(start_tokens) - 1\n#             if pad_len < 0:\n#                 x = start_tokens[:maxlen]\n#                 sample_index = maxlen - 1\n#             elif pad_len > 0:\n#                 x = start_tokens + [0] * pad_len\n#             else:\n#                 x = start_tokens\n#             x = np.array([x])\n#             y, _ = self.model.predict(x)\n#             sample_token = self.sample_from(y[0][sample_index])\n#             tokens_generated.append(sample_token)\n#             start_tokens.append(sample_token)\n#             num_tokens_generated = len(tokens_generated)\n#         txt = \" \".join(\n#             [self.detokenize(_) for _ in self.start_tokens + tokens_generated]\n#         )\n#         print(f\"generated text:\\n{txt}\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-06-26T01:21:59.082371Z","iopub.execute_input":"2023-06-26T01:21:59.083064Z","iopub.status.idle":"2023-06-26T01:21:59.089157Z","shell.execute_reply.started":"2023-06-26T01:21:59.083030Z","shell.execute_reply":"2023-06-26T01:21:59.088273Z"},"trusted":true},"execution_count":110,"outputs":[]},{"cell_type":"markdown","source":"## Create Word/Index Mapping Dictionary ","metadata":{}},{"cell_type":"code","source":"## Tokenize starting prompt\nword_to_index = {}\nfor index, word in enumerate(vocab):\n    word_to_index[word] = index","metadata":{"execution":{"iopub.status.busy":"2023-06-26T01:22:00.137526Z","iopub.execute_input":"2023-06-26T01:22:00.137883Z","iopub.status.idle":"2023-06-26T01:22:00.152235Z","shell.execute_reply.started":"2023-06-26T01:22:00.137855Z","shell.execute_reply":"2023-06-26T01:22:00.151195Z"},"trusted":true},"execution_count":111,"outputs":[]},{"cell_type":"markdown","source":"## Initialize Callback Object\n\n***We also need to supply a starting prompt to act as a qualitative validation set to evaluate the models performance from a 'does it make more sense' per epoch. It will generate(predict) a text sequence continuation from the starting prompt at the end of every epoch to inspect.***","metadata":{}},{"cell_type":"code","source":"start_prompt = \"I would have\"\n\nstart_tokens = [word_to_index.get(_, 1) for _ in start_prompt.split()]\nnum_tokens_generated = 42\ntext_gen_callback = TextGenerator(num_tokens_generated, start_tokens, vocab)","metadata":{"execution":{"iopub.status.busy":"2023-06-26T01:22:01.593609Z","iopub.execute_input":"2023-06-26T01:22:01.593967Z","iopub.status.idle":"2023-06-26T01:22:01.599264Z","shell.execute_reply.started":"2023-06-26T01:22:01.593940Z","shell.execute_reply":"2023-06-26T01:22:01.598133Z"},"trusted":true},"execution_count":112,"outputs":[]},{"cell_type":"markdown","source":"## Training\n\n***I apologize for the scrolling your about to do. I wanted to generate text at each epoch so that along with loss there would be some qualitative evaluation on the models performance throughout training but I could not find a way to remove the progress bars for each step inside the epochs... If anyone reading this knows a way please comment.***\n\n***Until about `25` epochs many of the generations depending on the satrting prompt during training had nonsensical outputs. So we will use `25` to get a good baseline model to evaluate.***","metadata":{}},{"cell_type":"code","source":"model = MiniGPT()\n\nN_EPOCHS = 25\nhistory  = model.fit(text_ds, verbose=0, epochs=N_EPOCHS, callbacks=[text_gen_callback])","metadata":{"execution":{"iopub.status.busy":"2023-06-26T01:23:53.097927Z","iopub.execute_input":"2023-06-26T01:23:53.098356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluate Training Loss Per Epoch","metadata":{}},{"cell_type":"code","source":"# Plot training loss\nplt.figure(figsize=(12, 6))\nplt.plot(history.history['loss'])\nplt.title('Training loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train'], loc='upper right')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluate Validation Set Per Epoch\n\n***Since we don't have a quantitative validation set in this situation we can use a qualitative validation set. These would be the generated text from the end of each epoch. This can give us some clues along with the losses per epoch to see how the models performance progressed through training.***\n\n***Lets inspect what the first five generations look llike compared to the last five during training.***","metadata":{}},{"cell_type":"code","source":"# for epoch, text in text_gen_callback.generated_texts:\n#     print(f\"Epoch: {epoch+1}\\nGenerated Text:\\n{text}\\n\")\n    # Create a DataFrame\n    \ndf_val = pd.DataFrame(text_gen_callback.generated_texts, columns=['Epoch', 'Generated Text'])\n\ndisplay(df_val.head(5));df_val.tail(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***It appears to that with more iterations the model becomes more and more realistic in its generations.***","metadata":{}},{"cell_type":"markdown","source":"## Inference\n\n***Now that we have trained the model to generate toxic comments from a starting prompt we can begin to generate our synthetic data.***","metadata":{}},{"cell_type":"code","source":"def generate_text(starting_prompt=''):\n    new_start_prompt = \"start something\"\n    new_start_tokens = [word_to_index.get(word, 1) for word in new_start_prompt.split()]\n\n    text_gen_callback.start_tokens = new_start_tokens\n    text_gen_callback.on_epoch_end(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generate_text(\"you sure\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}