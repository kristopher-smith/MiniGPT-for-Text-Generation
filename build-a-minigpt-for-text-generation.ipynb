{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Text generation with a miniature GPT","metadata":{}},{"cell_type":"markdown","source":"## Introduction\n\nThis example demonstrates how to implement an autoregressive language model using a miniature version of the GPT model. The model consists of a single Transformer block with causal masking in its attention layer. We use the text from the IMDB sentiment classification dataset for training and generate new movie reviews for a given prompt. When using this script with your own dataset, make sure it has at least 1 million words.","metadata":{}},{"cell_type":"markdown","source":"## Setup","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf, numpy as np, pandas as pd\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import TextVectorization\nimport os\nimport string\nimport random","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-25T05:36:27.796006Z","iopub.execute_input":"2023-06-25T05:36:27.796483Z","iopub.status.idle":"2023-06-25T05:36:35.648226Z","shell.execute_reply.started":"2023-06-25T05:36:27.796427Z","shell.execute_reply":"2023-06-25T05:36:35.647210Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Implement a Transformer block as a layer","metadata":{}},{"cell_type":"code","source":"def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n    \"\"\"\n    Mask the upper half of the dot product matrix in self attention.\n    This prevents flow of information from future tokens to current token.\n    1's in the lower triangle, counting from the lower right corner.\n    \"\"\"\n    i = tf.range(n_dest)[:, None]\n    j = tf.range(n_src)\n    m = i >= j - n_src + n_dest\n    mask = tf.cast(m, dtype)\n    mask = tf.reshape(mask, [1, n_dest, n_src])\n    mult = tf.concat(\n        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n    )\n    return tf.tile(mask, mult)\n\n\nclass TransformerBlock(layers.Layer):\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n        super().__init__()\n        self.att = layers.MultiHeadAttention(num_heads, embed_dim)\n        self.ffn = keras.Sequential(\n            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n        )\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(rate)\n        self.dropout2 = layers.Dropout(rate)\n\n    def call(self, inputs):\n        input_shape = tf.shape(inputs)\n        batch_size = input_shape[0]\n        seq_len = input_shape[1]\n        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n        attention_output = self.att(inputs, inputs, attention_mask=causal_mask)\n        attention_output = self.dropout1(attention_output)\n        out1 = self.layernorm1(inputs + attention_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output)\n        return self.layernorm2(out1 + ffn_output)","metadata":{"execution":{"iopub.status.busy":"2023-06-25T05:37:27.446004Z","iopub.execute_input":"2023-06-25T05:37:27.447145Z","iopub.status.idle":"2023-06-25T05:37:27.471134Z","shell.execute_reply.started":"2023-06-25T05:37:27.447097Z","shell.execute_reply":"2023-06-25T05:37:27.470117Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Implement an embedding layer\n\nCreate two separate embedding layers: one for tokens and one for token index (positions).","metadata":{}},{"cell_type":"code","source":"class TokenAndPositionEmbedding(layers.Layer):\n    def __init__(self, maxlen, vocab_size, embed_dim):\n        super().__init__()\n        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n\n    def call(self, x):\n        maxlen = tf.shape(x)[-1]\n        positions = tf.range(start=0, limit=maxlen, delta=1)\n        positions = self.pos_emb(positions)\n        x = self.token_emb(x)\n        return x + positions","metadata":{"execution":{"iopub.status.busy":"2023-06-25T05:37:30.068656Z","iopub.execute_input":"2023-06-25T05:37:30.069026Z","iopub.status.idle":"2023-06-25T05:37:30.076730Z","shell.execute_reply.started":"2023-06-25T05:37:30.068982Z","shell.execute_reply":"2023-06-25T05:37:30.075389Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Implement the miniature GPT model","metadata":{}},{"cell_type":"code","source":"vocab_size = 20000  # Only consider the top 20k words\nmaxlen = 80  # Max sequence size\nembed_dim = 256  # Embedding size for each token\nnum_heads = 2  # Number of attention heads\nfeed_forward_dim = 256  # Hidden layer size in feed forward network inside transformer\n\n\ndef create_model():\n    inputs = layers.Input(shape=(maxlen,), dtype=tf.int32)\n    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n    x = embedding_layer(inputs)\n    transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim)\n    x = transformer_block(x)\n    outputs = layers.Dense(vocab_size)(x)\n    model = keras.Model(inputs=inputs, outputs=[outputs, x])\n    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n    model.compile(\n        \"adam\", loss=[loss_fn, None],\n    )  # No loss and optimization based on word embeddings from transformer block\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-06-25T05:37:58.672406Z","iopub.execute_input":"2023-06-25T05:37:58.672938Z","iopub.status.idle":"2023-06-25T05:37:58.680932Z","shell.execute_reply.started":"2023-06-25T05:37:58.672906Z","shell.execute_reply":"2023-06-25T05:37:58.679858Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Prepare the data for word-level language modelling\n\nDownload the IMDB dataset and combine training and validation sets for a text generation task.","metadata":{}},{"cell_type":"code","source":"!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n!tar -xf aclImdb_v1.tar.gz","metadata":{"execution":{"iopub.status.busy":"2023-06-25T05:38:06.878767Z","iopub.execute_input":"2023-06-25T05:38:06.879158Z","iopub.status.idle":"2023-06-25T05:38:22.550239Z","shell.execute_reply.started":"2023-06-25T05:38:06.879126Z","shell.execute_reply":"2023-06-25T05:38:22.548836Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 80.2M  100 80.2M    0     0  10.4M      0  0:00:07  0:00:07 --:--:-- 17.7M\n","output_type":"stream"}]},{"cell_type":"code","source":"batch_size = 128\n\n# The dataset contains each review in a separate text file\n# The text files are present in four different folders\n# Create a list all files\nfilenames = []\ndirectories = [\n    \"aclImdb/train/pos\",\n    \"aclImdb/train/neg\",\n    \"aclImdb/test/pos\",\n    \"aclImdb/test/neg\",\n]\nfor dir in directories:\n    for f in os.listdir(dir):\n        filenames.append(os.path.join(dir, f))\n\nprint(f\"{len(filenames)} files\")\n\n# Create a dataset from text files\nrandom.shuffle(filenames)\ntext_ds = tf.data.TextLineDataset(filenames)\ntext_ds = text_ds.shuffle(buffer_size=256)\ntext_ds = text_ds.batch(batch_size)\n\n\ndef custom_standardization(input_string):\n    \"\"\" Remove html line-break tags and handle punctuation \"\"\"\n    lowercased = tf.strings.lower(input_string)\n    stripped_html = tf.strings.regex_replace(lowercased, \"<br />\", \" \")\n    return tf.strings.regex_replace(stripped_html, f\"([{string.punctuation}])\", r\" \\1\")\n\n\n# Create a vectorization layer and adapt it to the text\nvectorize_layer = TextVectorization(\n    standardize=custom_standardization,\n    max_tokens=vocab_size - 1,\n    output_mode=\"int\",\n    output_sequence_length=maxlen + 1,\n)\nvectorize_layer.adapt(text_ds)\nvocab = vectorize_layer.get_vocabulary()  # To get words back from token indices\n\n\ndef prepare_lm_inputs_labels(text):\n    \"\"\"\n    Shift word sequences by 1 position so that the target for position (i) is\n    word at position (i+1). The model will use all words up till position (i)\n    to predict the next word.\n    \"\"\"\n    text = tf.expand_dims(text, -1)\n    tokenized_sentences = vectorize_layer(text)\n    x = tokenized_sentences[:, :-1]\n    y = tokenized_sentences[:, 1:]\n    return x, y\n\n\ntext_ds = text_ds.map(prepare_lm_inputs_labels)\ntext_ds = text_ds.prefetch(tf.data.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2023-06-25T05:39:33.944050Z","iopub.execute_input":"2023-06-25T05:39:33.944446Z","iopub.status.idle":"2023-06-25T05:39:44.844726Z","shell.execute_reply.started":"2023-06-25T05:39:33.944415Z","shell.execute_reply":"2023-06-25T05:39:44.843642Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"50000 files\n","output_type":"stream"}]},{"cell_type":"code","source":"# data = pd.read_csv('../input/jigsaw-toxic-severity-rating/comments_to_score.csv')\n# text_column = data['text']\n\ndata = pd.read_csv('../input/jigsaw-toxic-severity-rating/validation_data.csv')\ntext_column = data['more_toxic']\n\nprint(len(text_column))\n# Create a dataset from the pandas column\ntext_ds = tf.data.Dataset.from_tensor_slices(text_column)\n\n# Shuffle and batch the dataset\ntext_ds = text_ds.shuffle(buffer_size=256)\ntext_ds = text_ds.batch(batch_size)\ntext_ds = text_ds.map(prepare_lm_inputs_labels)\ntext_ds = text_ds.prefetch(tf.data.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2023-06-25T06:08:42.792345Z","iopub.execute_input":"2023-06-25T06:08:42.792725Z","iopub.status.idle":"2023-06-25T06:08:43.333993Z","shell.execute_reply.started":"2023-06-25T06:08:42.792695Z","shell.execute_reply":"2023-06-25T06:08:43.333074Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"30108\n","output_type":"stream"}]},{"cell_type":"code","source":"# Preview a sample from the text_ds dataset\nsample = text_ds.take(5)  # Take one sample from the dataset\n\nfor x, y in sample:\n    # Convert token indices back to words\n    input_words  = [vocab[i] for i in x[0].numpy()]\n    target_words = [vocab[i] for i in y[0].numpy()]\n\n    # Print the input and target sequences\n    print(\"Input Sequence:\")\n    print(\" \".join(input_words))\n    print(\"\\nTarget Sequence:\")\n    print(\" \".join(target_words))","metadata":{"execution":{"iopub.status.busy":"2023-06-25T06:08:55.097743Z","iopub.execute_input":"2023-06-25T06:08:55.098340Z","iopub.status.idle":"2023-06-25T06:08:55.215909Z","shell.execute_reply.started":"2023-06-25T06:08:55.098298Z","shell.execute_reply":"2023-06-25T06:08:55.214995Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"Input Sequence:\nyou are an arrogant little so -and -so - aren 't you mr pompous ?                                                                 \n\nTarget Sequence:\nare an arrogant little so -and -so - aren 't you mr pompous ?                                                                  \nInput Sequence:\n[UNK] [UNK] [UNK] changing the title to [UNK] [UNK] [UNK] = ) [UNK] white [UNK] [UNK] [UNK] [UNK]                                                              \n\nTarget Sequence:\n[UNK] [UNK] changing the title to [UNK] [UNK] [UNK] = ) [UNK] white [UNK] [UNK] [UNK] [UNK]                                                               \nInput Sequence:\nleave 4 a [UNK] [UNK] and i [UNK] with a [UNK] [UNK] you are [UNK] [UNK] you suck you suck you suck you suck you suck you suck you suck you suck you suck you suck you suck you suck you suck you suck you suck you suck you suck you suck you suck you suck you suck you suck you suck you suck you suck you suck you suck you suck you suck you suck you suck you suck\n\nTarget Sequence:\n4 a [UNK] [UNK] and i [UNK] with a [UNK] [UNK] you are [UNK] [UNK] you suck you suck you suck you suck you suck you suck you suck you suck you suck you suck you suck you suck you suck you suck you suck you suck you suck you suck you suck you suck you suck you suck you suck you suck you suck you suck you suck you suck you suck you suck you suck you suck you\nInput Sequence:\nmore attempts to [UNK] [UNK] pages . some random [UNK] has put up silver [UNK] [UNK] society of london ) for [UNK] .                                                         \n\nTarget Sequence:\nattempts to [UNK] [UNK] pages . some random [UNK] has put up silver [UNK] [UNK] society of london ) for [UNK] .                                                          \nInput Sequence:\ni knew you were a dick but a [UNK] too ? nice job on the morgan discussion i liked how you buried the comments in the archive section . [UNK] archive stuff that is 5 days old nice job                                         \n\nTarget Sequence:\nknew you were a dick but a [UNK] too ? nice job on the morgan discussion i liked how you buried the comments in the archive section . [UNK] archive stuff that is 5 days old nice job                                          \n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Implement a Keras callback for generating text","metadata":{}},{"cell_type":"code","source":"class TextGenerator(keras.callbacks.Callback):\n    \"\"\"A callback to generate text from a trained model.\n    1. Feed some starting prompt to the model\n    2. Predict probabilities for the next token\n    3. Sample the next token and add it to the next input\n\n    Arguments:\n        max_tokens: Integer, the number of tokens to be generated after prompt.\n        start_tokens: List of integers, the token indices for the starting prompt.\n        index_to_word: List of strings, obtained from the TextVectorization layer.\n        top_k: Integer, sample from the `top_k` token predictions.\n        print_every: Integer, print after this many epochs.\n    \"\"\"\n\n    def __init__(\n        self, max_tokens, start_tokens, index_to_word, top_k=10, print_every=1\n    ):\n        self.max_tokens = max_tokens\n        self.start_tokens = start_tokens\n        self.index_to_word = index_to_word\n        self.print_every = print_every\n        self.k = top_k\n\n    def sample_from(self, logits):\n        logits, indices = tf.math.top_k(logits, k=self.k, sorted=True)\n        indices = np.asarray(indices).astype(\"int32\")\n        preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n        preds = np.asarray(preds).astype(\"float32\")\n        return np.random.choice(indices, p=preds)\n\n    def detokenize(self, number):\n        return self.index_to_word[number]\n\n    def on_epoch_end(self, epoch, logs=None):\n        start_tokens = [_ for _ in self.start_tokens]\n        if (epoch + 1) % self.print_every != 0:\n            return\n        num_tokens_generated = 0\n        tokens_generated = []\n        while num_tokens_generated <= self.max_tokens:\n            pad_len = maxlen - len(start_tokens)\n            sample_index = len(start_tokens) - 1\n            if pad_len < 0:\n                x = start_tokens[:maxlen]\n                sample_index = maxlen - 1\n            elif pad_len > 0:\n                x = start_tokens + [0] * pad_len\n            else:\n                x = start_tokens\n            x = np.array([x])\n            y, _ = self.model.predict(x)\n            sample_token = self.sample_from(y[0][sample_index])\n            tokens_generated.append(sample_token)\n            start_tokens.append(sample_token)\n            num_tokens_generated = len(tokens_generated)\n        txt = \" \".join(\n            [self.detokenize(_) for _ in self.start_tokens + tokens_generated]\n        )\n        print(f\"generated text:\\n{txt}\\n\")\n        \n        \n\n# Tokenize starting prompt\nword_to_index = {}\nfor index, word in enumerate(vocab):\n    word_to_index[word] = index\n","metadata":{"execution":{"iopub.status.busy":"2023-06-25T06:09:01.357076Z","iopub.execute_input":"2023-06-25T06:09:01.357432Z","iopub.status.idle":"2023-06-25T06:09:01.377392Z","shell.execute_reply.started":"2023-06-25T06:09:01.357402Z","shell.execute_reply":"2023-06-25T06:09:01.376315Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"start_prompt = \"this movie is\"\nstart_prompt = \"are you\"\n\nstart_tokens = [word_to_index.get(_, 1) for _ in start_prompt.split()]\nnum_tokens_generated = 40\ntext_gen_callback = TextGenerator(num_tokens_generated, start_tokens, vocab)","metadata":{"execution":{"iopub.status.busy":"2023-06-25T06:09:05.449771Z","iopub.execute_input":"2023-06-25T06:09:05.450169Z","iopub.status.idle":"2023-06-25T06:09:05.455873Z","shell.execute_reply.started":"2023-06-25T06:09:05.450139Z","shell.execute_reply":"2023-06-25T06:09:05.454718Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"## Train the model","metadata":{}},{"cell_type":"code","source":"model = create_model()\n\nN_EPOCHS = 25\n\nmodel.fit(text_ds, verbose=2, epochs=N_EPOCHS, callbacks=[text_gen_callback])","metadata":{"execution":{"iopub.status.busy":"2023-06-25T06:09:07.404078Z","iopub.execute_input":"2023-06-25T06:09:07.404902Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/25\n1/1 [==============================] - 0s 224ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 22ms/step\ngenerated text:\nare you 're not the [UNK] , and you can suck [UNK] you 're a [UNK] [UNK] ? you [UNK] me to [UNK] ?                   \n\n236/236 - 48s - loss: 2.8834 - dense_11_loss: 2.8834 - 48s/epoch - 204ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"text_gen_callback.on_epoch_end(N_EPOCHS)","metadata":{"execution":{"iopub.status.busy":"2023-06-25T05:54:18.707181Z","iopub.status.idle":"2023-06-25T05:54:18.707832Z","shell.execute_reply.started":"2023-06-25T05:54:18.707577Z","shell.execute_reply":"2023-06-25T05:54:18.707606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}