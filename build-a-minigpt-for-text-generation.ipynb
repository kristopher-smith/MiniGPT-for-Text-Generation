{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# MiniGPT For Generating Synthetic Text Data\n\nby Kris Smith","metadata":{}},{"cell_type":"markdown","source":"# ***WARNING*** \n\n## The data required to train the model for this task is known to be vulgar, offensive, toxic, racist, and otherwise not pleasant.","metadata":{}},{"cell_type":"markdown","source":"## Problem Statement\n\nToxic comments online come in many forms and in many arenas. There are currently several ways to mitigate these comments(for those organizations who wish to do so). Some of these ways include human moderators, and training machine learning models to detect toxicity in online comments.\n\nThe issue with human moderators is that some of these platforms have grown so large so quickly that there are not nearly enough moderators to achieve any sense of control for most of these comments. The shear volume of toxicity and bots online makes it unrealistic to think we could do this job with humans at this point.\n\nMany companies are employing machine learning to assist with identifying toxic comments online automatically. The problem with this approach is the lack of labeled training data to train the models on.\n\nThis is the problem I am going to solve using generative deep learning techniques. ","metadata":{}},{"cell_type":"markdown","source":"## References\n\n* [Improving Language Understanding by Generative Pre-Training](https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035)\n\n* [Language Models are Unsupervised Multitask Learners](https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe)\n\n* [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)\n\n* Many of the ideas and code were adapted from this Keras resource: https://keras.io/examples/generative/text_generation_with_miniature_gpt/","metadata":{}},{"cell_type":"markdown","source":"## Import Libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport string\nimport random\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\n\nfrom nltk import ngrams\nfrom collections import Counter\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import TextVectorization\n\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-25T19:26:40.804755Z","iopub.execute_input":"2023-06-25T19:26:40.805127Z","iopub.status.idle":"2023-06-25T19:26:40.815607Z","shell.execute_reply.started":"2023-06-25T19:26:40.805099Z","shell.execute_reply":"2023-06-25T19:26:40.814523Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n","output_type":"stream"},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"markdown","source":"## Data\n\nThe data I will be using to train the generative model was released on Kaggle as part of an ongoing series of competitions sponsored by the [Google company Jigsaw](https://en.wikipedia.org/wiki/Jigsaw_(company)).\n\nThe data consists of online comments with various severity levels of toxicity. There are versions of these comments labeled by human annotators wherein they label each comment as toxic or not, or other sets where they were labeled as different categories of toxic such as hatespeech, racist/sexist, obscene, etc. Although these are the labeled datasets we would be adding the synthetic data to in order to create more training data, for this task of simply generating similar text data we will only focus on the comments themselves.\n\nThe data provided by this competition includes a total of `14,251` unique toxic comments. Theses are the comments I will use to train the generative model with.","metadata":{}},{"cell_type":"markdown","source":"## EDA\n\nThe data came in two different files.\n\n1) Comments to score: This acts as a test dataset of comments for scoring after the model was trained.\n\n2) Validation data: This was the training data for the competition wherein there are two columns. One column labeled less toxic was a comment which human annotators labeled as less toxic than its more toxic counterpart in the other column. There was no actual training data where a comment was paired with its severity rating. The models were trained using creative techniques with the validation data and other classification data sets to train a model which predicted severity of comments.\n\nSince for our purposes we are only interested in the actual text comments themselves, I will only be using those columns from these datasources.\n\nI start by reading them all into pandas dataframes, isolating the text columns from each one, and stacking them all together so we have a single column of text when it is all said and done.\n","metadata":{}},{"cell_type":"code","source":"data1 = pd.read_csv('../input/jigsaw-toxic-severity-rating/comments_to_score.csv')\ndata1.info()\n\n## Isolate only text column\ndata1 = data1['text']\n\ndata1.sample(6)","metadata":{"execution":{"iopub.status.busy":"2023-06-25T19:26:40.824515Z","iopub.execute_input":"2023-06-25T19:26:40.826957Z","iopub.status.idle":"2023-06-25T19:26:40.888692Z","shell.execute_reply.started":"2023-06-25T19:26:40.826930Z","shell.execute_reply":"2023-06-25T19:26:40.887650Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 7537 entries, 0 to 7536\nData columns (total 2 columns):\n #   Column      Non-Null Count  Dtype \n---  ------      --------------  ----- \n 0   comment_id  7537 non-null   int64 \n 1   text        7537 non-null   object\ndtypes: int64(1), object(1)\nmemory usage: 117.9+ KB\n","output_type":"stream"},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"3265    \"HEY VACUOS, EMPTY- MINDED COLA! GET A LIFE! T...\n7293    Because it is an R&B; album, and Wikipedia's c...\n921     Dakota is a baby and so is angel they are not ...\n6319     I am coming to get you \\n\\ni know where you l...\n5356    Justafax, you are an angry individual and you ...\n827     A big thank you\\n\\nYou have just blocked me in...\nName: text, dtype: object"},"metadata":{}}]},{"cell_type":"markdown","source":"We can see the comments to score was the test file which contained only comments and their corresponding id's","metadata":{}},{"cell_type":"code","source":"data2 = pd.read_csv('../input/jigsaw-toxic-severity-rating/validation_data.csv')\ndata2.info()\n\ndata2.sample(6)","metadata":{"execution":{"iopub.status.busy":"2023-06-25T19:26:40.890892Z","iopub.execute_input":"2023-06-25T19:26:40.891726Z","iopub.status.idle":"2023-06-25T19:26:41.179909Z","shell.execute_reply.started":"2023-06-25T19:26:40.891692Z","shell.execute_reply":"2023-06-25T19:26:41.178905Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 30108 entries, 0 to 30107\nData columns (total 3 columns):\n #   Column      Non-Null Count  Dtype \n---  ------      --------------  ----- \n 0   worker      30108 non-null  int64 \n 1   less_toxic  30108 non-null  object\n 2   more_toxic  30108 non-null  object\ndtypes: int64(1), object(2)\nmemory usage: 705.8+ KB\n","output_type":"stream"},{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"       worker                                         less_toxic  \\\n19819     209  Stop deleting my edits! They are useful so I w...   \n5933      244  \"\\n\\nWell done yourself, Shot. Nothing like a ...   \n16050     382    Beware websites that HAVE NO AUTHOR NAME. Wh...   \n28702     604  Jenny Craig==\\n\\nYeah, homie. I don't know if ...   \n27888     654   bad motherfuckers \\n\\nThere are a lot of them...   \n28263     465   Stupid \\n\\nyou are not a macedonian, you have...   \n\n                                              more_toxic  \n19819         Sign your fucking comments! 80.192.32.85    \n5933   I have done nothing wrong, so I dont care. I a...  \n16050  You're ridiculous if you jump to conclusions l...  \n28702        I'll bet 80% of what she did was rubbish...  \n27888  I agree with Carribean about the unsourced and...  \n28263   Gwen Gale's Pimp \\n\\nI know you are her pimp,...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>worker</th>\n      <th>less_toxic</th>\n      <th>more_toxic</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>19819</th>\n      <td>209</td>\n      <td>Stop deleting my edits! They are useful so I w...</td>\n      <td>Sign your fucking comments! 80.192.32.85</td>\n    </tr>\n    <tr>\n      <th>5933</th>\n      <td>244</td>\n      <td>\"\\n\\nWell done yourself, Shot. Nothing like a ...</td>\n      <td>I have done nothing wrong, so I dont care. I a...</td>\n    </tr>\n    <tr>\n      <th>16050</th>\n      <td>382</td>\n      <td>Beware websites that HAVE NO AUTHOR NAME. Wh...</td>\n      <td>You're ridiculous if you jump to conclusions l...</td>\n    </tr>\n    <tr>\n      <th>28702</th>\n      <td>604</td>\n      <td>Jenny Craig==\\n\\nYeah, homie. I don't know if ...</td>\n      <td>I'll bet 80% of what she did was rubbish...</td>\n    </tr>\n    <tr>\n      <th>27888</th>\n      <td>654</td>\n      <td>bad motherfuckers \\n\\nThere are a lot of them...</td>\n      <td>I agree with Carribean about the unsourced and...</td>\n    </tr>\n    <tr>\n      <th>28263</th>\n      <td>465</td>\n      <td>Stupid \\n\\nyou are not a macedonian, you have...</td>\n      <td>Gwen Gale's Pimp \\n\\nI know you are her pimp,...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"This was the data provided to validate the models performance during training. The three columns are workers(annotators) and the other two are text columns which we will use both to train our generative model with.","metadata":{}},{"cell_type":"markdown","source":"#### Combine all columns into a single column","metadata":{}},{"cell_type":"code","source":"## Isolate text column\ndata2 = data2['more_toxic']\n\n## Isolate text column\ndata3 = pd.read_csv('../input/jigsaw-toxic-severity-rating/validation_data.csv')\ndata3 = data3['less_toxic']\n\ntext_column = pd.concat([data1, data2, data3], axis=0, ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2023-06-25T19:26:41.181907Z","iopub.execute_input":"2023-06-25T19:26:41.182666Z","iopub.status.idle":"2023-06-25T19:26:41.523414Z","shell.execute_reply.started":"2023-06-25T19:26:41.182633Z","shell.execute_reply":"2023-06-25T19:26:41.522365Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"#### Check for duplicates","metadata":{}},{"cell_type":"code","source":"text_column.value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-06-25T19:26:41.530164Z","iopub.execute_input":"2023-06-25T19:26:41.532952Z","iopub.status.idle":"2023-06-25T19:26:41.591349Z","shell.execute_reply.started":"2023-06-25T19:26:41.532903Z","shell.execute_reply":"2023-06-25T19:26:41.590209Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":" sorry i jumped to conclusions \\n\\non christian terrorism article man, I don't agree with you, and I want you to go and listen to 'prophet of doom' (now in audio format) as it is good. But I was wrong to be so rude. It is not the Southern European way.                                                                                                                                                                               19\nthis irishtom guy is turning every article into an ad for islam                                                                                                                                                                                                                                                                                                                                                                            19\nYou are not sorry one damned bit.  You have yet to refute what I have written.  All you do is pass the insults as if it were salt on the dinner table.  This is on every article in which we disagree.  If you have something useful and constructive to say, then don't be a harpy troll.                                                                                                                                                 19\n YOUR BIASED! \\n\\nPLEASE OTHER THAN HIDE BEHIND WK RULES\\n\\nacutally IDENTITY THE OFFESNES COMMIMITED!\\n\\nYOU JUST SAID you dont care about my OPINIONS!..yet the opinions that where QUOTED WHERE FROM THE REFERENCES YOU HAD ACCEPTED!!\\n\\nLOL...\\n\\nSO in which case i am formally complaining about YOU AND YOUR BIASED STANCE!\\n\\nALL MY REFERNCES HAVE ISBN NUMBERS, YEAR AND PUBLISHERS!\\n\\nYOU ARE PROTECTING YOUR BIASED VIEW!    16\nI erased your cuss word\\nFrom: some random person out there in the world                                                                                                                                                                                                                                                                                                                                                                   16\n                                                                                                                                                                                                                                                                                                                                                                                                                                           ..\nHey\\n\\nI bet you Quinsareth are gay and like telling lies to your mother.                                                                                                                                                                                                                                                                                                                                                                   2\nVandalism on Muhammad page\\n\\nPerhaps that was the wrong way to deal with it, but                                                                                                                                                                                                                                                                                                                                                           2\n Thank You \\n\\nHey Nishkid I really appreciate the unblock.  Once again I apologize for any vandalism I caused on user pages and I have read Wikipedia's user policy.  Thank You!!!                                                                                                                                                                                                                                                         2\n Your low self-esteem \\n\\nI see you have such a low self-esteem that you have to warn others not to attack you.4.130.134.233                                                                                                                                                                                                                                                                                                                2\nVANDALISE MY ASS ==\\n\\n==                                                                                                                                                                                                                                                                                                                                                                                                                   2\nLength: 14251, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"It looks like between the data provided for the competition there are many duplicates. However we can see that some comments are reused many more times than other comments. For example the most used comments were repeated `19` times in the datasets while others only `2` times. \n\nSince the duplications are not balanced if we left the data like this I am afraid we would be biasing the model towards the comments which were present more in the data. \n\nI will remove all duplicate comments.","metadata":{}},{"cell_type":"code","source":"print(f\"Total numer of comments in text data = {len(text_column)}\")\nprint(f\"Numer of unique comments in text data = {len(text_column.unique())}\")\n\ntext_column = text_column.drop_duplicates()\nprint(\"Duplicate comments dropped\")","metadata":{"execution":{"iopub.status.busy":"2023-06-25T19:26:41.595966Z","iopub.execute_input":"2023-06-25T19:26:41.598366Z","iopub.status.idle":"2023-06-25T19:26:41.693599Z","shell.execute_reply.started":"2023-06-25T19:26:41.598330Z","shell.execute_reply":"2023-06-25T19:26:41.692474Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Total numer of comments in text data = 67753\nNumer of unique comments in text data = 14251\nDuplicate comments dropped\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Exploring the toxic comments","metadata":{}},{"cell_type":"code","source":"data = pd.DataFrame()\ndata['text'] = text_column\ndata = data.sample(100)\n\n# Function to calculate word count\ndef count_words(text):\n    words = nltk.word_tokenize(text)\n    return len(words)\n\n# Function to calculate verb count\ndef count_verbs(text):\n    words = nltk.word_tokenize(text)\n    tagged_words = nltk.pos_tag(words)\n    verb_count = len([word for word, tag in tagged_words if tag.startswith('V')])\n    return verb_count\n\n# Function to calculate noun count\ndef count_nouns(text):\n    words = nltk.word_tokenize(text)\n    tagged_words = nltk.pos_tag(words)\n    noun_count = len([word for word, tag in tagged_words if tag.startswith('N')])\n    return noun_count\n\n# Add word count column\ndata['word_count'] = data['text'].apply(count_words)\n\n# Add verb count column\ndata['verb_count'] = data['text'].apply(count_verbs)\n\n# Add noun count column\ndata['noun_count'] = data['text'].apply(count_nouns)\n\ndata.describe()","metadata":{"execution":{"iopub.status.busy":"2023-06-25T19:26:41.696047Z","iopub.execute_input":"2023-06-25T19:26:41.699834Z","iopub.status.idle":"2023-06-25T19:26:43.343292Z","shell.execute_reply.started":"2023-06-25T19:26:41.699798Z","shell.execute_reply":"2023-06-25T19:26:43.342282Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"        word_count  verb_count   noun_count\ncount   100.000000  100.000000   100.000000\nmean    104.740000   16.120000    31.550000\nstd     175.901179   31.514717   109.997096\nmin       5.000000    0.000000     1.000000\n25%      22.000000    3.000000     5.000000\n50%      38.500000    7.000000    10.000000\n75%      99.000000   16.000000    21.000000\nmax    1111.000000  278.000000  1064.000000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word_count</th>\n      <th>verb_count</th>\n      <th>noun_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>104.740000</td>\n      <td>16.120000</td>\n      <td>31.550000</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>175.901179</td>\n      <td>31.514717</td>\n      <td>109.997096</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>5.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>22.000000</td>\n      <td>3.000000</td>\n      <td>5.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>38.500000</td>\n      <td>7.000000</td>\n      <td>10.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>99.000000</td>\n      <td>16.000000</td>\n      <td>21.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1111.000000</td>\n      <td>278.000000</td>\n      <td>1064.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"ax = data['word_count'].plot(kind='kde')\ndata['verb_count'].plot(kind='kde', ax=ax)\ndata['noun_count'].plot(kind='kde', ax=ax)\n\nax.legend(['Word Count', 'Verb Count', 'Noun Count'])\nax.set_title('Distribution of Word Count, Verb Count, and Noun Count')\nax.set_xlabel('Count')\nax.set_ylabel('Density')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-25T19:26:43.344746Z","iopub.execute_input":"2023-06-25T19:26:43.347061Z","iopub.status.idle":"2023-06-25T19:26:43.806258Z","shell.execute_reply.started":"2023-06-25T19:26:43.347024Z","shell.execute_reply":"2023-06-25T19:26:43.805198Z"},"trusted":true},"execution_count":32,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"markdown","source":"We can see that these comments on average are quite short in length and contain more nouns than verbs on average.\n\nSince we have not done any cleaning of the data yet these distributions are not exact as the nltk package is not currently looking for misspelled words or different versions of word spellings which are used online sometimes.\n\nFor example if a user knows that the platform they are on has limitations on language than they may spell a profane word to try to fool any auto detecting systems such as `Fuck==>Fxck, F*ck, Fukk, Fuuu*uukk`, etc.\n\nTherefore these counts will not detect all nouns and verbs but should give a decent sample.\n\nKnowing the underlying distributions of some of these features is important because after the synthetic data is generated we would most likely want it to follow the same distributions for these attributes of the text. ","metadata":{}},{"cell_type":"markdown","source":"### Looking at the most common N-grams","metadata":{}},{"cell_type":"code","source":"# Tokenize the text into words\ndata['words'] = data['text'].apply(nltk.word_tokenize)\n\n# Get bigrams and trigrams for each row\ndata['bigrams']   = data['words'].apply(lambda x: list(ngrams(x, 2)))\ndata['trigrams']  = data['words'].apply(lambda x: list(ngrams(x, 3)))\n# data['quadgrams'] = data['words'].apply(lambda x: list(ngrams(x, 4)))\n\n# Count the occurrences of bigrams and trigrams\nbigram_counts   = Counter([gram for grams in data['bigrams'] for gram in grams])\ntrigram_counts  = Counter([gram for grams in data['trigrams'] for gram in grams])\n# quadgram_counts = Counter([gram for grams in data['quadgrams'] for gram in grams])\n\n# Get the most common bigrams, trigrams, and quadgrams\nmost_common_bigrams   = bigram_counts.most_common(50)\nmost_common_trigrams  = trigram_counts.most_common(50)\n# most_common_quadgrams = quadgram_counts.most_common(50)\n\ndf_common_grams = pd.DataFrame()\ndf_common_grams['bigrams']   = most_common_bigrams\ndf_common_grams['trigrams']  = most_common_trigrams\n# df_common_grams['quadgrams'] = most_common_quadgrams\n\n# # Display the results\n# print('Most common bigrams:')\n# for bigram, count in most_common_bigrams:\n#     print(' '.join(bigram), count)\n\n# print('\\nMost common trigrams:')\n# for trigram, count in most_common_trigrams:\n#     print(' '.join(trigram), count)\n    \n# print('\\nMost common quadgrams:')\n# for quadgram, count in most_common_quadgrams:\n#     print(' '.join(quadgram), count)\n\n\ndf_common_grams.iloc[:, :]","metadata":{"execution":{"iopub.status.busy":"2023-06-25T19:26:43.807765Z","iopub.execute_input":"2023-06-25T19:26:43.808802Z","iopub.status.idle":"2023-06-25T19:26:44.097475Z","shell.execute_reply.started":"2023-06-25T19:26:43.808766Z","shell.execute_reply":"2023-06-25T19:26:44.096251Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"                   bigrams                       trigrams\n0       ((YOU, SUCK), 471)        ((YOU, SUCK, YOU), 360)\n1       ((SUCK, YOU), 360)       ((SUCK, YOU, SUCK), 360)\n2   ((BunkSteve, is), 278)    ((BunkSteve, is, gay), 277)\n3         ((is, gay), 277)            ((is, gay, !), 277)\n4          ((gay, !), 277)     ((gay, !, BunkSteve), 277)\n5    ((!, BunkSteve), 277)      ((!, BunkSteve, is), 277)\n6      ((SUCK, DICK), 111)       ((YOU, SUCK, DICK), 111)\n7       ((DICK, YOU), 110)       ((SUCK, DICK, YOU), 110)\n8           (('', ''), 89)       ((DICK, YOU, SUCK), 110)\n9           ((``, ''), 78)                ((!, !, !), 56)\n10            ((!, !), 64)                ((?, ?, ?), 10)\n11          ((,, and), 35)               ((., '', ''), 9)\n12            ((., I), 28)               (('', '', ,), 7)\n13         ((in, the), 24)              ((., I, have), 7)\n14            ((,, I), 21)               ((,, ``, ''), 6)\n15          ((,, but), 19)             ((the, ``, ''), 6)\n16         ((of, the), 18)              ((is, ``, ''), 5)\n17         ((I, have), 17)              (('', '', is), 5)\n18           ((I, am), 16)               ((a, ``, ''), 5)\n19            ((?, ?), 16)              ((I, am, not), 5)\n20           ((., ''), 15)               ((,, '', ''), 5)\n21         ((on, the), 15)           ((..., ..., ...), 5)\n22           ((is, a), 14)  ((MONKEY, MONKEY, MONKEY), 4)\n23         ((do, n't), 14)                 ((:, :, :), 4)\n24        ((you, are), 14)            ((,, which, is), 4)\n25          ((,, you), 13)              ((as, ``, ''), 4)\n26          ((to, be), 13)               (('', '', .), 4)\n27         ((to, the), 12)             (('', '', and), 4)\n28       ((you, have), 11)               ((., ``, ``), 4)\n29    ((the, article), 11)              ((I, do, n't), 4)\n30         ((is, the), 10)           ((talk, page, .), 4)\n31        ((,, which), 10)              (('', '', as), 4)\n32           ((,, the), 9)     ((deleate, ,, deleate), 4)\n33            ((WP, :), 9)                ((``, :, :), 3)\n34          ((is, not), 8)           ((seems, to, be), 3)\n35       ((trying, to), 8)  ((``, '', macroevolution), 3)\n36          ((if, you), 8)  (('', macroevolution, ''), 3)\n37           ((., The), 8)  ((macroevolution, '', ''), 3)\n38           ((., You), 8)  ((microevolution, '', ''), 3)\n39            ((,, or), 8)             ((James, H, ,), 3)\n40       ((should, be), 8)            ((H, ,, Fetzer), 3)\n41           ((for, a), 7)         ((in, the, United), 3)\n42         ((are, you), 7)     ((the, United, States), 3)\n43            ((it, .), 7)            ((., You, have), 3)\n44             ((:, :), 7)           ((you, are, not), 3)\n45            (('', ,), 7)           ((part, of, the), 3)\n46          ((,, that), 7)              (('', '', ''), 3)\n47            ((,, as), 7)               ((!, '', ''), 3)\n48         ((you, 're), 7)                ((,, I, am), 3)\n49             ((), ,), 7)               (((, UTC, )), 3)","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>bigrams</th>\n      <th>trigrams</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>((YOU, SUCK), 471)</td>\n      <td>((YOU, SUCK, YOU), 360)</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>((SUCK, YOU), 360)</td>\n      <td>((SUCK, YOU, SUCK), 360)</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>((BunkSteve, is), 278)</td>\n      <td>((BunkSteve, is, gay), 277)</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>((is, gay), 277)</td>\n      <td>((is, gay, !), 277)</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>((gay, !), 277)</td>\n      <td>((gay, !, BunkSteve), 277)</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>((!, BunkSteve), 277)</td>\n      <td>((!, BunkSteve, is), 277)</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>((SUCK, DICK), 111)</td>\n      <td>((YOU, SUCK, DICK), 111)</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>((DICK, YOU), 110)</td>\n      <td>((SUCK, DICK, YOU), 110)</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>(('', ''), 89)</td>\n      <td>((DICK, YOU, SUCK), 110)</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>((``, ''), 78)</td>\n      <td>((!, !, !), 56)</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>((!, !), 64)</td>\n      <td>((?, ?, ?), 10)</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>((,, and), 35)</td>\n      <td>((., '', ''), 9)</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>((., I), 28)</td>\n      <td>(('', '', ,), 7)</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>((in, the), 24)</td>\n      <td>((., I, have), 7)</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>((,, I), 21)</td>\n      <td>((,, ``, ''), 6)</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>((,, but), 19)</td>\n      <td>((the, ``, ''), 6)</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>((of, the), 18)</td>\n      <td>((is, ``, ''), 5)</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>((I, have), 17)</td>\n      <td>(('', '', is), 5)</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>((I, am), 16)</td>\n      <td>((a, ``, ''), 5)</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>((?, ?), 16)</td>\n      <td>((I, am, not), 5)</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>((., ''), 15)</td>\n      <td>((,, '', ''), 5)</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>((on, the), 15)</td>\n      <td>((..., ..., ...), 5)</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>((is, a), 14)</td>\n      <td>((MONKEY, MONKEY, MONKEY), 4)</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>((do, n't), 14)</td>\n      <td>((:, :, :), 4)</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>((you, are), 14)</td>\n      <td>((,, which, is), 4)</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>((,, you), 13)</td>\n      <td>((as, ``, ''), 4)</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>((to, be), 13)</td>\n      <td>(('', '', .), 4)</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>((to, the), 12)</td>\n      <td>(('', '', and), 4)</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>((you, have), 11)</td>\n      <td>((., ``, ``), 4)</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>((the, article), 11)</td>\n      <td>((I, do, n't), 4)</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>((is, the), 10)</td>\n      <td>((talk, page, .), 4)</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>((,, which), 10)</td>\n      <td>(('', '', as), 4)</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>((,, the), 9)</td>\n      <td>((deleate, ,, deleate), 4)</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>((WP, :), 9)</td>\n      <td>((``, :, :), 3)</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>((is, not), 8)</td>\n      <td>((seems, to, be), 3)</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>((trying, to), 8)</td>\n      <td>((``, '', macroevolution), 3)</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>((if, you), 8)</td>\n      <td>(('', macroevolution, ''), 3)</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>((., The), 8)</td>\n      <td>((macroevolution, '', ''), 3)</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>((., You), 8)</td>\n      <td>((microevolution, '', ''), 3)</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>((,, or), 8)</td>\n      <td>((James, H, ,), 3)</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>((should, be), 8)</td>\n      <td>((H, ,, Fetzer), 3)</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>((for, a), 7)</td>\n      <td>((in, the, United), 3)</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>((are, you), 7)</td>\n      <td>((the, United, States), 3)</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>((it, .), 7)</td>\n      <td>((., You, have), 3)</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>((:, :), 7)</td>\n      <td>((you, are, not), 3)</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>(('', ,), 7)</td>\n      <td>((part, of, the), 3)</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>((,, that), 7)</td>\n      <td>(('', '', ''), 3)</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>((,, as), 7)</td>\n      <td>((!, '', ''), 3)</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>((you, 're), 7)</td>\n      <td>((,, I, am), 3)</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>((), ,), 7)</td>\n      <td>(((, UTC, )), 3)</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"We can see the initial 10 or so most common bi-grams and tri-grams are repetitive punctuation marks.\n\nTraditionally these would be cleaned and removed when training models for NLP tasks, however due to the nature of this work many of these traditional techniques will limit the models ability to predict toxicity as well as with clean text.\n\nI happened to have competed in this competition and one thing all of us learned was that leaving capital letters and punctuation improved the models ability to infer toxicity and especially levels of toxicity. \n\nFor example a phrase such as:\n\n`Are you kidding?`\n\nConveys a much different meaning than the same words but put this way:\n\n`ARE YOU KIDDING!!!??`\n\nTraditional NLP techniques would have us convert all characters to lower case and remove punctuation so the model will interpret both of those texts the exact same way.\n\nWhen training sentiment based models or models where feeling and emotion is being conveyed in some way such as toxicity of comments, it is more than just the raw content of the words alone which gives the meaning. The puncuation and capitalizations are very expressive forms of language and as such for these problems do better left in the data.","metadata":{}},{"cell_type":"markdown","source":"## Pre-Processing\n\n* First we need load in our text column as tensorflow formatted dataset\n\n* Next we shuffle the data to avoid any patterns which may have been present\n\n* We then slice the data into batches for processing\n\n* Vectorize the text which will be used to create a corpus of vocabulary used when training and act as vector representations of our text\n\n* Create the corpus of vocabulary which is used to train and evaluate throughout","metadata":{}},{"cell_type":"code","source":"vocab_size = 20000  ## Only consider the top 20k words\nmaxlen = 80  ## Max sequence length\nbatch_size = 32  ## Data loading batch sizes\n\n# Create a dataset from the pandas column\ntext_ds = tf.data.Dataset.from_tensor_slices(text_column)\n\n# Shuffle and batch the dataset\ntext_ds = text_ds.shuffle(buffer_size=256)\ntext_ds = text_ds.batch(batch_size)\n\n# def custom_standardization(input_string):\n#     \"\"\" Remove html line-break tags and handle punctuation \"\"\"\n#     lowercased = tf.strings.lower(input_string)\n#     stripped_html = tf.strings.regex_replace(lowercased, \"<br />\", \" \")\n#     return tf.strings.regex_replace(stripped_html, f\"([{string.punctuation}])\", r\" \\1\")\n\n\n## Create a vectorization layer and adapt it to the text\nvectorize_layer = TextVectorization(\n    standardize=None,\n    max_tokens=vocab_size - 1,\n    output_mode=\"int\",\n    output_sequence_length=maxlen + 1,\n)\nvectorize_layer.adapt(text_ds)\nvocab = vectorize_layer.get_vocabulary()  ## To get words back from token indices","metadata":{"execution":{"iopub.status.busy":"2023-06-25T19:26:44.099249Z","iopub.execute_input":"2023-06-25T19:26:44.099875Z","iopub.status.idle":"2023-06-25T19:26:45.496472Z","shell.execute_reply.started":"2023-06-25T19:26:44.099839Z","shell.execute_reply":"2023-06-25T19:26:45.495338Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"## Generate Labels\n\nSince we are building a generative auto-regressive model, we must train it to predict the next word by looking backwards and using the previous tokens to predict the highest probability for the next token.\n\nThis is fairly easy to create labels for because we simply shuffle the `TRUE` data be one token and then when training the model compares the predicted text with the next indexed word.\n\nWe can inspect what these samples and labels look like below:","metadata":{}},{"cell_type":"code","source":"## Function to create target column\ndef prepare_lm_inputs_labels(text):\n    \"\"\"\n    Shift word sequences by 1 position so that the target for position (i) is\n    word at position (i+1). The model will use all words up till position (i)\n    to predict the next word.\n    \"\"\"\n    text = tf.expand_dims(text, -1)\n    tokenized_sentences = vectorize_layer(text)\n    x = tokenized_sentences[:, :-1]\n    y = tokenized_sentences[:, 1:]\n    return x, y\n\n\ntext_ds = text_ds.map(prepare_lm_inputs_labels)\ntext_ds = text_ds.prefetch(tf.data.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2023-06-25T19:26:45.500961Z","iopub.execute_input":"2023-06-25T19:26:45.501407Z","iopub.status.idle":"2023-06-25T19:26:45.610930Z","shell.execute_reply.started":"2023-06-25T19:26:45.501370Z","shell.execute_reply":"2023-06-25T19:26:45.609755Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"## Select samples from the training data set to inspect\nsample = text_ds.take(5) \n\n## Display some samples\nfor x, y in sample:\n    # Convert token indices back to words\n    input_words  = [vocab[i] for i in x[0].numpy()]\n    target_words = [vocab[i] for i in y[0].numpy()]\n\n    print(\"\\n\\n\\n\\nInput Sequence:\")\n    print(\" \".join(input_words))\n    print(\"\\nTarget Sequence:\")\n    print(\" \".join(target_words))","metadata":{"execution":{"iopub.status.busy":"2023-06-25T19:26:45.612541Z","iopub.execute_input":"2023-06-25T19:26:45.612914Z","iopub.status.idle":"2023-06-25T19:26:45.672092Z","shell.execute_reply.started":"2023-06-25T19:26:45.612883Z","shell.execute_reply":"2023-06-25T19:26:45.670945Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"\n\n\n\nInput Sequence:\nDamn, fellas, Paul [UNK] came to the NBA after John [UNK] retired from it! [UNK] 7 Jan 2005 (UTC)                                                             \n\nTarget Sequence:\nfellas, Paul [UNK] came to the NBA after John [UNK] retired from it! [UNK] 7 Jan 2005 (UTC)                                                              \n\n\n\n\nInput Sequence:\nOh shit! I better flee for the [UNK] By the way, I didn't cause any disruption. D. J. [UNK] blew this way out of proportion.                                                       \n\nTarget Sequence:\nshit! I better flee for the [UNK] By the way, I didn't cause any disruption. D. J. [UNK] blew this way out of proportion.                                                        \n\n\n\n\nInput Sequence:\n[UNK] -) at least this is one thing I'm glad about. I'm suggesting that we keep the [UNK] [UNK] it probably won't be back. I'm still having a [UNK] check out [UNK] Niggers Association of America for details. I just [UNK] at [UNK] First time on this website ever. Time for a [UNK] -                          \n\nTarget Sequence:\n-) at least this is one thing I'm glad about. I'm suggesting that we keep the [UNK] [UNK] it probably won't be back. I'm still having a [UNK] check out [UNK] Niggers Association of America for details. I just [UNK] at [UNK] First time on this website ever. Time for a [UNK] -                           \n\n\n\n\nInput Sequence:\nAt first I thought your [UNK] was correct. You may be right about what I am saying about Van [UNK] maybe I [UNK] insert it at all. But don't ever go and make fun of me. What's your fuckin' problem [UNK] Why did you put that stuff about me and the [UNK] pounds, what's your fuckin' problem. My size has nothing to do with the Van [UNK] article. I may be a little [UNK] I'm a [UNK] and I guarantee\n\nTarget Sequence:\nfirst I thought your [UNK] was correct. You may be right about what I am saying about Van [UNK] maybe I [UNK] insert it at all. But don't ever go and make fun of me. What's your fuckin' problem [UNK] Why did you put that stuff about me and the [UNK] pounds, what's your fuckin' problem. My size has nothing to do with the Van [UNK] article. I may be a little [UNK] I'm a [UNK] and I guarantee you\n\n\n\n\nInput Sequence:\nOne other reason I had created the article is that I travel the US as, among other things, as a graphics [UNK] I do NOT sketch people, due to the [UNK] debates going on, and I've ran into people who has stated that IF there is alien [UNK] alien life is [UNK] they may [UNK] Some will do so due to religious reasons, some will do so AS vengeance for the [UNK] of the [UNK] [UNK] [UNK] to make all\n\nTarget Sequence:\nother reason I had created the article is that I travel the US as, among other things, as a graphics [UNK] I do NOT sketch people, due to the [UNK] debates going on, and I've ran into people who has stated that IF there is alien [UNK] alien life is [UNK] they may [UNK] Some will do so due to religious reasons, some will do so AS vengeance for the [UNK] of the [UNK] [UNK] [UNK] to make all who\n","output_type":"stream"}]},{"cell_type":"markdown","source":"* ***We can see that the target or label sequence is merely our ground truth text sequence we have just shifted by `1` token. This is what our model will use to evaluate during training.***\n\n* ***Cell below was for loading in and preprocessing the IMBD movie quotes dataset. This is the dataset I tested this approach on first.***","metadata":{}},{"cell_type":"code","source":"# !curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n# !tar -xf aclImdb_v1.tar.gz\n\n# batch_size = 128\n\n# # The dataset contains each review in a separate text file\n# # The text files are present in four different folders\n# # Create a list all files\n# filenames = []\n# directories = [\n#     \"aclImdb/train/pos\",\n#     \"aclImdb/train/neg\",\n#     \"aclImdb/test/pos\",\n#     \"aclImdb/test/neg\",\n# ]\n# for dir in directories:\n#     for f in os.listdir(dir):\n#         filenames.append(os.path.join(dir, f))\n\n# print(f\"{len(filenames)} files\")\n\n# # Create a dataset from text files\n# random.shuffle(filenames)\n# text_ds = tf.data.TextLineDataset(filenames)\n# text_ds = text_ds.shuffle(buffer_size=256)\n# text_ds = text_ds.batch(batch_size)\n\n# def custom_standardization(input_string):\n#     \"\"\" Remove html line-break tags and handle punctuation \"\"\"\n#     lowercased = tf.strings.lower(input_string)\n#     stripped_html = tf.strings.regex_replace(lowercased, \"<br />\", \" \")\n#     return tf.strings.regex_replace(stripped_html, f\"([{string.punctuation}])\", r\" \\1\")\n\n\n# # Create a vectorization layer and adapt it to the text\n# vectorize_layer = TextVectorization(\n#     standardize=custom_standardization,\n#     max_tokens=vocab_size - 1,\n#     output_mode=\"int\",\n#     output_sequence_length=maxlen + 1,\n# )\n# vectorize_layer.adapt(text_ds)\n# vocab = vectorize_layer.get_vocabulary()  # To get words back from token indices\n\n# ## Functoin to create target column\n# def prepare_lm_inputs_labels(text):\n#     \"\"\"\n#     Shift word sequences by 1 position so that the target for position (i) is\n#     word at position (i+1). The model will use all words up till position (i)\n#     to predict the next word.\n#     \"\"\"\n#     text = tf.expand_dims(text, -1)\n#     tokenized_sentences = vectorize_layer(text)\n#     x = tokenized_sentences[:, :-1]\n#     y = tokenized_sentences[:, 1:]\n#     return x, y\n\n\n# text_ds = text_ds.map(prepare_lm_inputs_labels)\n# text_ds = text_ds.prefetch(tf.data.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2023-06-25T16:54:39.672498Z","iopub.execute_input":"2023-06-25T16:54:39.672899Z","iopub.status.idle":"2023-06-25T16:54:55.769014Z","shell.execute_reply.started":"2023-06-25T16:54:39.672868Z","shell.execute_reply":"2023-06-25T16:54:55.767732Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 80.2M  100 80.2M    0     0  9419k      0  0:00:08  0:00:08 --:--:-- 17.8M\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Implement a Transformer block as a layer","metadata":{}},{"cell_type":"code","source":"def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n    \"\"\"\n    Mask the upper half of the dot product matrix in self attention.\n    This prevents flow of information from future tokens to current token.\n    1's in the lower triangle, counting from the lower right corner.\n    \"\"\"\n    i = tf.range(n_dest)[:, None]\n    j = tf.range(n_src)\n    m = i >= j - n_src + n_dest\n    mask = tf.cast(m, dtype)\n    mask = tf.reshape(mask, [1, n_dest, n_src])\n    mult = tf.concat(\n        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n    )\n    return tf.tile(mask, mult)\n\n\nclass TransformerBlock(layers.Layer):\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n        super().__init__()\n        self.att = layers.MultiHeadAttention(num_heads, embed_dim)\n        self.ffn = keras.Sequential(\n            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n        )\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(rate)\n        self.dropout2 = layers.Dropout(rate)\n\n    def call(self, inputs):\n        input_shape = tf.shape(inputs)\n        batch_size = input_shape[0]\n        seq_len = input_shape[1]\n        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n        attention_output = self.att(inputs, inputs, attention_mask=causal_mask)\n        attention_output = self.dropout1(attention_output)\n        out1 = self.layernorm1(inputs + attention_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output)\n        return self.layernorm2(out1 + ffn_output)","metadata":{"execution":{"iopub.status.busy":"2023-06-25T16:54:39.634011Z","iopub.execute_input":"2023-06-25T16:54:39.635264Z","iopub.status.idle":"2023-06-25T16:54:39.647588Z","shell.execute_reply.started":"2023-06-25T16:54:39.635229Z","shell.execute_reply":"2023-06-25T16:54:39.646676Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Implement an embedding layer\n\nCreate two separate embedding layers: one for tokens and one for token index (positions).","metadata":{}},{"cell_type":"code","source":"class TokenAndPositionEmbedding(layers.Layer):\n    def __init__(self, maxlen, vocab_size, embed_dim):\n        super().__init__()\n        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n\n    def call(self, x):\n        maxlen = tf.shape(x)[-1]\n        positions = tf.range(start=0, limit=maxlen, delta=1)\n        positions = self.pos_emb(positions)\n        x = self.token_emb(x)\n        return x + positions","metadata":{"execution":{"iopub.status.busy":"2023-06-25T16:54:39.649912Z","iopub.execute_input":"2023-06-25T16:54:39.650555Z","iopub.status.idle":"2023-06-25T16:54:39.657795Z","shell.execute_reply.started":"2023-06-25T16:54:39.650524Z","shell.execute_reply":"2023-06-25T16:54:39.656702Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Implement the miniature GPT model","metadata":{}},{"cell_type":"code","source":"vocab_size = 20000  # Only consider the top 20k words\nmaxlen = 80  # Max sequence size\nembed_dim = 256  # Embedding size for each token\nnum_heads = 2  # Number of attention heads\nfeed_forward_dim = 256  # Hidden layer size in feed forward network inside transformer\n\n\ndef create_model():\n    inputs = layers.Input(shape=(maxlen,), dtype=tf.int32)\n    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n    x = embedding_layer(inputs)\n    transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim)\n    x = transformer_block(x)\n    outputs = layers.Dense(vocab_size)(x)\n    model = keras.Model(inputs=inputs, outputs=[outputs, x])\n    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n    model.compile(\n        \"adam\", loss=[loss_fn, None],\n    )  # No loss and optimization based on word embeddings from transformer block\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-06-25T16:54:39.660708Z","iopub.execute_input":"2023-06-25T16:54:39.661322Z","iopub.status.idle":"2023-06-25T16:54:39.670841Z","shell.execute_reply.started":"2023-06-25T16:54:39.661291Z","shell.execute_reply":"2023-06-25T16:54:39.669815Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Prepare the data for word-level language modelling\n\nDownload the IMDB dataset and combine training and validation sets for a text generation task.","metadata":{}},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-06-25T16:55:09.057157Z","iopub.execute_input":"2023-06-25T16:55:09.057508Z","iopub.status.idle":"2023-06-25T16:55:09.903678Z","shell.execute_reply.started":"2023-06-25T16:55:09.057475Z","shell.execute_reply":"2023-06-25T16:55:09.902724Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"14251"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-06-25T16:55:09.905231Z","iopub.execute_input":"2023-06-25T16:55:09.905613Z","iopub.status.idle":"2023-06-25T16:55:12.217453Z","shell.execute_reply.started":"2023-06-25T16:55:09.905579Z","shell.execute_reply":"2023-06-25T16:55:12.216469Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Implement a Keras callback for generating text","metadata":{}},{"cell_type":"code","source":"# class TextGenerator(keras.callbacks.Callback):\n#     \"\"\"A callback to generate text from a trained model.\n#     1. Feed some starting prompt to the model\n#     2. Predict probabilities for the next token\n#     3. Sample the next token and add it to the next input\n\n#     Arguments:\n#         max_tokens: Integer, the number of tokens to be generated after prompt.\n#         start_tokens: List of integers, the token indices for the starting prompt.\n#         index_to_word: List of strings, obtained from the TextVectorization layer.\n#         top_k: Integer, sample from the `top_k` token predictions.\n#         print_every: Integer, print after this many epochs.\n#     \"\"\"\n\n#     def __init__(\n#         self, max_tokens, start_tokens, index_to_word, top_k=10, print_every=1\n#     ):\n#         self.max_tokens = max_tokens\n#         self.start_tokens = start_tokens\n#         self.index_to_word = index_to_word\n#         self.print_every = print_every\n#         self.k = top_k\n\n#     def sample_from(self, logits):\n#         logits, indices = tf.math.top_k(logits, k=self.k, sorted=True)\n#         indices = np.asarray(indices).astype(\"int32\")\n#         preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n#         preds = np.asarray(preds).astype(\"float32\")\n#         return np.random.choice(indices, p=preds)\n\n#     def detokenize(self, number):\n#         return self.index_to_word[number]\n\n#     def on_epoch_end(self, epoch, logs=None):\n#         start_tokens = [_ for _ in self.start_tokens]\n#         if (epoch + 1) % self.print_every != 0:\n#             return\n#         num_tokens_generated = 0\n#         tokens_generated = []\n#         while num_tokens_generated <= self.max_tokens:\n#             pad_len = maxlen - len(start_tokens)\n#             sample_index = len(start_tokens) - 1\n#             if pad_len < 0:\n#                 x = start_tokens[:maxlen]\n#                 sample_index = maxlen - 1\n#             elif pad_len > 0:\n#                 x = start_tokens + [0] * pad_len\n#             else:\n#                 x = start_tokens\n#             x = np.array([x])\n#             y, _ = self.model.predict(x)\n#             sample_token = self.sample_from(y[0][sample_index])\n#             tokens_generated.append(sample_token)\n#             start_tokens.append(sample_token)\n#             num_tokens_generated = len(tokens_generated)\n#         txt = \" \".join(\n#             [self.detokenize(_) for _ in self.start_tokens + tokens_generated]\n#         )\n#         print(f\"generated text:\\n{txt}\\n\")\n        \n        \n\n# # Tokenize starting prompt\n# word_to_index = {}\n# for index, word in enumerate(vocab):\n#     word_to_index[word] = index\n","metadata":{"execution":{"iopub.status.busy":"2023-06-25T16:55:12.311350Z","iopub.execute_input":"2023-06-25T16:55:12.311646Z","iopub.status.idle":"2023-06-25T16:55:12.318127Z","shell.execute_reply.started":"2023-06-25T16:55:12.311622Z","shell.execute_reply":"2023-06-25T16:55:12.317114Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class TextGenerator(keras.callbacks.Callback):\n    \"\"\"A callback to generate text from a trained model.\n    1. Feed some starting prompt to the model\n    2. Predict probabilities for the next token\n    3. Sample the next token and add it to the next input\n\n    Arguments:\n        max_tokens: Integer, the number of tokens to be generated after prompt.\n        start_tokens: List of integers, the token indices for the starting prompt.\n        index_to_word: List of strings, obtained from the TextVectorization layer.\n        top_k: Integer, sample from the `top_k` token predictions.\n        print_every: Integer, print after this many epochs.\n    \"\"\"\n\n    def __init__(\n        self, max_tokens, start_tokens, index_to_word, top_k=10, print_every=1\n    ):\n        self.max_tokens = max_tokens\n        self.start_tokens = start_tokens\n        self.index_to_word = index_to_word\n        self.print_every = print_every\n        self.k = top_k\n\n    def sample_from(self, logits):\n        logits, indices = tf.math.top_k(logits, k=self.k, sorted=True)\n        indices = np.asarray(indices).astype(\"int32\")\n        preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n        preds = np.asarray(preds).astype(\"float32\")\n        return np.random.choice(indices, p=preds)\n\n    def detokenize(self, number):\n        return self.index_to_word[number]\n\n    def on_epoch_end(self, epoch, logs=None):\n        if (epoch + 1) % self.print_every != 0:\n            return\n        print(f\"Epoch {epoch+1}: loss = {logs['loss']:.4f}\")\n        start_tokens = [_ for _ in self.start_tokens]\n        num_tokens_generated = 0\n        tokens_generated = []\n        while num_tokens_generated <= self.max_tokens:\n            pad_len = maxlen - len(start_tokens)\n            sample_index = len(start_tokens) - 1\n            if pad_len < 0:\n                x = start_tokens[:maxlen]\n                sample_index = maxlen - 1\n            elif pad_len > 0:\n                x = start_tokens + [0] * pad_len\n            else:\n                x = start_tokens\n            x = np.array([x])\n            y, _ = self.model.predict(x)\n            sample_token = self.sample_from(y[0][sample_index])\n            tokens_generated.append(sample_token)\n            start_tokens.append(sample_token)\n            num_tokens_generated = len(tokens_generated)\n        txt = \" \".join(\n            [self.detokenize(_) for _ in self.start_tokens + tokens_generated]\n        )\n        print(f\"Generated text:\\n{txt}\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-06-25T16:55:12.319809Z","iopub.execute_input":"2023-06-25T16:55:12.320506Z","iopub.status.idle":"2023-06-25T16:55:12.335714Z","shell.execute_reply.started":"2023-06-25T16:55:12.320475Z","shell.execute_reply":"2023-06-25T16:55:12.334783Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Tokenize starting prompt\nword_to_index = {}\nfor index, word in enumerate(vocab):\n    word_to_index[word] = index","metadata":{"execution":{"iopub.status.busy":"2023-06-25T16:55:40.097478Z","iopub.execute_input":"2023-06-25T16:55:40.097878Z","iopub.status.idle":"2023-06-25T16:55:40.113464Z","shell.execute_reply.started":"2023-06-25T16:55:40.097846Z","shell.execute_reply":"2023-06-25T16:55:40.112474Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"start_prompt = \"this movie is\"\nstart_prompt = \"what is the\"\n\n\nstart_tokens = [word_to_index.get(_, 1) for _ in start_prompt.split()]\nnum_tokens_generated = 75\ntext_gen_callback = TextGenerator(num_tokens_generated, start_tokens, vocab)","metadata":{"execution":{"iopub.status.busy":"2023-06-25T16:55:41.256235Z","iopub.execute_input":"2023-06-25T16:55:41.256582Z","iopub.status.idle":"2023-06-25T16:55:41.261728Z","shell.execute_reply.started":"2023-06-25T16:55:41.256555Z","shell.execute_reply":"2023-06-25T16:55:41.260823Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Train the model","metadata":{}},{"cell_type":"code","source":"# from tqdm import tqdm\n\n# # Create a wrapper function for the training loop\n# def train_with_progress_bar(model, dataset, epochs, callbacks):\n#     # Disable the progress bar by setting `disable=True`\n#     with tqdm(total=epochs, disable=True) as pbar:\n#         for epoch in range(epochs):\n#             # Perform one epoch of training\n#             model.fit(dataset, verbose=0, epochs=epochs, callbacks=[text_gen_callback])\n            \n#             # Update the progress bar manually\n#             pbar.set_postfix({'Epoch': epoch + 1})\n#             pbar.update(1)\n\n# # Train the model using the wrapper function\n# train_with_progress_bar(model, text_ds, N_EPOCHS, callbacks=[text_gen_callback])","metadata":{"execution":{"iopub.status.busy":"2023-06-25T18:22:15.294890Z","iopub.execute_input":"2023-06-25T18:22:15.295598Z","iopub.status.idle":"2023-06-25T18:29:41.560664Z","shell.execute_reply.started":"2023-06-25T18:22:15.295564Z","shell.execute_reply":"2023-06-25T18:29:41.558973Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stdout","text":"Epoch 1: loss = 0.3659\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 22ms/step\nGenerated text:\nwhat is the difference ? i was doing to the same thing . you have a [UNK] ? if i was gonna check the facts before you remove them .              \n\nEpoch 2: loss = 0.3651\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\nGenerated text:\nwhat is the fuck up . [UNK] ? fuck you , just go to a message those who lack of military newspaper is [UNK] - i have a crime . maybe an actual photos of the page . - ] . - i should\n\nEpoch 3: loss = 0.3562\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\nGenerated text:\nwhat is the problem ? i 'm sure that you 're a person and the idiot . . . . i 'm a [UNK] asshole . .you deserve it               \n\nEpoch 4: loss = 0.3487\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 33ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\nGenerated text:\nwhat is the 3rr ? just on it is clear the way i am not only attacking you today .                        \n\nEpoch 5: loss = 0.3417\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 36ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\nGenerated text:\nwhat is the problem ! i know who the hell are you doing ?                              \n\nEpoch 6: loss = 0.3357\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 60ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\nGenerated text:\nwhat is the vandalism . you 've vandalized user : i split into the trouble with the article .                         \n\nEpoch 7: loss = 0.3292\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\nGenerated text:\nwhat is the one idiot keeps reverting                                     \n\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[59], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m             pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Train the model using the wrapper function\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[43mtrain_with_progress_bar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtext_gen_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[59], line 9\u001b[0m, in \u001b[0;36mtrain_with_progress_bar\u001b[0;34m(model, dataset, epochs, callbacks)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39mepochs, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;66;03m# Perform one epoch of training\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m         \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtext_gen_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;66;03m# Update the progress bar manually\u001b[39;00m\n\u001b[1;32m     12\u001b[0m         pbar\u001b[38;5;241m.\u001b[39mset_postfix({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m'\u001b[39m: epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m})\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/engine/training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1678\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1679\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1683\u001b[0m ):\n\u001b[1;32m   1684\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1685\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1687\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    891\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 894\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    896\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    897\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    923\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    924\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    925\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 926\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    928\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    929\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    930\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    141\u001b[0m   (concrete_function,\n\u001b[1;32m    142\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1753\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1755\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1756\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1757\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1758\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1759\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1760\u001b[0m     args,\n\u001b[1;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1762\u001b[0m     executing_eagerly)\n\u001b[1;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    380\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 381\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    389\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    390\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    394\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"model = create_model()\n\nN_EPOCHS = 25\nverbose = 2 ## Set to a number such as 2 to see each steps progress bar\nhistory = model.fit(text_ds, verbose=0, epochs=N_EPOCHS, callbacks=[text_gen_callback])","metadata":{"execution":{"iopub.status.busy":"2023-06-25T18:30:10.796376Z","iopub.execute_input":"2023-06-25T18:30:10.796743Z","iopub.status.idle":"2023-06-25T18:34:14.726457Z","shell.execute_reply.started":"2023-06-25T18:30:10.796708Z","shell.execute_reply":"2023-06-25T18:34:14.723342Z"},"trusted":true},"execution_count":60,"outputs":[{"name":"stdout","text":"Epoch 1: loss = 3.0473\n1/1 [==============================] - 0s 224ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 22ms/step\nGenerated text:\nwhat is the hell ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n\nEpoch 2: loss = 2.2580\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 31ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 19ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\nGenerated text:\nwhat is the problem with the problem is this joke on earth . it was not an edit warring , untwirl , i have already been a history of my talk  user page \" \" \" , and \" \" [UNK] \" \"\n\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[60], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m N_EPOCHS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m25\u001b[39m\n\u001b[1;32m      4\u001b[0m verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;66;03m## Set to a number such as 2 to see each steps progress bar\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtext_gen_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/engine/training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1678\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1679\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1683\u001b[0m ):\n\u001b[1;32m   1684\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1685\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1687\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    891\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 894\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    896\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    897\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    923\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    924\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    925\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 926\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    928\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    929\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    930\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    141\u001b[0m   (concrete_function,\n\u001b[1;32m    142\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1753\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1755\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1756\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1757\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1758\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1759\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1760\u001b[0m     args,\n\u001b[1;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1762\u001b[0m     executing_eagerly)\n\u001b[1;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    380\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 381\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    389\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    390\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    394\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"Now we can generate text continuuing from a new prmopt ","metadata":{}},{"cell_type":"code","source":"new_start_prompt = \"start something\"\nnew_start_tokens = [word_to_index.get(word, 1) for word in new_start_prompt.split()]\n\ntext_gen_callback.start_tokens = new_start_tokens\ntext_gen_callback.on_epoch_end(0)","metadata":{"execution":{"iopub.status.busy":"2023-06-25T16:55:12.607587Z","iopub.status.idle":"2023-06-25T16:55:12.608278Z","shell.execute_reply.started":"2023-06-25T16:55:12.608028Z","shell.execute_reply":"2023-06-25T16:55:12.608050Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}